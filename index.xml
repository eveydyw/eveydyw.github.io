<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>EV's BLOG on EV's Blog</title><link>https://eveydyw.github.io/</link><description>Recent content in EV's BLOG on EV's Blog</description><generator>Hugo</generator><language>en</language><lastBuildDate>Sun, 23 Feb 2025 15:37:26 +0800</lastBuildDate><atom:link href="https://eveydyw.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>QuickStart</title><link>https://eveydyw.github.io/blog/quick_start/</link><pubDate>Sun, 23 Feb 2025 15:37:26 +0800</pubDate><guid>https://eveydyw.github.io/blog/quick_start/</guid><description>&lt;p>在 &lt;code>quickstart&lt;/code> 目录中为您的项目创建目录结构。&lt;/p>
&lt;h2 id="添加内容" class="heading">
 添加内容&lt;a href="#%e6%b7%bb%e5%8a%a0%e5%86%85%e5%ae%b9" aria-labelledby="添加内容">&lt;svg class="svg-inline--fa fas fa-link anchor" fill="currentColor" aria-hidden="true" role="img" viewBox="0 0 640 512">&lt;use href="#fas-link">&lt;/use>&lt;/svg>&lt;/a>
&lt;/h2>&lt;p>向您的网站添加一个新页面。&lt;/p>
&lt;div class="mb-3 syntax-highlight">&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt" id="hl-0-1">&lt;a class="lnlinks" href="#hl-0-1">1&lt;/a>
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-go" data-lang="go">&lt;span class="line">&lt;span class="cl">&lt;span class="nx">hugo&lt;/span> &lt;span class="nx">new&lt;/span> &lt;span class="nx">content&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="nx">posts&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="nx">my&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="nx">first&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="nx">post&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="nx">md&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;/div>&lt;p>启动 Hugo 的开发服务器以查看网站&lt;/p></description></item><item><title>Transformer Attention</title><link>https://eveydyw.github.io/blogs/transformer-attention/</link><pubDate>Sat, 21 Sep 2024 23:35:21 +0800</pubDate><guid>https://eveydyw.github.io/blogs/transformer-attention/</guid><description>&lt;h2 id="kv-cache" class="heading">
 KV Cache&lt;a href="#kv-cache" aria-labelledby="kv-cache">&lt;svg class="svg-inline--fa fas fa-link anchor" fill="currentColor" aria-hidden="true" role="img" viewBox="0 0 640 512">&lt;use href="#fas-link">&lt;/use>&lt;/svg>&lt;/a>
&lt;/h2>&lt;p>&lt;code>KV Cache&lt;/code> 在不影响任何计算精度的前提下，通过空间换时间思想，提高推理性能。&lt;/p>
&lt;p>&lt;code>KV Cache&lt;/code> &lt;strong>只能用于 Decoder 架构的模型&lt;/strong>，因为 Decoder 有 &lt;strong>Causal Mask&lt;/strong>，在推理的时候前面已经生成的字符不需要与后面的字符产生 attention，从而使得前面已经计算的 




 
 
 
 
 
 
 &lt;span class="mathjax-inline">
 &lt;span class="katex">&lt;math xmlns="http://www.w3.org/1998/Math/MathML">&lt;semantics>&lt;mrow>&lt;mi>K&lt;/mi>&lt;/mrow>&lt;annotation encoding="application/x-tex">K&lt;/annotation>&lt;/semantics>&lt;/math>&lt;/span>
 &lt;/span>
 
 

 和 




 
 
 
 
 
 
 &lt;span class="mathjax-inline">
 &lt;span class="katex">&lt;math xmlns="http://www.w3.org/1998/Math/MathML">&lt;semantics>&lt;mrow>&lt;mi>V&lt;/mi>&lt;/mrow>&lt;annotation encoding="application/x-tex">V&lt;/annotation>&lt;/semantics>&lt;/math>&lt;/span>
 &lt;/span>
 
 

 可以缓存起来。&lt;/p></description></item><item><title>RoPE</title><link>https://eveydyw.github.io/blogs/rope/</link><pubDate>Tue, 27 Aug 2024 13:42:30 +0800</pubDate><guid>https://eveydyw.github.io/blogs/rope/</guid><description>&lt;h2 id="rope" class="heading">
 RoPE&lt;a href="#rope" aria-labelledby="rope">&lt;svg class="svg-inline--fa fas fa-link anchor" fill="currentColor" aria-hidden="true" role="img" viewBox="0 0 640 512">&lt;use href="#fas-link">&lt;/use>&lt;/svg>&lt;/a>
&lt;/h2>&lt;p>&lt;code>RoPE &lt;/code> 通过 &lt;strong>绝对位置编码&lt;/strong> 的方式实现 &lt;strong>相对位置编码&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>绝对位置编码&lt;/strong>：&lt;strong>位置索引&lt;/strong> 直接进行编码。一般都是直接构建 &lt;strong>词嵌入向量&lt;/strong> 和 &lt;strong>位置嵌入向量&lt;/strong> 直接相加。&lt;/p></description></item><item><title>LSH</title><link>https://eveydyw.github.io/blogs/lsh/</link><pubDate>Mon, 17 Jun 2024 23:04:04 +0800</pubDate><guid>https://eveydyw.github.io/blogs/lsh/</guid><description>&lt;p>在 &lt;strong>Top N 推荐&lt;/strong>中，我们需要处理的是大量高维度的数据，如何快速地从大量的高维度数据集中找出与某条数据最为接近的一条或多条数据成为了难题。&lt;/p>
&lt;p>如果只是一些小规模的低维度数据集，可以很容易地使用线性搜索来解决问题；但如果我们要在一个庞大的高维度数据集中使用线性搜索来进行匹配，则会消耗很多时间。&lt;/p>
&lt;p>因此，需要采取一些类似于索引的技术来加速查询过程，这些技术通常被统称为 &lt;strong>最近邻查找 (Nearest Neighbor, &lt;code>NN&lt;/code>)&lt;/strong> ，而在处理大规模数据时，还可以考虑采用 &lt;strong>近似最近邻查找 (Approximate Nearest Neighbor, &lt;code>ANN&lt;/code>)&lt;/strong>。其中一种常用的方法就是&lt;strong>局部敏感哈希 (Locality-Sensitive Hashing, &lt;code>LSH&lt;/code>)&lt;/strong>。&lt;/p>
&lt;h2 id="hash" class="heading">
 Hash&lt;a href="#hash" aria-labelledby="hash">&lt;svg class="svg-inline--fa fas fa-link anchor" fill="currentColor" aria-hidden="true" role="img" viewBox="0 0 640 512">&lt;use href="#fas-link">&lt;/use>&lt;/svg>&lt;/a>
&lt;/h2>&lt;p>&lt;code>Hash&lt;/code>一般翻译做 &lt;strong>散列&lt;/strong>，就是把任意长度的输入（又叫做 &lt;strong>预映射&lt;/strong>， &lt;strong>pre-image&lt;/strong>），通过散列算法，变换成固定长度的输出，该输出就是散列值。&lt;/p></description></item><item><title>FlashAttention</title><link>https://eveydyw.github.io/blogs/flashattention/</link><pubDate>Wed, 05 Jun 2024 22:58:50 +0800</pubDate><guid>https://eveydyw.github.io/blogs/flashattention/</guid><description>&lt;h2 id="background" class="heading">
 Background&lt;a href="#background" aria-labelledby="background">&lt;svg class="svg-inline--fa fas fa-link anchor" fill="currentColor" aria-hidden="true" role="img" viewBox="0 0 640 512">&lt;use href="#fas-link">&lt;/use>&lt;/svg>&lt;/a>
&lt;/h2>&lt;h3 id="structure-of-gpu-memory" class="heading">
 Structure of GPU Memory&lt;a href="#structure-of-gpu-memory" aria-labelledby="structure-of-gpu-memory">&lt;svg class="svg-inline--fa fas fa-link anchor" fill="currentColor" aria-hidden="true" role="img" viewBox="0 0 640 512">&lt;use href="#fas-link">&lt;/use>&lt;/svg>&lt;/a>
&lt;/h3>&lt;center>
&lt;img style = " border-radius: 0.3125em; box-shadow: 0 0.2rem 0.4rem 0 rgba(34,36,38,.12); margin: 1rem;" 
src = "pics/image-20250310221404825.png" 
width = "35%" 
alt = "">
&lt;/center>
&lt;p>在 GPU 当中，memory 也跟 CPU memory 一样分成不同的 level，通常 &lt;strong>越上层空间越小&lt;/strong> 但是 &lt;strong>速度越快&lt;/strong>&lt;/p></description></item><item><title>Transformer</title><link>https://eveydyw.github.io/blogs/transformer/</link><pubDate>Tue, 12 Mar 2024 19:52:32 +0800</pubDate><guid>https://eveydyw.github.io/blogs/transformer/</guid><description>&lt;p>标准的 &lt;strong>Transformer&lt;/strong> 模型主要由两个模块构成：&lt;/p>
&lt;center>
&lt;img style = " border-radius: 0.3125em; box-shadow: 0 0.2rem 0.4rem 0 rgba(34,36,38,.12); margin: 1rem;" width = "35%" alt = "" 
src = "pics/image-20250312200523538.png" >
&lt;/center>





 &lt;blockquote class="blockquote">
 &lt;ul>
&lt;li>&lt;strong>Encoder（左边）&lt;/strong>：负责理解输入文本，为每个输入构造对应的语义表示（语义特征）；&lt;/li>
&lt;li>&lt;strong>Decoder（右边）&lt;/strong>：负责生成输出，使用 Encoder 输出的语义表示结合其他输入来生成目标序列。&lt;/li>
&lt;/ul>
 &lt;/blockquote>
&lt;p>这两个模块可以根据任务的需求而单独使用：&lt;/p></description></item><item><title>Agents 框架</title><link>https://eveydyw.github.io/blogs/agents/</link><pubDate>Wed, 01 Nov 2023 14:26:34 +0800</pubDate><guid>https://eveydyw.github.io/blogs/agents/</guid><description>&lt;p>&lt;strong>LLM Powered Autonomous Agents&lt;/strong>&lt;/p>
&lt;p>在基于 LLM 驱动的 agent 体系里, LLM 作为agent的大脑，加其他 3个能力作为补充：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>规划能力&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>
&lt;p>子目标与拆解: Agent能够把大的任务拆解为更小的，可管理的子任务，实现复杂任务的高效处理。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>自省与改进: Agent能基于过去的行动做自我批评和自我改进，从过去的错误中学习从而改进未来的动作，从而能够改善最终的结果&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>记忆&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>
&lt;p>感官记忆：原始输入&lt;/p>
&lt;/li>
&lt;li>
&lt;p>短期记忆：in-context learning&lt;/p></description></item><item><title>PythonTools</title><link>https://eveydyw.github.io/blogs/pythontools/</link><pubDate>Wed, 25 Oct 2023 13:49:14 +0800</pubDate><guid>https://eveydyw.github.io/blogs/pythontools/</guid><description>&lt;h1 id="tools" class="heading">
 Tools&lt;a href="#tools" aria-labelledby="tools">&lt;svg class="svg-inline--fa fas fa-link anchor" fill="currentColor" aria-hidden="true" role="img" viewBox="0 0 640 512">&lt;use href="#fas-link">&lt;/use>&lt;/svg>&lt;/a>
&lt;/h1>&lt;h2 id="docstring-parser" class="heading">
 Docstring Parser&lt;a href="#docstring-parser" aria-labelledby="docstring-parser">&lt;svg class="svg-inline--fa fas fa-link anchor" fill="currentColor" aria-hidden="true" role="img" viewBox="0 0 640 512">&lt;use href="#fas-link">&lt;/use>&lt;/svg>&lt;/a>
&lt;/h2>&lt;p>
 








 



 


&lt;a href="https://rr-.github.io/docstring_parser/" class="markdown-link" >docstring_parser&lt;/a>&lt;/p></description></item><item><title>自信息&amp;互信息&amp;熵</title><link>https://eveydyw.github.io/blogs/%E8%87%AA%E4%BF%A1%E6%81%AF%E4%BA%92%E4%BF%A1%E6%81%AF%E7%86%B5/</link><pubDate>Mon, 31 Jul 2023 19:27:15 +0800</pubDate><guid>https://eveydyw.github.io/blogs/%E8%87%AA%E4%BF%A1%E6%81%AF%E4%BA%92%E4%BF%A1%E6%81%AF%E7%86%B5/</guid><description>&lt;h2 id="自信息" class="heading">
 自信息&lt;a href="#%e8%87%aa%e4%bf%a1%e6%81%af" aria-labelledby="自信息">&lt;svg class="svg-inline--fa fas fa-link anchor" fill="currentColor" aria-hidden="true" role="img" viewBox="0 0 640 512">&lt;use href="#fas-link">&lt;/use>&lt;/svg>&lt;/a>
&lt;/h2>&lt;p>在信息论中，&lt;strong>
 








 



 


&lt;a href="https://zh.wikipedia.org/wiki/%E8%87%AA%E4%BF%A1%E6%81%AF" class="markdown-link" >自信息（self-information）&lt;/a>&lt;/strong>，由克劳德·香农提出。&lt;strong>自信息&lt;/strong> 指的是当我们接收到一个消息时所获得的信息量。&lt;/p>
&lt;p>具体来说，对于一个事件，它的 &lt;strong>自信息&lt;/strong> 大小与其发生概率有关。它是衡量与概率空间中单个事件或离散随机变量取值相关的信息量的一种 &lt;strong>量度&lt;/strong>。&lt;/p>
&lt;p>它用信息的单位表示，例如 &lt;code>bit&lt;/code>、&lt;code>nat&lt;/code> 或是 &lt;code>hart&lt;/code>，使用哪个单位取决于在计算中使用的对数的底。&lt;/p></description></item><item><title>Python基础</title><link>https://eveydyw.github.io/blogs/python%E5%9F%BA%E7%A1%80/</link><pubDate>Thu, 10 Mar 2022 21:57:39 +0800</pubDate><guid>https://eveydyw.github.io/blogs/python%E5%9F%BA%E7%A1%80/</guid><description>&lt;h2 id="赋值浅-copy深-copy" class="heading">
 赋值、浅 copy、深 copy&lt;a href="#%e8%b5%8b%e5%80%bc%e6%b5%85-copy%e6%b7%b1-copy" aria-labelledby="赋值浅-copy深-copy">&lt;svg class="svg-inline--fa fas fa-link anchor" fill="currentColor" aria-hidden="true" role="img" viewBox="0 0 640 512">&lt;use href="#fas-link">&lt;/use>&lt;/svg>&lt;/a>
&lt;/h2>&lt;ul>
&lt;li>
&lt;p>&lt;strong>赋值&lt;/strong>：相当于多贴了一个标签（引用），指向同一个对象，引用计数 +1。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>浅拷贝&lt;/strong>：会开辟新的内存地址存储 &lt;strong>被拷贝对象的外层对象&lt;/strong>，但是 &lt;strong>不拷贝内层的对象&lt;/strong>，不能算一个完整的拷贝副本。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>深拷贝&lt;/strong>：会开辟新的内存地址存储被拷贝对象的外层对象，同时 &lt;strong>对于内层对象也会递归拷贝&lt;/strong>，即是一个完整的拷贝副本。&lt;/p></description></item></channel></rss>
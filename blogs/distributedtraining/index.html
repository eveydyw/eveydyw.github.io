<!doctype html><html lang=en class=no-js><head><script src=/js/critical.bundle.min.85a7f5f23dc031d38877ecdb71b00d49e8a62c54f1ba98e75a734706ea09f30a.js integrity="sha256-haf18j3AMdOId+zbcbANSeimLFTxupjnWnNHBuoJ8wo=" crossorigin=anonymous></script><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=generator content="Hugo 0.143.1"><meta name=theme content="Hinode 0.29.3"><link rel=stylesheet href="/css/main.min.87520833af13a26c9eab0db4b8b9e6c7afddf5dd51d32505d34d673a925cb9af.css" integrity="sha256-h1IIM68Tomyeqw20uLnmx6/d9d1R0yUF001nOpJcua8=" crossorigin=anonymous><link rel=preload href=/fonts/inter-v12-latin-regular.woff2 as=font type=font/woff2 crossorigin><meta name=robots content="index, follow"><meta name=googlebot content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1"><meta name=bingbot content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1"><title>EV's Blog - 分布式训练</title>
<meta name=description content="DP & DDP，TP，PP，ZeRO，混合精度，通讯"><meta property="og:locale" content="en_US"><meta property="og:type" content="article"><meta property="og:title" content="分布式训练"><meta property="og:description" content="DP & DDP，TP，PP，ZeRO，混合精度，通讯"><meta property="og:url" content="https://eveydyw.github.io/blogs/distributedtraining/"><meta property="og:site_name" content="EV's Blog"><meta property="article:published_time" content="2023-12-18T23:17:24+08:00"><meta property="article:modified_time" content="2023-12-18T23:17:24+08:00"><meta property="og:image" content="https://eveydyw.github.io/img/logo1280x640.png"><meta property="og:image:alt" content="分布式训练"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="分布式训练"><meta name=twitter:description content="DP & DDP，TP，PP，ZeRO，混合精度，通讯"><meta name=twitter:image content="https://eveydyw.github.io/img/logo1280x640.png"><meta name=twitter:image:alt content="分布式训练"><script type=application/ld+json>{"@context":"https://schema.org","@graph":[{"@type":"Organization","@id":"https://eveydyw.github.io/#/schema/organization/1","name":"Hinode","url":"https://eveydyw.github.io/","sameAs":[null,"https://github.com/gethinode/hinode"],"logo":{"@type":"ImageObject","@id":"https://eveydyw.github.io/#/schema/image/1","url":"https://eveydyw.github.io/img/logo512x512.png","width":512,"height":512,"caption":"Hinode"},"image":{"@id":"https://eveydyw.github.io/#/schema/image/1"}},{"@type":"WebSite","@id":"https://eveydyw.github.io/#/schema/website/1","url":"https://eveydyw.github.io/","name":"EV\u0027s Blog","description":"Hinode is a clean documentation and blog theme for your Hugo site based on Bootstrap 5.","publisher":{"@id":"https://eveydyw.github.io/#/schema/organization/1"}},{"@type":"WebPage","@id":"https://eveydyw.github.io/blogs/distributedtraining/","url":"https://eveydyw.github.io/blogs/distributedtraining/","name":"分布式训练","description":"DP \u0026 DDP，TP，PP，ZeRO，混合精度，通讯","isPartOf":{"@id":"https://eveydyw.github.io/#/schema/website/1"},"about":{"@id":"https://eveydyw.github.io/#/schema/organization/1"},"datePublished":"2023-12-18T23:17:24CET","dateModified":"2023-12-18T23:17:24CET","breadcrumb":{"@id":"https://eveydyw.github.io/blogs/distributedtraining/#/schema/breadcrumb/1"},"primaryImageOfPage":{"@id":"https://eveydyw.github.io/blogs/distributedtraining/#/schema/image/2"},"inLanguage":"en-US","potentialAction":[{"@type":"ReadAction","target":["https://eveydyw.github.io/blogs/distributedtraining/"]}]},{"@type":"BreadcrumbList","@id":"https://eveydyw.github.io/blogs/distributedtraining/#/schema/breadcrumb/1","name":"Breadcrumbs","itemListElement":[{"@type":"ListItem","position":1,"item":{"@type":"WebPage","@id":"https://eveydyw.github.io/","url":"https://eveydyw.github.io/","name":"Home"}},{"@type":"ListItem","position":2,"item":{"@type":"WebPage","@id":"https://eveydyw.github.io/blogs/","url":"https://eveydyw.github.io/blogs/","name":"Blogs"}},{"@type":"ListItem","position":3,"item":{"@id":"https://eveydyw.github.io/blogs/distributedtraining/"}}]},{"@context":"https://schema.org","@graph":[{"@type":"ImageObject","@id":"https://eveydyw.github.io/blogs/distributedtraining/#/schema/image/2","url":"https://eveydyw.github.io/img/logo1280x640.png","contentUrl":"https://eveydyw.github.io/img/logo1280x640.png","caption":"分布式训练"}]}]}</script><link rel=icon type=image/png sizes=16x16 href=/img/my_logo_hu_10729e67805acfd8.png><link rel=icon type=image/png sizes=32x32 href=/img/my_logo_hu_f267cd4735c3fb50.png><link rel=icon type=image/png sizes=48x48 href=/img/my_logo_hu_bc348718a5ba4099.png><link rel=apple-touch-icon sizes=180x180 href=/img/my_logo_hu_a8f52fe79483cdab.png><script>MathJax={loader:{load:["[tex]/html","[tex]/ams","[tex]/amscd"]},tex:{inlineMath:[["\\(","\\)"],["$","$"]],displayMath:[["\\[","\\]"],["$$","$$"]],processEscapes:!0,packages:["base","ams","amscd"],tags:"ams"},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"],renderActions:{addMenu:[0,"",""],checkLoading:[0,"",""]}},chtml:{scale:1,displayAlign:"center",displayIndent:"0em",lineWidth:"container"},svg:{fontCache:"global"}}</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-svg.js></script></head><body><div class="d-flex flex-column min-vh-100"><div class="d-flex flex-column"><div class="container-fluid fixed-top p-0"><nav class="navbar p-4 bg-body navbar-fixed-top navbar-expand-md"><div class="container-xxl p-0"><div class="d-flex navbar-container justify-content-center"><div class="d-flex align-items-center"><button class="navbar-toggler collapsed p-0 mx-auto invisible fw-30" type=button><svg class="svg-inline--fa fas fa-ellipsis fa-fw" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 448 512"><use href="#fas-ellipsis"/></svg></button></div><div class=mx-auto><a class=navbar-brand href=/ aria-label=Home><img src=/img/my_logo.svg alt="EV's Blog logo" height=30 width=30></a></div><div class="d-flex align-items-center"><button class="navbar-toggler main-nav-toggler collapsed p-0" type=button data-bs-toggle=collapse data-bs-target=#navbar-0-collapse aria-controls=navbar-0 aria-expanded=false aria-label="Toggle main navigation">
<span class="toggler-icon top-bar emphasis"></span>
<span class="toggler-icon middle-bar emphasis"></span>
<span class="toggler-icon bottom-bar emphasis"></span></button></div></div><div class="navbar-collapse collapse" id=navbar-0-collapse><div class="d-flex flex-fill ms-md-3 mt-4 mt-md-0"><form class="search flex-fill position-relative me-auto"><input class="search-input form-control is-search" type=search placeholder="Search this site" aria-label="Search this site" autocomplete=off name=search-input><div class="search-suggestions shadow bg-body rounded d-none" data-no-results="No results for"></div></form></div><ul class="navbar-nav ms-auto"><li class=nav-item><a class=nav-link data-nav=main data-nav-main=home href=/><span>Home</span>&nbsp;</a></li><li class=nav-item><a class=nav-link data-nav=main data-nav-main=blogs href=/blogs/><span>Blogs</span>&nbsp;</a></li><li class=nav-item><a class=nav-link data-nav=main data-nav-main=tags href=/tags><span>Tags</span>&nbsp;</a></li><li class="d-flex mode-switch align-items-center" id=navbar-mode><input type=checkbox class="checkbox navbar-mode-selector" id=navbar-mode-checkbox aria-label="Toggle theme">
<label class=label for=navbar-mode-checkbox><svg class="svg-inline--fa fas fa-sun fa-fw" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 512 512"><use href="#fas-sun"/></svg><svg class="svg-inline--fa fas fa-moon fa-fw" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 384 512"><use href="#fas-moon"/></svg><div class=ball></div></label></li></ul></div></div></nav></div><div class=main-content></div></div><div class="container-xxl flex-fill p-4 px-xxl-0"><div class="row row-cols-1 row-cols-md-2 row-cols-lg-3"><div class="col col-lg-2 d-none d-lg-block sidebar-overflow sticky-top pt-5"></div><div class="col-12 col-md-9 col-lg-8 mb-5 p-4"><nav aria-label=breadcrumb class=d-sm-none><ol class=breadcrumb><li class=breadcrumb-item><a href=/blogs/><svg class="svg-inline--fa fas fa-angle-left" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 320 512"><use href="#fas-angle-left"/></svg>&nbsp;&nbsp;Blogs</a></li></ol></nav><nav aria-label=breadcrumb class="d-none d-sm-block"><ol class=breadcrumb><li class=breadcrumb-item><a href=/>Home</a></li><li class=breadcrumb-item><a href=/blogs/>Blogs</a></li><li class="breadcrumb-item active" aria-current=page>分布式训练</li></ol></nav><p class="display-4 mt-5">分布式训练</p><small class="text-body-secondary text-uppercase">Posted on December 18, 2023
&bull;
7&nbsp;min read &bull;
1,309&nbsp;words</small><p class="lead mb-5 mt-3">DP & DDP，TP，PP，ZeRO，混合精度，通讯</p><div class="d-md-none pb-5"><div class="d-grid gap-2 mx-auto"><a aria-label="On this page" href=#toc-collapse class="btn btn-outline-secondary position-relative toc-button" data-bs-toggle=collapse aria-expanded=false aria-controls=toc-collapse role=button><span class="d-flex justify-content-between"><span class=my-auto>On this page</span><span class="align-self-center ps-1"><svg class="svg-inline--fa fas fa-sort" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 320 512"><use href="#fas-sort"/></svg></span></span></a></div><div class="collapse border bg-body-tertiary rounded p-1 navbar-nav-scroll" id=toc-collapse><small><div class="toc toc-panel text-body p-2"><a class="toc-item toc-level-1" href=/blogs/distributedtraining/#数据并行dp--ddp>数据并行(DP & DDP) </a><a class="toc-item toc-level-2" href=/blogs/distributedtraining/#dataparallel>DataParallel </a><a class="toc-item toc-level-2" href=/blogs/distributedtraining/#distributeddataparallel>DistributedDataParallel </a><a class="toc-item toc-level-2" href=/blogs/distributedtraining/#ddp-与-dp-的区别>DDP 与 DP 的区别 </a><a class="toc-item toc-level-1" href=/blogs/distributedtraining/#tp-tensor-parallelism>TP (Tensor Parallelism) </a><a class="toc-item toc-level-2" href=/blogs/distributedtraining/#mlp-的并行化>MLP 的并行化 </a><a class="toc-item toc-level-2" href=/blogs/distributedtraining/#self-attention-的并行化>Self-Attention 的并行化 </a><a class="toc-item toc-level-1" href=/blogs/distributedtraining/#pp-pipeline-parallelism>PP (Pipeline Parallelism) </a><a class="toc-item toc-level-2" href=/blogs/distributedtraining/#朴素-pp-方案>朴素 PP 方案 </a><a class="toc-item toc-level-2" href=/blogs/distributedtraining/#pp>PP </a><a class="toc-item toc-level-1" href=/blogs/distributedtraining/#zero>ZeRO </a><a class="toc-item toc-level-2" href=/blogs/distributedtraining/#显存占用>显存占用 </a><a class="toc-item toc-level-2" href=/blogs/distributedtraining/#zero-dp-model-states>ZeRO-DP （Model States） </a><a class="toc-item toc-level-2" href=/blogs/distributedtraining/#zero-rresidual-states>ZeRO-R（Residual States） </a><a class="toc-item toc-level-2" href=/blogs/distributedtraining/#zero-offload>ZeRO-Offload </a><a class="toc-item toc-level-1" href=/blogs/distributedtraining/#混合精度>混合精度 </a><a class="toc-item toc-level-2" href=/blogs/distributedtraining/#半精度>半精度 </a><a class="toc-item toc-level-2" href=/blogs/distributedtraining/#混合精度训练mixed-precision>混合精度训练（Mixed Precision </a><a class="toc-item toc-level-2" href=/blogs/distributedtraining/#损失放大-loss-scale>损失放大 Loss Scale </a><a class="toc-item toc-level-1" href=/blogs/distributedtraining/#通讯>通讯 </a><a class="toc-item toc-level-2" href=/blogs/distributedtraining/#broadcast>Broadcast </a><a class="toc-item toc-level-2" href=/blogs/distributedtraining/#scatter>Scatter </a><a class="toc-item toc-level-2" href=/blogs/distributedtraining/#gather>Gather </a><a class="toc-item toc-level-2" href=/blogs/distributedtraining/#reduce>Reduce </a><a class="toc-item toc-level-2" href=/blogs/distributedtraining/#all-reduce>All-reduce </a><a class="toc-item toc-level-2" href=/blogs/distributedtraining/#ring-all-reduce>Ring-All-Reduce</a></div></small></div></div><div class=content><h2 id=数据并行dp--ddp class=heading>数据并行(DP & DDP)<a href=#%e6%95%b0%e6%8d%ae%e5%b9%b6%e8%a1%8cdp--ddp aria-labelledby=数据并行dp--ddp><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h2><h3 id=dataparallel class=heading>DataParallel<a href=#dataparallel aria-labelledby=dataparallel><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h3><p><strong>DP</strong> 是较简单的一种数据并行方式，直接将模型复制到多个 GPU 上并行计算，每个 GPU 计算 batch 中的一部分数据，各自完成前向和反向后，将梯度汇总到主 GPU 上。</p><hr><p><strong>基本流程</strong>：</p><ol><li><p>加载模型、数据至内存；</p></li><li><p>创建 DP 模型；</p></li><li><p>DP 模型的 forward 过程：</p><ol><li><p>一个 batch 的数据均分到不同 device 上；</p></li><li><p>为每个 device 复制一份模型；</p></li><li><p>至此，每个 device 上有模型和一份数据，并行进行前向传播；</p></li><li><p>收集各个 device 上的输出；</p></li></ol></li><li><p>每个 device 上的模型反向传播后，<strong>收集梯度到主 device 上</strong>，更新主 device 上的模型，将<strong>模型广播</strong>到其他 device 上；</p></li><li><p>3-4 循环</p></li></ol><hr><ul><li><p>只有<strong>一个主进程</strong>，主进程下有<strong>多个线程</strong></p></li><li><p><strong>每个线程管理一个 device 的训练</strong>。</p></li><li><p><strong>DP 中内存中只存在一份数据</strong>，各个线程间<strong>共享数据</strong>。DP 和 Parameter Server 的方式很像。</p></li></ul><h3 id=distributeddataparallel class=heading>DistributedDataParallel<a href=#distributeddataparallel aria-labelledby=distributeddataparallel><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h3><p><strong>基本流程</strong></p><ol><li><p>准备阶段</p><ul><li><p>环境初始化：在各张卡上初始化进程并建立进程间通信，对应代码：<code>init_process_group</code>。</p></li><li><p>模型广播：<strong>将模型 parameter、buffer 广播到各节点</strong>，对应代码：<code>model = DDP(model).to(local_rank)</code>。</p></li><li><p>创建管理器 reducer，给每个参数注册梯度平均 hook。</p></li></ul></li><li><p>准备数据</p><ul><li>加载数据集，创建适用于分布式场景的数据采样器，以防不同节点使用的数据重叠。</li></ul></li></ol><hr><p><strong>训练阶段</strong></p><ul><li><p>前向传播</p><ul><li>同步各进程状态（parameter 和 buffer）；</li><li>当 DDP 参数 <code>find_unused_parameter</code> 为 <code>true</code> 时，其会在 <code>forward</code> 结束时，启动一个回溯，标记未用到的参数，提前将这些设置为 <code>ready</code>。</li></ul></li><li><p>计算梯度</p><ul><li><p>reducer 外面：</p><ul><li>各进程各自开始反向计算梯度；</li><li>当某个参数的梯度计算好了，其之前注册的 grad hook 就会触发，在 reducer 里把这个参数的状态标记为 <code>ready</code>；</li></ul></li><li><p>reducer 里面：</p><ul><li>当某个 bucket 的所有参数都是 <code>ready</code> 时，reducer 开始对这个 bucket 的所有参数开始一个异步的 all-reduce 梯度平均操作；</li><li>当所有 bucket 的梯度平均都结束后，reducer 把得到的平均梯度正式写入到 <code>parameter.grad</code> 里。</li></ul></li></ul></li><li><p>优化器应用梯度更新参数。</p></li></ul><h3 id=ddp-与-dp-的区别 class=heading>DDP 与 DP 的区别<a href=#ddp-%e4%b8%8e-dp-%e7%9a%84%e5%8c%ba%e5%88%ab aria-labelledby=ddp-与-dp-的区别><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h3><table class=table><thead><tr><th></th><th>DP</th><th>DDP</th></tr></thead><tbody><tr><td></td><td><strong>多线程</strong><br>1. 受到 GIL 的限制<br>2. 单机工作</td><td><strong>多进程</strong><br>1. 多机多卡</td></tr><tr><td><strong>迭代更新</strong></td><td>传输数据包括 <strong>梯度和参数</strong><br>1. 全程维护 <strong>一个 optimizer</strong><br>2 <strong>梯度</strong>汇总到主 GPU, 主 GPU 进行参数更新<br>3. 主 GPU Broadcast <strong>参数</strong> 给其他的 GPU</td><td>传输数据包括 <strong>梯度</strong><br>1. 每个进程具有 <strong>自己的 optimizer</strong><br>2. 各进程自己计算梯度<br>3. Ring All-Reduce 将 <strong>梯度</strong> 汇总平均<br>4. 各进程用梯度来独立的更新参数</td></tr><tr><td><strong>通信效率</strong></td><td>通信成本随着 GPU 数量线性增长</td><td><strong>Ring All-Reduce</strong> 通信成本恒定，与 GPU 数量无关</td></tr></tbody></table><p>DDP 中由于各进程中的模型，初始参数一致 (初始时刻进行一次 broadcast)，而每次用于更新参数的梯度也一致，因此，<strong>各进程的模型参数始终保持一致。</strong></p><h2 id=tp-tensor-parallelism class=heading>TP (Tensor Parallelism)<a href=#tp-tensor-parallelism aria-labelledby=tp-tensor-parallelism><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h2><p><a href=https://arxiv.org/abs/1909.08053 class=markdown-link>Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism</a></p><p>每个张量都被 <strong>水平</strong> 分成多个块，因此张量的每个分片都位于其指定的 GPU 上，而不是让整个张量驻留在单个 GPU 上。在处理过程中，每个分片在不同的 GPU 上分别并行处理，结果在步骤结束时同步。</p><h3 id=mlp-的并行化 class=heading>MLP 的并行化<a href=#mlp-%e7%9a%84%e5%b9%b6%e8%a1%8c%e5%8c%96 aria-labelledby=mlp-的并行化><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h3><center><img style="border-radius:.3125em;box-shadow:0 .2rem .4rem rgba(34,36,38,.12);margin:1rem" src=pics/image-20250331195513596.png width=55% alt></center><ol><li><p>对于输入
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">X</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mo stretchy="false">(</mo><mi>B</mi><mo>×</mo><mi>L</mi><mo stretchy="false">)</mo><mo>×</mo><mi>D</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf{X} \in \mathbb{R}^{(B\times L) \times D}</annotation></semantics></math></span>
</span>，它的行数是<strong>批量大小
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi></mrow><annotation encoding="application/x-tex">B</annotation></semantics></math></span>
</span></strong>乘以<strong>序列长度
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi></mrow><annotation encoding="application/x-tex">L</annotation></semantics></math></span>
</span></strong>，列数是<strong>隐藏层的宽度即
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span>
</span></strong>。</p><p>为了方便，令
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">B=1</annotation></semantics></math></span>
</span>，即
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">X</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>L</mi><mo>×</mo><mi>D</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf{X} \in \mathbb{R}^{L \times D}</annotation></semantics></math></span></span></p></li><li><p>MLP 模块里面其实就是两个全连接层</p><ul><li><p>假定第一个隐藏层的权重是
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">A</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>D</mi><mo>×</mo><msup><mi>D</mi><mo mathvariant="normal">′</mo></msup></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf A \in \mathbb{R}^{D\times D^\prime}</annotation></semantics></math></span>
</span>(
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>D</mi><mo mathvariant="normal">′</mo></msup></mrow><annotation encoding="application/x-tex">D^\prime</annotation></semantics></math></span>
</span>一般是
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span>
</span>的
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn></mrow><annotation encoding="application/x-tex">4</annotation></semantics></math></span>
</span>倍)，则先做矩阵乘法，然后再接一个激活函数比如 <code>GELU</code></p></li><li><p>假定第二个隐藏层的权重是
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">B</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><msup><mi>D</mi><mo mathvariant="normal">′</mo></msup><mo>×</mo><mi>D</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf B \in \mathbb{R}^{D^\prime \times D}</annotation></semantics></math></span>
</span>，最终得到
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">Z</mi><mo>=</mo><mi>σ</mi><mo stretchy="false">(</mo><mi mathvariant="bold">X</mi><mo>⋅</mo><mi mathvariant="bold">A</mi><mo stretchy="false">)</mo><mi mathvariant="bold">B</mi></mrow><annotation encoding="application/x-tex">\mathbf Z = \sigma(\mathbf X \cdot \mathbf A) \mathbf B</annotation></semantics></math></span></span></p></li></ul></li><li><p><strong>为了保证每个数据的完整，避免GPU 之间的通讯</strong>：</p><ul><li>对
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">A</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>D</mi><mo>×</mo><msup><mi>D</mi><mo mathvariant="normal">′</mo></msup></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf A \in \mathbb{R}^{D\times D^\prime}</annotation></semantics></math></span>
</span>按
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>D</mi><mo mathvariant="normal">′</mo></msup></mrow><annotation encoding="application/x-tex">D^\prime</annotation></semantics></math></span>
</span>所在的那一维作拆分（按行切），此时
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">X</mi></mrow><annotation encoding="application/x-tex">\mathbf{X}</annotation></semantics></math></span>
</span><strong>不需要拆分</strong>，直接复制保证每个GPU上都有即可</li><li>对
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">B</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><msup><mi>D</mi><mo mathvariant="normal">′</mo></msup><mo>×</mo><mi>D</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf B \in \mathbb{R}^{D^\prime \times D}</annotation></semantics></math></span>
</span>按
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>D</mi><mo mathvariant="normal">′</mo></msup></mrow><annotation encoding="application/x-tex">D^\prime</annotation></semantics></math></span>
</span>所在的那一维作拆分（按列切）。</li></ul></li><li><p><strong>将
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">A</mi></mrow><annotation encoding="application/x-tex">\mathbf A</annotation></semantics></math></span>
</span>按行拆分成
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span>
</span>份</strong>：
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">A</mi><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msub><mi mathvariant="bold">A</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>⋯</mo><mtext> </mtext><mo separator="true">,</mo><msub><mi mathvariant="bold">A</mi><mi>n</mi></msub></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf A= \begin{bmatrix}\mathbf A_1,\cdots, \mathbf A_n \end{bmatrix}</annotation></semantics></math></span>
</span>，其中
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">A</mi><mi>i</mi></msub><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>D</mi><mo>×</mo><mfrac><msup><mi>D</mi><mo mathvariant="normal">′</mo></msup><mi>n</mi></mfrac></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf A_i \in \mathbb{R}^{D \times \frac{D^\prime}{n}}</annotation></semantics></math></span>
</span>。通过执行矩阵乘法得到:</p><div class=mathjax-display><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="bold">X</mi><mo>⋅</mo><mi mathvariant="bold">A</mi><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi mathvariant="bold">X</mi><msub><mi mathvariant="bold">A</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>⋯</mo><mtext> </mtext><mo separator="true">,</mo><mi mathvariant="bold">X</mi><msub><mi mathvariant="bold">A</mi><mi>n</mi></msub></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><mo separator="true">,</mo><mspace width="1em"/><mi mathvariant="bold">X</mi><msub><mi mathvariant="bold">A</mi><mi>i</mi></msub><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>L</mi><mo>×</mo><mfrac><msup><mi>D</mi><mo mathvariant="normal">′</mo></msup><mi>n</mi></mfrac></mrow></msup></mrow><annotation encoding="application/x-tex">
    \mathbf X \cdot \mathbf A = \begin{bmatrix}\mathbf X \mathbf A_1,\cdots, \mathbf X\mathbf A_n \end{bmatrix} , \quad \mathbf X \mathbf A_i \in \mathbb{R}^{L\times \frac{D^\prime}{n}}
    </annotation></semantics></math></span></div><p>它们可以独立输入<code>GeLU</code>：</p><div class=mathjax-display><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msub><mi mathvariant="bold">Y</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>⋯</mo><mtext> </mtext><mo separator="true">,</mo><msub><mi mathvariant="bold">Y</mi><mi>n</mi></msub></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi mathvariant="normal">GeLU</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><mi mathvariant="bold">X</mi><msub><mi mathvariant="bold">A</mi><mn>1</mn></msub><mo fence="true">)</mo></mrow><mo separator="true">,</mo><mo>⋯</mo><mtext> </mtext><mo separator="true">,</mo><mi mathvariant="normal">GeLU</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><mi mathvariant="bold">X</mi><msub><mi mathvariant="bold">A</mi><mi>n</mi></msub><mo fence="true">)</mo></mrow></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><mo separator="true">,</mo><mspace width="1em"/><msub><mi mathvariant="bold">Y</mi><mi>i</mi></msub><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>L</mi><mo>×</mo><mfrac><msup><mi>D</mi><mo mathvariant="normal">′</mo></msup><mi>n</mi></mfrac></mrow></msup></mrow><annotation encoding="application/x-tex">
    \begin{bmatrix}\mathbf Y_1,\cdots, \mathbf Y_n\end{bmatrix} = 
    \begin{bmatrix}\operatorname{GeLU}\left(\mathbf X \mathbf A_1\right),\cdots, \operatorname{GeLU} \left(\mathbf X\mathbf A_n \right)\end{bmatrix} , \quad \mathbf Y_i \in \mathbb{R}^{L\times \frac{D^\prime}{n}}
    </annotation></semantics></math></span></div></li><li><p><strong>将
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">B</mi></mrow><annotation encoding="application/x-tex">\mathbf B</annotation></semantics></math></span>
</span>按列拆分成
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span>
</span>份</strong>：
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">B</mi><mo>=</mo><msup><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msub><mi mathvariant="bold">B</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>⋯</mo><mtext> </mtext><mo separator="true">,</mo><msub><mi mathvariant="bold">B</mi><mi>n</mi></msub></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><mi mathvariant="normal">⊤</mi></msup></mrow><annotation encoding="application/x-tex">\mathbf B=  \begin{bmatrix}\mathbf B_1,\cdots, \mathbf B_n \end{bmatrix}^{\top}</annotation></semantics></math></span>
</span>，其中
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">B</mi><mi>i</mi></msub><mo>∈</mo><msup><mi mathvariant="bold">R</mi><mrow><mfrac><msup><mi>D</mi><mo mathvariant="normal">′</mo></msup><mi>n</mi></mfrac><mo>×</mo><mi>D</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf B_i \in \mathbf{R}^{\frac{D^\prime}{n}\times D}</annotation></semantics></math></span>
</span>。通过执行矩阵乘法得到</p><div class=mathjax-display><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="bold">Z</mi><mo>=</mo><munderover><mo>∑</mo><mi>i</mi><mi>n</mi></munderover><msub><mi mathvariant="bold">Z</mi><mi>i</mi></msub><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msub><mi mathvariant="bold">Y</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>⋯</mo><mtext> </mtext><mo separator="true">,</mo><msub><mi mathvariant="bold">Y</mi><mi>n</mi></msub></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><msup><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msub><mi mathvariant="bold">B</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>⋯</mo><mtext> </mtext><mo separator="true">,</mo><msub><mi mathvariant="bold">B</mi><mi>n</mi></msub></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><mi mathvariant="normal">⊤</mi></msup><mo separator="true">,</mo><mspace width="1em"/><mi mathvariant="bold">Z</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>L</mi><mo>×</mo><mi>D</mi></mrow></msup></mrow><annotation encoding="application/x-tex">
    \mathbf Z =\sum_i^n\mathbf Z_i  = \begin{bmatrix}\mathbf Y_1,\cdots, \mathbf Y_n\end{bmatrix} \begin{bmatrix}\mathbf B_1,\cdots, \mathbf B_n \end{bmatrix}^{\top} , \quad \mathbf Z \in \mathbb{R}^{L\times D}
    </annotation></semantics></math></span></div></li><li><p>通过上述操作，我们可以更新任意深度的 MLP，只需在每个 <strong>拆列-拆行</strong> 序列之后同步 GPU</p></li></ol><h3 id=self-attention-的并行化 class=heading>Self-Attention 的并行化<a href=#self-attention-%e7%9a%84%e5%b9%b6%e8%a1%8c%e5%8c%96 aria-labelledby=self-attention-的并行化><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h3><center><img style="border-radius:.3125em;box-shadow:0 .2rem .4rem rgba(34,36,38,.12);margin:1rem" src=pics/image-20250331195651320.png width=55% alt></center><p><strong>各个头各自计算</strong></p><ol><li><p>对于输入
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">X</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mo stretchy="false">(</mo><mi>B</mi><mo>×</mo><mi>L</mi><mo stretchy="false">)</mo><mo>×</mo><mi>D</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf{X} \in \mathbb{R}^{(B\times L) \times D}</annotation></semantics></math></span>
</span>，它的行数是<strong>批量大小
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi></mrow><annotation encoding="application/x-tex">B</annotation></semantics></math></span>
</span></strong>乘以<strong>序列长度
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi></mrow><annotation encoding="application/x-tex">L</annotation></semantics></math></span>
</span></strong>，列数是<strong>隐藏层的宽度即
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span>
</span></strong>。</p><p>为了方便，令
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>B</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">B=1</annotation></semantics></math></span>
</span>，即
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">X</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>L</mi><mo>×</mo><mi>D</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf{X} \in \mathbb{R}^{L \times D}</annotation></semantics></math></span>
</span>。</p><p>在自注意力机制中，输入
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">X</mi></mrow><annotation encoding="application/x-tex">\mathbf{X}</annotation></semantics></math></span>
</span>会被复制成三份，分别对应为
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">X</mi></mrow><annotation encoding="application/x-tex">\mathbf{X}</annotation></semantics></math></span>
</span>的
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">Q</mi></mrow><annotation encoding="application/x-tex">\mathbf Q</annotation></semantics></math></span>
</span>、
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">K</mi></mrow><annotation encoding="application/x-tex">\mathbf K</annotation></semantics></math></span>
</span>、
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">V</mi></mrow><annotation encoding="application/x-tex">\mathbf V</annotation></semantics></math></span>
</span>向量矩阵。</p></li><li><p>对于多头注意力，头的维度为
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mi>D</mi><mi>h</mi></mfrac></mrow><annotation encoding="application/x-tex">\frac{D}{h}</annotation></semantics></math></span>
</span>, 假定
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">h=2</annotation></semantics></math></span>
</span>，之后针对每个头中输入
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">X</mi></mrow><annotation encoding="application/x-tex">\mathbf{X}</annotation></semantics></math></span>
</span>矩阵中各个单词的
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">Q</mi></mrow><annotation encoding="application/x-tex">\mathbf Q</annotation></semantics></math></span>
</span>向量，会与各自上下文的
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">K</mi></mrow><annotation encoding="application/x-tex">\mathbf K</annotation></semantics></math></span>
</span>向量做缩放点积然后做 <code>Softmax</code> 得到一个注意力分数或权重，之后再与
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">V</mi></mrow><annotation encoding="application/x-tex">\mathbf V</annotation></semantics></math></span>
</span>相乘，得到一个
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo>×</mo><mfrac><mi>D</mi><mi>h</mi></mfrac></mrow><annotation encoding="application/x-tex">L \times \frac{D}{h}</annotation></semantics></math></span>
</span>的输出</p></li><li><p>每个头的计算是各自独立并行的，那意味着一个头可以放在 GPU
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">0</annotation></semantics></math></span>
</span>上，另一个头可以放在 GPU
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn></mrow><annotation encoding="application/x-tex">1</annotation></semantics></math></span>
</span>上，最后 <code>all reduce</code> 每个头的结果</p></li></ol><blockquote class=blockquote><p>由于前向和后向传播中每层都有
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn></mrow><annotation encoding="application/x-tex">2</annotation></semantics></math></span>
</span>个 <code>all reduce</code>( MLP+Self-Attention )，<strong>因此 TP 需要设备间有非常快速的互联</strong>。</p><p><strong>因此，不建议跨多个节点进行 TP</strong>。</p><p>如果节点有
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn></mrow><annotation encoding="application/x-tex">4</annotation></semantics></math></span>
</span>个 GPU，则最高 TP 度设为
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn></mrow><annotation encoding="application/x-tex">4</annotation></semantics></math></span>
</span>比较好。如果需要 TP 度为
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>8</mn></mrow><annotation encoding="application/x-tex">8</annotation></semantics></math></span>
</span>，则需要使用至少有
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>8</mn></mrow><annotation encoding="application/x-tex">8</annotation></semantics></math></span>
</span>个 GPU 的节点。</p></blockquote><h2 id=pp-pipeline-parallelism class=heading>PP (Pipeline Parallelism)<a href=#pp-pipeline-parallelism aria-labelledby=pp-pipeline-parallelism><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h2><p><a href=https://arxiv.org/pdf/1811.06965 class=markdown-link>GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism</a></p><p>模型在多个 GPU 上 <strong>垂直 (即按层)</strong> 拆分:</p><ul><li>因此只有一个或多个模型层放置在单个 GPU 上。</li><li>每个 GPU 并行处理流水线的不同阶段，并处理 batch 的一部分数据。</li></ul><center><img style="border-radius:.3125em;box-shadow:0 .2rem .4rem rgba(34,36,38,.12);margin:1rem" src=pics/image-20250331195931082.png width=55% alt></center><p>把网络分成
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn></mrow><annotation encoding="application/x-tex">4</annotation></semantics></math></span>
</span>块，每一块放在一个 GPU 上(<strong>不同的颜色表示不同的 GPU</strong>)，于是就有了
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>F</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">F_0</annotation></semantics></math></span>
</span>、
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>F</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">F_1</annotation></semantics></math></span>
</span>、
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>F</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">F_2</annotation></semantics></math></span>
</span>、
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>F</mi><mn>3</mn></msub></mrow><annotation encoding="application/x-tex">F_3</annotation></semantics></math></span>
</span>这
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn></mrow><annotation encoding="application/x-tex">4</annotation></semantics></math></span>
</span>个前向路径和
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>B</mi><mn>3</mn></msub></mrow><annotation encoding="application/x-tex">B_3</annotation></semantics></math></span>
</span>、
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>B</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">B_2</annotation></semantics></math></span>
</span>、
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>B</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">B_1</annotation></semantics></math></span>
</span>、
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>B</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">B_0</annotation></semantics></math></span>
</span>逆序后向路径。</p><center><img style="border-radius:.3125em;box-shadow:0 .2rem .4rem rgba(34,36,38,.12);margin:1rem" src=pics/image-20250331200018768.png width=15% alt></center><h3 id=朴素-pp-方案 class=heading>朴素 PP 方案<a href=#%e6%9c%b4%e7%b4%a0-pp-%e6%96%b9%e6%a1%88 aria-labelledby=朴素-pp-方案><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h3><p>在每个时间点只有一台设备在处理计算逻辑，完成计算后将结果发送给下一台设备。</p><center><img style="border-radius:.3125em;box-shadow:0 .2rem .4rem rgba(34,36,38,.12);margin:1rem" src=pics/image-20250331200123924.png width=28% alt></center><h3 id=pp class=heading>PP<a href=#pp aria-labelledby=pp><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h3><center><img style="border-radius:.3125em;box-shadow:0 .2rem .4rem rgba(34,36,38,.12);margin:1rem" src=pics/image-20250331200635330.png width=40% alt></center><p><code>PP</code> 引入了一个新的超参数来调整，称为 <strong>块 (chunks)</strong>。它定义了通过同一管级按顺序发送多少数据块。图中
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>chunks</mtext><mo>=</mo><mn>4</mn></mrow><annotation encoding="application/x-tex">\text{chunks} = 4</annotation></semantics></math></span>
</span>。</p><ol><li><p>GPU
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">0</annotation></semantics></math></span>
</span>在 chunk
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">0</annotation></semantics></math></span>
</span>、
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn></mrow><annotation encoding="application/x-tex">1</annotation></semantics></math></span>
</span>、
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn></mrow><annotation encoding="application/x-tex">2</annotation></semantics></math></span>
</span>和
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn></mrow><annotation encoding="application/x-tex">3</annotation></semantics></math></span>
</span>(
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>F</mi><mrow><mn>0</mn><mo separator="true">,</mo><mn>0</mn></mrow></msub></mrow><annotation encoding="application/x-tex">F_{0,0}</annotation></semantics></math></span>
</span>、
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>F</mi><mrow><mn>0</mn><mo separator="true">,</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">F_{0,1}</annotation></semantics></math></span>
</span>、
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>F</mi><mrow><mn>0</mn><mo separator="true">,</mo><mn>2</mn></mrow></msub></mrow><annotation encoding="application/x-tex">F_{0,2}</annotation></semantics></math></span>
</span>、
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>F</mi><mrow><mn>0</mn><mo separator="true">,</mo><mn>3</mn></mrow></msub></mrow><annotation encoding="application/x-tex">F_{0,3}</annotation></semantics></math></span>
</span>) 上执行相同的前向路径，然后等待。</p><p>等其他 GPU 完成工作后，GPU
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">0</annotation></semantics></math></span>
</span>会再次开始工作，为块
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn></mrow><annotation encoding="application/x-tex">3</annotation></semantics></math></span>
</span>、
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn></mrow><annotation encoding="application/x-tex">2</annotation></semantics></math></span>
</span>、
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn></mrow><annotation encoding="application/x-tex">1</annotation></semantics></math></span>
</span>和
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">0</annotation></semantics></math></span>
</span>(
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>B</mi><mrow><mn>0</mn><mo separator="true">,</mo><mn>3</mn></mrow></msub></mrow><annotation encoding="application/x-tex">B_{0,3}</annotation></semantics></math></span>
</span>、
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>B</mi><mrow><mn>0</mn><mo separator="true">,</mo><mn>2</mn></mrow></msub></mrow><annotation encoding="application/x-tex">B_{0,2}</annotation></semantics></math></span>
</span>、
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>B</mi><mrow><mn>0</mn><mo separator="true">,</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">B_{0,1}</annotation></semantics></math></span>
</span>、
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>B</mi><mrow><mn>0</mn><mo separator="true">,</mo><mn>0</mn></mrow></msub></mrow><annotation encoding="application/x-tex">B_{0,0}</annotation></semantics></math></span>
</span>) 执行后向路径。</p><p>请注意，从概念上讲，这与<strong>梯度累积 (gradient accumulation steps，<code>GAS</code>)</strong> 的意思相同。<code>PyTorch</code> 叫它<strong>chunks</strong>，而 DeepSpeed 叫它 <strong><code>GAS</code></strong> 。</p><p><strong>梯度累积（Gradient Accumulation）</strong> 的主要思想是在计算一个批次的梯度后不立刻更新模型参数，而是累积几个批次后再更新，这样便可以在不增加显存消耗的情况下模拟更大的批次。</p></li><li><p>因为 <strong>块 (chunks）</strong>，<code>PP</code> 引入了 <strong>micro-batches (<code>MBS</code>)</strong> 的概念。</p><ul><li><p><code>DP</code> 将全局 batch size 拆分为小 batch size。</p><p>如果
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>dp_degree</mtext><mo>=</mo><mn>4</mn></mrow><annotation encoding="application/x-tex">\text{dp\_degree} = 4</annotation></semantics></math></span>
</span>，则全局
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext>batch_size</mtext><mtext>all</mtext></msub><mo>=</mo><mn>1024</mn></mrow><annotation encoding="application/x-tex">\text{batch\_size}_{\text{all}}=1024</annotation></semantics></math></span>
</span>将拆分为
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn></mrow><annotation encoding="application/x-tex">4</annotation></semantics></math></span>
</span>个小 batch size，每个小batch有
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext>batch_size</mtext><mtext>dp</mtext></msub><mo>=</mo><mn>1024</mn><mi mathvariant="normal">/</mi><mn>4</mn><mo>=</mo><mn>256</mn></mrow><annotation encoding="application/x-tex">\text{batch\_size}_{\text{dp}}=1024/4 = 256</annotation></semantics></math></span>
</span>。</p></li><li><p>如果
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>chunks</mtext><mo>=</mo><mn>32</mn></mrow><annotation encoding="application/x-tex">\text{chunks} = 32</annotation></semantics></math></span>
</span>，最终得到的
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>micro batch_size</mtext><mo>=</mo><mn>256</mn><mi mathvariant="normal">/</mi><mn>32</mn><mo>=</mo><mn>8</mn></mrow><annotation encoding="application/x-tex">\text{micro batch\_size} = 256/32= 8</annotation></semantics></math></span>
</span>。</p></li><li><p>每个管级一次处理一个 <strong><code>micro batch</code></strong>。</p></li><li><p>计算 <code>DP</code> + <code>PP</code> 设置的全局批量大小的公式为:
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>mbs</mtext><mo>∗</mo><mtext>chunks</mtext><mo>∗</mo><mtext>dp_degree </mtext><mo stretchy="false">(</mo><mn>8</mn><mo>∗</mo><mn>32</mn><mo>∗</mo><mn>4</mn><mo>=</mo><mn>1024</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex"> \text{mbs}*\text{chunks}*\text{dp\_degree }(8*32*4=1024)</annotation></semantics></math></span></span></p></li></ul></li><li><p>将 <code>mini-batch</code> 进一步划分成更小的 <code>micro-batch</code>，同时利用 pipipline 方案，每次处理一个 <code>micro-batch</code> 的数据，得到结果后，将该 <code>micro-batch</code> 的结果发送给下游设备，同时开始处理后一个 <code>micro-batch</code> 的数据，通过这套方案减小设备中的 Bubble(<strong>设备空闲的时间称为 Bubble)</strong></p></li></ol><h2 id=zero class=heading>ZeRO<a href=#zero aria-labelledby=zero><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h2><p><a href=https://arxiv.org/abs/1910.02054 class=markdown-link>ZeRO: Memory Optimizations Toward Training Trillion Parameter Models</a></p><p>数据并行会产生大量冗余 Model States 的空间占用。每个 GPU 都需要存储大语言模型的相同副本，包括模型参数和优化器参数等。但是对于每个 GPU，在模型传播到某一层时，其他层的模型和优化器参数并不参与计算，这导致了严重的显存冗余现象。</p><p><strong><code>ZeRO</code> 的本质，是在数据并行的基础上，对冗余空间占用进行深度优化</strong>。</p><p><strong><code>ZeRO</code></strong> 仅<strong>在每个 GPU 上保留部分模型参数和优化器参数，当需要时再从其它 GPU 中读取</strong>进行计算，使用完之后便可以释放相应显存。</p><h3 id=显存占用 class=heading>显存占用<a href=#%e6%98%be%e5%ad%98%e5%8d%a0%e7%94%a8 aria-labelledby=显存占用><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h3><p>大规模训练中的显存占用可以分为 <strong>Model States</strong> 与 <strong>Residual states</strong> 两部分：</p><hr><p><strong>Model States</strong></p><ol><li><p><strong>Optimizer States</strong></p><p>Optimizer States 是 Optimizer 在进行梯度更新时所需要用到的数据，例如 SGD 中的 Momentum 以及使用混合精度训练时的 Float32 Master Parameters</p></li><li><p><strong>Gradient</strong></p><p>在反向传播后所产生的梯度信息，其决定了参数的更新方向。</p></li><li><p><strong>Model Parameter</strong></p><p>模型参数，也就是我们在整个过程中通过数据“学习”的信息</p></li></ol><p>在传统DDP下，每个进程都使用同样参数来进行训练。每个进程也会持有对 Optimizer States 的完整拷贝，同样占用了大量显存。</p><p>在混合精度场景下，设模型参数量为
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">Φ</mi></mrow><annotation encoding="application/x-tex"> \mathbf\Phi</annotation></semantics></math></span>
</span>, 那么梯度的元素数量为
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">Φ</mi></mrow><annotation encoding="application/x-tex">\mathbf\Phi</annotation></semantics></math></span>
</span>，<strong>模型参数（fp16）</strong>、<strong>模型梯度（fp16）</strong> 和 <strong>优化器状态（fp32</strong>）<strong>总占用</strong> 显存：</p><div class=mathjax-display><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mo stretchy="false">(</mo><mn>2</mn><mo>+</mo><mn>2</mn><mo>+</mo><mi>K</mi><mo stretchy="false">)</mo><mi mathvariant="bold">Φ</mi></mrow><annotation encoding="application/x-tex">
(2 +2+K)\mathbf\Phi
</annotation></semantics></math></span></div><hr><p><strong>Residual States</strong></p><p>除了模型状态之外的显存占用，包括 <strong>激活值（activation）</strong>、各种临时缓冲区（buffer）以及无法使用的显存碎片（fragmentation）。</p><hr><h3 id=zero-dp-model-states class=heading>ZeRO-DP （Model States）<a href=#zero-dp-model-states aria-labelledby=zero-dp-model-states><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h3><center><img style="border-radius:.3125em;box-shadow:0 .2rem .4rem rgba(34,36,38,.12);margin:1rem" src=pics/image-20250331230303181.png width=60% alt></center><p>ZeRO 有三个不同级别，分别对应对 <strong>Model States 不同程度的分割 (Paritition)</strong>，图中的
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext>P</mtext><mtext>os</mtext></msub></mrow><annotation encoding="application/x-tex">\text{P}_{\text{os}}</annotation></semantics></math></span>
</span>、
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext>P</mtext><mrow><mtext>os</mtext><mo>+</mo><mtext>g</mtext></mrow></msub></mrow><annotation encoding="application/x-tex">\text{P}_{\text{os}+\text{g}}</annotation></semantics></math></span>
</span>、
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext>P</mtext><mrow><mtext>os</mtext><mo>+</mo><mtext>g</mtext><mo>+</mo><mtext>p</mtext></mrow></msub></mrow><annotation encoding="application/x-tex">\text{P}_{\text{os}+\text{g}+\text{p}}</annotation></semantics></math></span>
</span>分别代表 <strong>ZeRO-1</strong>、<strong>ZeRO-2</strong>、<strong>ZeRO-3</strong></p><ol><li><p><strong>ZeRO-1 [
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext>P</mtext><mtext>os</mtext></msub></mrow><annotation encoding="application/x-tex">\text{P}_{\text{os}}</annotation></semantics></math></span>
</span>]：</strong> <strong>分割 Optimizer States</strong></p><p>模型参数（parameters）和梯度（gradients）仍旧是每张卡保持一份，此时，每张卡的模型状态所需显存是
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mi mathvariant="bold">Φ</mi><mo>+</mo><mn>2</mn><mi mathvariant="bold">Φ</mi><mo>+</mo><mfrac><mrow><mi>K</mi><mo>⋅</mo><mi mathvariant="bold">Φ</mi></mrow><msub><mi>N</mi><mi>d</mi></msub></mfrac></mrow><annotation encoding="application/x-tex">2\mathbf\Phi+2\mathbf\Phi+ \frac{K \cdot \mathbf\Phi}{N_d}</annotation></semantics></math></span>
</span>，当
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span>
</span>比较大时，趋向于
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn><mi mathvariant="bold">Φ</mi></mrow><annotation encoding="application/x-tex">4\mathbf\Phi</annotation></semantics></math></span>
</span>。</p></li><li><p><strong>ZeRO-2 [
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext>P</mtext><mrow><mtext>os</mtext><mo>+</mo><mtext>g</mtext></mrow></msub></mrow><annotation encoding="application/x-tex">\text{P}_{\text{os}+\text{g}}</annotation></semantics></math></span>
</span>]：</strong> <strong>分割 Optimizer States 与 Gradients</strong></p><p>继续对模型<strong>梯度</strong>进行分片，模型参数仍旧是每张卡保持一份，此时，每张卡的模型状态所需显存是
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mi mathvariant="bold">Φ</mi><mo>+</mo><mfrac><mrow><mo stretchy="false">(</mo><mn>2</mn><mo>+</mo><mi>K</mi><mo stretchy="false">)</mo><mo>⋅</mo><mi mathvariant="bold">Φ</mi></mrow><msub><mi>N</mi><mi>d</mi></msub></mfrac></mrow><annotation encoding="application/x-tex">2\mathbf\Phi+ \frac{(2+K) \cdot \mathbf\Phi}{N_d}</annotation></semantics></math></span>
</span>，当
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span>
</span>比较大时，趋向于
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mi mathvariant="bold">Φ</mi></mrow><annotation encoding="application/x-tex">2\mathbf\Phi</annotation></semantics></math></span>
</span>。</p></li><li><p><strong>ZeRO-3 [
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext>P</mtext><mrow><mtext>os</mtext><mo>+</mo><mtext>g</mtext><mo>+</mo><mtext>p</mtext></mrow></msub></mrow><annotation encoding="application/x-tex">\text{P}_{\text{os}+\text{g}+\text{p}}</annotation></semantics></math></span>
</span>]：</strong> <strong>分割 Optimizer States、Gradients 与 Parameters</strong></p><p>继续对模型<strong>参数</strong>进行分片，此时每张卡的模型状态所需显存是
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mo stretchy="false">(</mo><mn>2</mn><mo>+</mo><mn>2</mn><mo>+</mo><mi>K</mi><mo stretchy="false">)</mo><mo>⋅</mo><mi mathvariant="bold">Φ</mi></mrow><msub><mi>N</mi><mi>d</mi></msub></mfrac></mrow><annotation encoding="application/x-tex">\frac{(2+2+K) \cdot  \mathbf\Phi}{N_d}</annotation></semantics></math></span>
</span>，当
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span>
</span>比较大时，趋向于
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">0</annotation></semantics></math></span>
</span>。</p></li></ol><p><strong>ZeRO-1 和 ZeRO-2 并不会带来额外的通讯</strong>，但 <strong>ZeRO-3</strong> 每一次要算
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">W</mi></mrow><annotation encoding="application/x-tex">\mathbf W</annotation></semantics></math></span>
</span>的时候，都得去别的机器拿回来，相当于带来了额外的通讯(增加了
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>50</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">50\%</annotation></semantics></math></span>
</span>)</p><h4 id=zero-vs-模型并行 class=heading>ZeRO V.S. 模型并行<a href=#zero-vs-%e6%a8%a1%e5%9e%8b%e5%b9%b6%e8%a1%8c aria-labelledby=zero-vs-模型并行><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h4><p><strong><code>ZeRO</code> 是模型并行的形式，数据并行的实质</strong>。</p><ul><li><p>模型并行，是指在 forward 和 backward 的过程中，只需要用自己维护的那块
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">W</mi></mrow><annotation encoding="application/x-tex">\mathbf W</annotation></semantics></math></span>
</span>来计算。</p><p>即 <strong>同样的输入 X，每块 GPU 上各算模型的一部分，最后通过某些方式聚合结果</strong>。</p></li><li><p><strong><code>ZeRO</code></strong> 做 forward 和 backward 的时候，需要把各 GPU 上维护的
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">W</mi></mrow><annotation encoding="application/x-tex">\mathbf W</annotation></semantics></math></span>
</span>聚合起来。</p><p>即 <strong>本质上还是用完整的 W 进行计算</strong>。<strong>它是不同的输入
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">X</mi></mrow><annotation encoding="application/x-tex">\mathbf X</annotation></semantics></math></span>
</span>，完整的参数
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">W</mi></mrow><annotation encoding="application/x-tex">\mathbf W</annotation></semantics></math></span>
</span>，最终再做聚合</strong>。</p></li></ul><h3 id=zero-rresidual-states class=heading>ZeRO-R（Residual States）<a href=#zero-rresidual-states aria-labelledby=zero-rresidual-states><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h3><ul><li><p><strong><span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mi>α</mi></msub></mrow><annotation encoding="application/x-tex">P_\alpha</annotation></semantics></math></span>
</span>: Partitioned Activation Checkpointing</strong></p><p>activation 起到加速梯度计算的作用。</p><p>使用分片方法，并且配合 checkpointing，可以灵活设置 activation的存储。每块 GPU 上只维护部分的 activation，需要时再从别的地方聚合过来就行。需要注意的是，activation 对显存的占用一般会远高于模型本身，通讯量也是巨大的，所以这块要灵活、有效地实验设计。</p></li><li><p><strong><span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>C</mi><mi>B</mi></msub></mrow><annotation encoding="application/x-tex">C_B</annotation></semantics></math></span>
</span>: Constant Size Buffer 临时缓冲区</strong></p><p>模型训练过程中经常会创建一些大小不等的临时缓冲区，比如对梯度进行 AllReduce。</p><p><strong>解决办法为预先创建一个固定的缓冲区</strong> ，训练过程中不再动态创建，如果要传输的数据较小，则多组数据 bucket 后再一次性传输，提高效率</p><p>固定大小的内存 buffer，它的目的在于：</p><ul><li><p><strong>提升带宽利用率</strong>。当 GPU 数量上升，GPU 间的通讯次数也上升，每次的通讯量可能下降（但总通讯量不会变）。数据切片小了，就不能很好利用带宽了。所以这个 buffer 起到了积攒数据的作用：等数据积攒到一定大小，再进行通讯。</p></li><li><p><strong>使得存储大小可控</strong>。在每次通讯前，积攒的存储大小是常量，是已知可控的。更方便使用者对训练中的存储消耗和通讯时间进行预估。</p></li></ul></li><li><p><strong><span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>M</mi><mi>D</mi></msub></mrow><annotation encoding="application/x-tex">M_D</annotation></semantics></math></span>
</span>: Memory Defragmentation 显存碎片</strong></p><p>显存出现碎片的一大原因是 gradient checkpointing 后，不断地创建和销毁那些不保存的激活值。</p><p><strong>解决方法是预先分配一块连续的显存</strong>，将常驻显存的模型状态和 checkpointed activation 存在里面，剩余显存用于动态创建和销毁 discarded activation。</p><p>设置机制，对碎片化的存储空间进行重新整合，整出连续的存储空间。防止出现总存储足够，但连续存储不够而引起的存储请求 fail。</p></li></ul><h3 id=zero-offload class=heading>ZeRO-Offload<a href=#zero-offload aria-labelledby=zero-offload><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h3><ul><li><p><strong>forward 和 backward 计算量高</strong>，因此和它们相关的部分，例如参数
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">W</mi></mrow><annotation encoding="application/x-tex">\mathbf W</annotation></semantics></math></span>
</span>（fp16），activation，就全放入 GPU。</p></li><li><p><strong>update 的部分计算量低</strong>，因此和它相关的部分，全部放入 CPU 中。例如 参数
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">W</mi></mrow><annotation encoding="application/x-tex">\mathbf W</annotation></semantics></math></span>
</span>(fp32)，optimizer states（fp32）和 gradients(fp16)等。</p></li></ul><h2 id=混合精度 class=heading>混合精度<a href=#%e6%b7%b7%e5%90%88%e7%b2%be%e5%ba%a6 aria-labelledby=混合精度><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h2><p><a href=https://arxiv.org/abs/1710.03740 class=markdown-link>Mixed Precision Training</a></p><p>混合精度训练，指代的是<strong>单精度 float</strong>（
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>32</mn></mrow><annotation encoding="application/x-tex">32</annotation></semantics></math></span>
</span>bit，
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn></mrow><annotation encoding="application/x-tex">4</annotation></semantics></math></span>
</span>个字节）和<strong>半精度 float16</strong>（
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>12</mn></mrow><annotation encoding="application/x-tex">12</annotation></semantics></math></span>
</span>bit，
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn></mrow><annotation encoding="application/x-tex">2</annotation></semantics></math></span>
</span>个字节） 混合。</p><h3 id=半精度 class=heading>半精度<a href=#%e5%8d%8a%e7%b2%be%e5%ba%a6 aria-labelledby=半精度><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h3><p><strong>半精度优点</strong>：</p><ul><li><p><strong>内存占用更少：</strong> 通用的模型 fp16 占用的内存只需原来的一半：</p><ul><li>模型占用的内存更小，训练的时候可以用更大的 batchsize。</li><li>模型训练时，通信量（特别是多卡，或者多机多卡）大幅减少，大幅减少等待时间，加快数据的流通。</li></ul></li><li><p><strong>计算更快：</strong></p><ul><li>目前的不少 GPU 都有针对 fp16 的计算进行优化。论文指出：在近期的 GPU 中，半精度的计算吞吐量可以是单精度的
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn></mrow><annotation encoding="application/x-tex">2</annotation></semantics></math></span>
</span>-
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>8</mn></mrow><annotation encoding="application/x-tex">8</annotation></semantics></math></span>
</span>倍；</li></ul></li></ul><p><strong>半精度问题</strong></p><ul><li><strong>数据溢出 Overflow / Underflow</strong>：对于深度学习而言，最大的问题在于 Underflow（下溢出），在训练后期，例如激活函数的梯度会非常小， 甚至在梯度乘以学习率后，值会更加小。</li><li><strong>舍入误差 Rounding Error</strong>。</li></ul><h3 id=混合精度训练mixed-precision class=heading>混合精度训练（Mixed Precision<a href=#%e6%b7%b7%e5%90%88%e7%b2%be%e5%ba%a6%e8%ae%ad%e7%bb%83mixed-precision aria-labelledby=混合精度训练mixed-precision><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h3><p><strong>利用 fp16 进行乘法和存储，利用 fp32 来进行加法计算</strong>。这样可以减少加法过程中的舍入误差，保证精度不损失</p><ul><li><p>在模型矩阵乘法的过程中，利用 fp32 来进行矩阵乘法中间的累加(accumulated)。</p></li><li><p>然后再将 fp32 的值转化为 fp16 进行存储。</p></li></ul><center><img style="border-radius:.3125em;box-shadow:0 .2rem .4rem rgba(34,36,38,.12);margin:1rem" src=pics/image-20250331231527602.png width=60% alt></center><h4 id=fp32-权重备份 class=heading>FP32 权重备份<a href=#fp32-%e6%9d%83%e9%87%8d%e5%a4%87%e4%bb%bd aria-labelledby=fp32-权重备份><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h4><p>主要用于<strong>解决舍入误差</strong>的问题。</p><ul><li><p><strong>weights, activations, gradients 等数据</strong>在训练中都 <strong>利用 fp16 来存储</strong></p><ul><li><p>fp32 额外拷贝一份 weight 会新增加训练时候存储的占用。</p><p>实际训练过程中，内存中占据大部分的基本都是 activations 的值。特别是在 batchsize 很大的情况下， activations 更是特别占据空间。 保存 activiations 主要是为了在 back-propogation 的时候进行计算。</p><p>因此，只要 activation 的值基本都是使用 fp16 来进行存储的话，则最终模型与 fp32 相比起来， 内存占用也基本能够减半。</p></li></ul></li><li><p>拷贝一份 <strong>fp32 的 weights，用于更新</strong>。</p><ul><li><p>在更新权重的时候，
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext>weight</mtext><mi>t</mi></msub><mo>=</mo><msub><mtext>weight</mtext><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>+</mo><mtext>lr</mtext><mo>⋅</mo><mtext>gradients</mtext></mrow><annotation encoding="application/x-tex"> \text{weight}_t= \text {weight}_{t-1}+ \text{lr} \cdot \text{gradients}</annotation></semantics></math></span>
</span>，而在深度模型中，
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>lr</mtext><mo>⋅</mo><mtext>gradients</mtext></mrow><annotation encoding="application/x-tex"> \text{lr} \cdot \text{gradients}</annotation></semantics></math></span>
</span>往往非常小，如果利用 fp16 来进行相加的话， 则很可能会出现 <strong>舍入误差 Rounding Error</strong>，导致更新无效。</p></li><li><p>通过将 weights 拷贝成 fp32 格式，并且确保整个更新（update）过程在 fp32 格式下进行</p></li></ul></li></ul><h3 id=损失放大-loss-scale class=heading>损失放大 Loss Scale<a href=#%e6%8d%9f%e5%a4%b1%e6%94%be%e5%a4%a7-loss-scale aria-labelledby=损失放大-loss-scale><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h3><p>主要用于<strong>解决 fp16 underflow 的问题</strong>。</p><p>训练到了后期，梯度（特别是激活函数平滑段的梯度）会特别小，fp16 表示容易产生 underflow 现象。</p><blockquote class=blockquote><p><strong>Loss Scale</strong></p><p><strong>对计算出来的 loss 值进行 scale</strong>，由于链式法则的存在，loss 上的 scale 也会作用在梯度上。这样比起对每个梯度进行 scale 更加划算。 scaled 过后的梯度，就会平移到 fp16 有效的展示范围内。</p><ol><li><strong>反向传播前</strong> ，将损失变化（dLoss）手动增大
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mn>2</mn><mi>k</mi></msup></mrow><annotation encoding="application/x-tex">2^k</annotation></semantics></math></span>
</span>倍，因此反向传播时得到的中间变量（激活函数梯度）则不会溢出；</li><li><strong>反向传播后</strong>，将权重梯度缩
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mn>2</mn><mi>k</mi></msup></mrow><annotation encoding="application/x-tex">2^k</annotation></semantics></math></span>
</span>倍，恢复正常值。</li></ol></blockquote><p>这样，scaled-gradient 就可以一直使用 fp16 进行存储了。<strong>只有在进行更新的时候，才会将 scaled-gradient 转化为 fp32，同时将 scale 抹去</strong>。</p><h2 id=通讯 class=heading>通讯<a href=#%e9%80%9a%e8%ae%af aria-labelledby=通讯><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h2><h3 id=broadcast class=heading>Broadcast<a href=#broadcast aria-labelledby=broadcast><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h3><p><strong>Broadcast</strong> 把同一份数据分发广播给所有人。</p><center><img style="border-radius:.3125em;box-shadow:0 .2rem .4rem rgba(34,36,38,.12);margin:1rem" src=pics/Broadcasting.png width=45% alt></center><h3 id=scatter class=heading>Scatter<a href=#scatter aria-labelledby=scatter><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h3><p><strong>Scatter</strong> 将不同数据分发给不同的进程。</p><center><img style="border-radius:.3125em;box-shadow:0 .2rem .4rem rgba(34,36,38,.12);margin:1rem" src=pics/scatter.png width=45% alt></center><h3 id=gather class=heading>Gather<a href=#gather aria-labelledby=gather><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h3><p><strong>Gather</strong> 函数是反向的 <strong>Scatter</strong> ，即收集所有进程发送向 root 进程的数据。</p><center><img style="border-radius:.3125em;box-shadow:0 .2rem .4rem rgba(34,36,38,.12);margin:1rem" src=pics/Gathering.png width=45% alt></center><h3 id=reduce class=heading>Reduce<a href=#reduce aria-labelledby=reduce><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h3><p><strong>Reduce</strong> 将多个进程中的数据按照指定的映射函数<strong>进行运算</strong>，得到最后的结果存在一个进程中。</p><p>在下图中，左边每个进程包含一个整数。 调用 <code>MPI_Reduce</code> 的 root 进程为
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">0</annotation></semantics></math></span>
</span>，并使用 <code>MPI_SUM</code> 作 reduction 运算。 这
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn></mrow><annotation encoding="application/x-tex">4</annotation></semantics></math></span>
</span>个数字相加后将结果存储在 root 进程中。</p><p>右边的每个进程都有
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn></mrow><annotation encoding="application/x-tex">2</annotation></semantics></math></span>
</span>个元素。 结果求和基于每个元素进行。 即将每个数组中的第
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span>
</span>个元素累加到进程
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">0</annotation></semantics></math></span>
</span>的结果数组中的第
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span>
</span>个元素中。</p><center><img style="border-radius:.3125em;box-shadow:0 .2rem .4rem rgba(34,36,38,.12);margin:.5rem" src=pics/mpi_reduce_1.png width=45% alt>
<img style="border-radius:.3125em;box-shadow:0 .2rem .4rem rgba(34,36,38,.12);margin:.5rem" src=pics/mpi_reduce_2.png width=45% alt></center><h3 id=all-reduce class=heading>All-reduce<a href=#all-reduce aria-labelledby=all-reduce><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h3><p><strong>All-reduce</strong> 与 <strong>Reduce</strong> 的区别就在于后者最后的结果是只保存在一个进程中，而 All-reduce 需要每个进程都有同样的结果。</p><p>所以 <strong>All-reduce 一般包含 Scatter 操作</strong>，所以有时候也会看到 <strong>reduce-scatter</strong> 这种说法，其实 <strong>reduce-scatter</strong> 可以看成是 all reduce 的一种实现方式。</p><center><img style="border-radius:.3125em;box-shadow:0 .2rem .4rem rgba(34,36,38,.12);margin:1rem" src=pics/mpi_allreduce_1.png width=45% alt></center><h3 id=ring-all-reduce class=heading>Ring-All-Reduce<a href=#ring-all-reduce aria-labelledby=ring-all-reduce><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h3><p>All-Reduce 可以有多种实现方法，目前主流的实现方法是基于 Ring 的方式。总的来说，Ring-AllReduce 可以分为 <strong>reduce-scatter</strong> 和 <strong>all-gather</strong> 两部分。</p><p>这里假设有
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn></mrow><annotation encoding="application/x-tex">3</annotation></semantics></math></span>
</span>张显卡，逻辑拓扑结构为一个环。此外，每块显卡中的数据都被切分为
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn></mrow><annotation encoding="application/x-tex">3</annotation></semantics></math></span>
</span>块。<strong>每个显卡都会从红色的数据块开始，然后沿着箭头的方向进行传递和累积。这个逻辑在后续的 reduce-scatter 和 all-gather 中完全相同。</strong></p><p>在
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span>
</span>个显卡上将数据分成
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span>
</span>块，第
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span>
</span>个显卡以第
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span>
</span>块为起始，经过
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">n-1</annotation></semantics></math></span>
</span>步完成 reduce-scatter 或者 all-gather。</p><center><img style="border-radius:.3125em;box-shadow:0 .2rem .4rem rgba(34,36,38,.12);margin:1rem" src=pics/79756639b3c16c71cacd76df2b512431.png width=45% alt></center><h4 id=reduce-scatter class=heading>Reduce-scatter<a href=#reduce-scatter aria-labelledby=reduce-scatter><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h4><p>各个显卡从红色的数据开始传输，经过 <strong><span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn></mrow><annotation encoding="application/x-tex">2</annotation></semantics></math></span>
</span>步</strong> 后，reduce 的结果存储在了绿色的位置。</p><center><img style="border-radius:.3125em;box-shadow:0 .2rem .4rem rgba(34,36,38,.12);margin:1rem" src=pics/d178490d828ad550ce214efe9ef71cfa.png width=45% alt></center><h4 id=all-gather class=heading>All-gather<a href=#all-gather aria-labelledby=all-gather><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h4><p>经过 reduce-scatter 后，reduce 的数据分布在绿色的数据块上。all-gather 从绿色的数据块开始，经过 <strong><span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn></mrow><annotation encoding="application/x-tex">2</annotation></semantics></math></span>
</span>步</strong> 后，所有的显卡都有了完整的 reduce 结果。</p><center><img style="border-radius:.3125em;box-shadow:0 .2rem .4rem rgba(34,36,38,.12);margin:1rem" src=pics/b2136519fb3afe4d9cbf42afdc6fd663.png width=45% alt></center><h4 id=ring-allreduce-通讯量分析 class=heading>Ring-AllReduce 通讯量分析<a href=#ring-allreduce-%e9%80%9a%e8%ae%af%e9%87%8f%e5%88%86%e6%9e%90 aria-labelledby=ring-allreduce-通讯量分析><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h4><p>假设模型参数 W 的大小为
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">Φ</mi></mrow><annotation encoding="application/x-tex">\mathbf\Phi </annotation></semantics></math></span>
</span>，GPU 个数为
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span>
</span>。则梯度大小也为
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">Φ</mi></mrow><annotation encoding="application/x-tex">\mathbf\Phi </annotation></semantics></math></span>
</span>，每个梯度块的大小为
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mi mathvariant="bold">Φ</mi><mi>N</mi></mfrac></mrow><annotation encoding="application/x-tex">\frac{\mathbf\Phi}{N} </annotation></semantics></math></span>
</span>，对单卡 GPU 来说（只算其 send 通讯量）：</p><ul><li><strong>Reduce-Scatter 阶段</strong>，通讯量为
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo><mfrac><mi mathvariant="bold">Φ</mi><mi>N</mi></mfrac></mrow><annotation encoding="application/x-tex">(N-1)\frac{\mathbf\Phi}{N} </annotation></semantics></math></span></span></li><li><strong>All-Gather 阶段</strong>，通讯量为
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo><mfrac><mi mathvariant="bold">Φ</mi><mi>N</mi></mfrac></mrow><annotation encoding="application/x-tex">(N-1)\frac{\mathbf\Phi}{N} </annotation></semantics></math></span></span></li></ul><p><strong>单卡总通讯量为
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo stretchy="false">(</mo><mi>N</mi><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo><mfrac><mi mathvariant="bold">Φ</mi><mi>N</mi></mfrac></mrow><annotation encoding="application/x-tex">2(N-1)\frac{\mathbf\Phi}{N} </annotation></semantics></math></span>
</span>，随着
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span>
</span>的增大，可以近似为
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mi mathvariant="bold">Φ</mi></mrow><annotation encoding="application/x-tex">2\mathbf\Phi</annotation></semantics></math></span>
</span>。</strong> 全卡总通讯量为
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mi>N</mi><mi mathvariant="bold">Φ</mi></mrow><annotation encoding="application/x-tex">2N\mathbf\Phi</annotation></semantics></math></span>
</span>，通讯量均衡负载到了每一时刻的每个 Worker 上。</p><h1 id=reference class=heading>Reference<a href=#reference aria-labelledby=reference><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h1><ul><li><p><a href=https://arxiv.org/abs/1909.08053 class=markdown-link>Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism</a></p></li><li><p><a href=https://arxiv.org/pdf/1811.06965 class=markdown-link>GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism</a></p></li><li><p><a href=https://arxiv.org/abs/1910.02054 class=markdown-link>ZeRO: Memory Optimizations Toward Training Trillion Parameter Models</a></p></li><li><p><a href=https://arxiv.org/abs/1710.03740 class=markdown-link>Mixed Precision Training</a></p></li><li><p><a href=https://python-parallel-programmning-cookbook.readthedocs.io/zh-cn/latest/chapter3/14_Collective_communication_using_broadcast.html class=markdown-link>python-parallel-programmning-cookbook</a></p></li><li><p><a href=https://mpitutorial.com/tutorials/mpi-reduce-and-allreduce/zh_cn/ class=markdown-link>MPI Reduce and Allreduce</a></p></li></ul></div><div class="row row-cols-2 mt-5 mb-3"><div class=col><a class=next href=/blogs/transformer/><svg class="svg-inline--fa fas fa-arrow-left" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 448 512"><use href="#fas-arrow-left"/></svg>&nbsp;Transformer</a></div><div class="col text-end"><a class=previous href=/blogs/agents/>Agents 框架&nbsp;<svg class="svg-inline--fa fas fa-arrow-right" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 448 512"><use href="#fas-arrow-right"/></svg></a></div></div><a href=# id=back-to-top class=back-to-top><span>▲</span>
</a><a href=# id=scroll-to-bottom class=back-to-top><span>▼</span>
</a><script>document.addEventListener("DOMContentLoaded",function(){var e=document.getElementById("back-to-top"),t=document.getElementById("scroll-to-bottom");e.addEventListener("click",function(e){e.preventDefault(),window.scrollTo({top:0,behavior:"smooth"})}),t.addEventListener("click",function(e){e.preventDefault(),window.scrollTo({top:document.body.scrollHeight,behavior:"smooth"})})})</script></div><div class="col col-md-3 col-lg-2 d-none d-md-block pt-5"><div class="toc toc-sidebar mb-5 my-md-0 mb-lg-5 p-3 text-body-secondary sticky-top"><strong class="d-block h6 my-2 pt-4">On this page:</strong><nav class=toc><a class="toc-item toc-level-1" href=/blogs/distributedtraining/#数据并行dp--ddp>数据并行(DP & DDP) </a><a class="toc-item toc-level-2" href=/blogs/distributedtraining/#dataparallel>DataParallel </a><a class="toc-item toc-level-2" href=/blogs/distributedtraining/#distributeddataparallel>DistributedDataParallel </a><a class="toc-item toc-level-2" href=/blogs/distributedtraining/#ddp-与-dp-的区别>DDP 与 DP 的区别 </a><a class="toc-item toc-level-1" href=/blogs/distributedtraining/#tp-tensor-parallelism>TP (Tensor Parallelism) </a><a class="toc-item toc-level-2" href=/blogs/distributedtraining/#mlp-的并行化>MLP 的并行化 </a><a class="toc-item toc-level-2" href=/blogs/distributedtraining/#self-attention-的并行化>Self-Attention 的并行化 </a><a class="toc-item toc-level-1" href=/blogs/distributedtraining/#pp-pipeline-parallelism>PP (Pipeline Parallelism) </a><a class="toc-item toc-level-2" href=/blogs/distributedtraining/#朴素-pp-方案>朴素 PP 方案 </a><a class="toc-item toc-level-2" href=/blogs/distributedtraining/#pp>PP </a><a class="toc-item toc-level-1" href=/blogs/distributedtraining/#zero>ZeRO </a><a class="toc-item toc-level-2" href=/blogs/distributedtraining/#显存占用>显存占用 </a><a class="toc-item toc-level-2" href=/blogs/distributedtraining/#zero-dp-model-states>ZeRO-DP （Model States） </a><a class="toc-item toc-level-2" href=/blogs/distributedtraining/#zero-rresidual-states>ZeRO-R（Residual States） </a><a class="toc-item toc-level-2" href=/blogs/distributedtraining/#zero-offload>ZeRO-Offload </a><a class="toc-item toc-level-1" href=/blogs/distributedtraining/#混合精度>混合精度 </a><a class="toc-item toc-level-2" href=/blogs/distributedtraining/#半精度>半精度 </a><a class="toc-item toc-level-2" href=/blogs/distributedtraining/#混合精度训练mixed-precision>混合精度训练（Mixed Precision </a><a class="toc-item toc-level-2" href=/blogs/distributedtraining/#损失放大-loss-scale>损失放大 Loss Scale </a><a class="toc-item toc-level-1" href=/blogs/distributedtraining/#通讯>通讯 </a><a class="toc-item toc-level-2" href=/blogs/distributedtraining/#broadcast>Broadcast </a><a class="toc-item toc-level-2" href=/blogs/distributedtraining/#scatter>Scatter </a><a class="toc-item toc-level-2" href=/blogs/distributedtraining/#gather>Gather </a><a class="toc-item toc-level-2" href=/blogs/distributedtraining/#reduce>Reduce </a><a class="toc-item toc-level-2" href=/blogs/distributedtraining/#all-reduce>All-reduce </a><a class="toc-item toc-level-2" href=/blogs/distributedtraining/#ring-all-reduce>Ring-All-Reduce</a></nav></div></div></div></div><footer class="container-fluid footer text-center p-3"><div class="container-xxl text-center"><small>Copyright © 2025 EV's Blog All rights reserved.
|
Powered by
<a href=https://gethinode.com class="link-bg-footer markdown-link">Hinode</a>.</small></div></footer></div><div id=toast-container class="toast-container position-fixed bottom-0 end-0 p-3"><div id=toast-copied-code-message class=toast role=alert aria-live=assertive aria-atomic=true><div class=toast-header><strong class=me-auto>EV's Blog</strong>
<button type=button class=btn-close data-bs-dismiss=toast aria-label=Close></button></div><div class=toast-body>Code copied to clipboard</div></div></div><svg xmlns:xlink="http://www.w3.org/1999/xlink" display="none"><symbol id="fas-ellipsis"><path d="M8 256a56 56 0 11112 0A56 56 0 118 256zm160 0a56 56 0 11112 0 56 56 0 11-112 0zm216-56a56 56 0 110 112 56 56 0 110-112z"/></symbol><symbol id="fas-sun"><path d="M361.5 1.2c5 2.1 8.6 6.6 9.6 11.9L391 121l107.9 19.8c5.3 1 9.8 4.6 11.9 9.6s1.5 10.7-1.6 15.2L446.9 256l62.3 90.3c3.1 4.5 3.7 10.2 1.6 15.2s-6.6 8.6-11.9 9.6L391 391 371.1 498.9c-1 5.3-4.6 9.8-9.6 11.9s-10.7 1.5-15.2-1.6L256 446.9l-90.3 62.3c-4.5 3.1-10.2 3.7-15.2 1.6s-8.6-6.6-9.6-11.9L121 391 13.1 371.1c-5.3-1-9.8-4.6-11.9-9.6s-1.5-10.7 1.6-15.2L65.1 256 2.8 165.7c-3.1-4.5-3.7-10.2-1.6-15.2s6.6-8.6 11.9-9.6L121 121 140.9 13.1c1-5.3 4.6-9.8 9.6-11.9s10.7-1.5 15.2 1.6L256 65.1 346.3 2.8c4.5-3.1 10.2-3.7 15.2-1.6zM160 256a96 96 0 11192 0 96 96 0 11-192 0zm224 0a128 128 0 10-256 0 128 128 0 10256 0z"/></symbol><symbol id="fas-moon"><path d="M223.5 32C1e2 32 0 132.3.0 256S1e2 480 223.5 480c60.6.0 115.5-24.2 155.8-63.4 5-4.9 6.3-12.5 3.1-18.7s-10.1-9.7-17-8.5c-9.8 1.7-19.8 2.6-30.1 2.6-96.9.0-175.5-78.8-175.5-176 0-65.8 36-123.1 89.3-153.3 6.1-3.5 9.2-10.5 7.7-17.3s-7.3-11.9-14.3-12.5c-6.3-.5-12.6-.8-19-.8z"/></symbol><symbol id="fas-angle-left"><path d="M41.4 233.4c-12.5 12.5-12.5 32.8.0 45.3l160 160c12.5 12.5 32.8 12.5 45.3.0s12.5-32.8.0-45.3L109.3 256 246.6 118.6c12.5-12.5 12.5-32.8.0-45.3s-32.8-12.5-45.3.0l-160 160z"/></symbol><symbol id="fas-sort"><path d="M137.4 41.4c12.5-12.5 32.8-12.5 45.3.0l128 128c9.2 9.2 11.9 22.9 6.9 34.9S301 224.1 288 224.1L32 224c-12.9.0-24.6-7.8-29.6-19.8s-2.2-25.7 6.9-34.9l128-128zm0 429.3-128-128c-9.2-9.2-11.9-22.9-6.9-34.9S19.1 288 32.1 288h256c12.9.0 24.6 7.8 29.6 19.8s2.2 25.7-6.9 34.9l-128 128c-12.5 12.5-32.8 12.5-45.3.0z"/></symbol><symbol id="fas-arrow-left"><path d="M9.4 233.4c-12.5 12.5-12.5 32.8.0 45.3l160 160c12.5 12.5 32.8 12.5 45.3.0s12.5-32.8.0-45.3L109.2 288H416c17.7.0 32-14.3 32-32s-14.3-32-32-32H109.3L214.6 118.6c12.5-12.5 12.5-32.8.0-45.3s-32.8-12.5-45.3.0l-160 160z"/></symbol><symbol id="fas-arrow-right"><path d="M438.6 278.6c12.5-12.5 12.5-32.8.0-45.3l-160-160c-12.5-12.5-32.8-12.5-45.3.0s-12.5 32.8.0 45.3L338.8 224H32c-17.7.0-32 14.3-32 32s14.3 32 32 32h306.7L233.4 393.4c-12.5 12.5-12.5 32.8.0 45.3s32.8 12.5 45.3.0l160-160z"/></symbol></svg>
<script src=/js/core.bundle-analytics.en.min.ceb6a67c169a28031391976dac91e1e2f460951862201b6249516a55d0fd6109.js data-category=analytics integrity="sha256-zramfBaaKAMTkZdtrJHh4vRglRhiIBtiSVFqVdD9YQk=" crossorigin=anonymous async></script><script src=/js/core.bundle.en.min.5ced88a4f77bedae238353d0ecd4735062d1de8b76e0efe9323fdf249f1e6854.js integrity="sha256-XO2IpPd77a4jg1PQ7NRzUGLR3ot24O/pMj/fJJ8eaFQ=" crossorigin=anonymous async></script></body></html>
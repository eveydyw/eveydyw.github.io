<!doctype html><html lang=en class=no-js><head><script src=/js/critical.bundle.min.85a7f5f23dc031d38877ecdb71b00d49e8a62c54f1ba98e75a734706ea09f30a.js integrity="sha256-haf18j3AMdOId+zbcbANSeimLFTxupjnWnNHBuoJ8wo=" crossorigin=anonymous></script><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=generator content="Hugo 0.143.1"><meta name=theme content="Hinode 0.29.3"><link rel=stylesheet href="/css/main.min.2691f39c75d24543daa5a297922c3a38f4e541082cd5abf6de0fb6267005047b.css" integrity="sha256-JpHznHXSRUPapaKXkiw6OPTlQQgs1av23g+2JnAFBHs=" crossorigin=anonymous><link rel=preload href=/fonts/inter-v12-latin-regular.woff2 as=font type=font/woff2 crossorigin><meta name=robots content="index, follow"><meta name=googlebot content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1"><meta name=bingbot content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1"><title>EV's Blog - Transformer</title>
<meta name=description content="Transformer 模型"><meta property="og:locale" content="en_US"><meta property="og:type" content="article"><meta property="og:title" content="Transformer"><meta property="og:description" content="Transformer 模型"><meta property="og:url" content="https://eveydyw.github.io/blogs/transformer/"><meta property="og:site_name" content="EV's Blog"><meta property="article:published_time" content="2024-03-12T19:52:32+08:00"><meta property="article:modified_time" content="2024-03-12T19:52:32+08:00"><meta property="og:image" content="https://eveydyw.github.io/img/logo1280x640.png"><meta property="og:image:alt" content="Transformer"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="Transformer"><meta name=twitter:description content="Transformer 模型"><meta name=twitter:image content="https://eveydyw.github.io/img/logo1280x640.png"><meta name=twitter:image:alt content="Transformer"><script type=application/ld+json>{"@context":"https://schema.org","@graph":[{"@type":"Organization","@id":"https://eveydyw.github.io/#/schema/organization/1","name":"Hinode","url":"https://eveydyw.github.io/","sameAs":[null,"https://github.com/gethinode/hinode"],"logo":{"@type":"ImageObject","@id":"https://eveydyw.github.io/#/schema/image/1","url":"https://eveydyw.github.io/img/logo512x512.png","width":512,"height":512,"caption":"Hinode"},"image":{"@id":"https://eveydyw.github.io/#/schema/image/1"}},{"@type":"WebSite","@id":"https://eveydyw.github.io/#/schema/website/1","url":"https://eveydyw.github.io/","name":"EV\u0027s Blog","description":"Hinode is a clean documentation and blog theme for your Hugo site based on Bootstrap 5.","publisher":{"@id":"https://eveydyw.github.io/#/schema/organization/1"}},{"@type":"WebPage","@id":"https://eveydyw.github.io/blogs/transformer/","url":"https://eveydyw.github.io/blogs/transformer/","name":"Transformer","description":"Transformer 模型","isPartOf":{"@id":"https://eveydyw.github.io/#/schema/website/1"},"about":{"@id":"https://eveydyw.github.io/#/schema/organization/1"},"datePublished":"2024-03-12T19:52:32CET","dateModified":"2024-03-12T19:52:32CET","breadcrumb":{"@id":"https://eveydyw.github.io/blogs/transformer/#/schema/breadcrumb/1"},"primaryImageOfPage":{"@id":"https://eveydyw.github.io/blogs/transformer/#/schema/image/2"},"inLanguage":"en-US","potentialAction":[{"@type":"ReadAction","target":["https://eveydyw.github.io/blogs/transformer/"]}]},{"@type":"BreadcrumbList","@id":"https://eveydyw.github.io/blogs/transformer/#/schema/breadcrumb/1","name":"Breadcrumbs","itemListElement":[{"@type":"ListItem","position":1,"item":{"@type":"WebPage","@id":"https://eveydyw.github.io/","url":"https://eveydyw.github.io/","name":"Home"}},{"@type":"ListItem","position":2,"item":{"@type":"WebPage","@id":"https://eveydyw.github.io/blogs/","url":"https://eveydyw.github.io/blogs/","name":"Blogs"}},{"@type":"ListItem","position":3,"item":{"@id":"https://eveydyw.github.io/blogs/transformer/"}}]},{"@context":"https://schema.org","@graph":[{"@type":"ImageObject","@id":"https://eveydyw.github.io/blogs/transformer/#/schema/image/2","url":"https://eveydyw.github.io/img/logo1280x640.png","contentUrl":"https://eveydyw.github.io/img/logo1280x640.png","caption":"Transformer"}]}]}</script><link rel=icon type=image/png sizes=16x16 href=/img/my_logo_hu_10729e67805acfd8.png><link rel=icon type=image/png sizes=32x32 href=/img/my_logo_hu_f267cd4735c3fb50.png><link rel=icon type=image/png sizes=48x48 href=/img/my_logo_hu_bc348718a5ba4099.png><link rel=apple-touch-icon sizes=180x180 href=/img/my_logo_hu_a8f52fe79483cdab.png><script>MathJax={loader:{load:["[tex]/html","[tex]/ams","[tex]/amscd"]},tex:{inlineMath:[["\\(","\\)"],["$","$"]],displayMath:[["\\[","\\]"],["$$","$$"]],processEscapes:!0,packages:["base","ams","amscd"],tags:"ams"},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"],renderActions:{addMenu:[0,"",""],checkLoading:[0,"",""]}},chtml:{scale:1,displayAlign:"center",displayIndent:"0em",lineWidth:"container"},svg:{fontCache:"global"}}</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-svg.js></script></head><body><div class="d-flex flex-column min-vh-100"><div class="d-flex flex-column"><div class="container-fluid fixed-top p-0"><nav class="navbar p-4 bg-body navbar-fixed-top navbar-expand-md"><div class="container-xxl p-0"><div class="d-flex navbar-container justify-content-center"><div class="d-flex align-items-center"><button class="navbar-toggler collapsed p-0 mx-auto invisible fw-30" type=button><svg class="svg-inline--fa fas fa-ellipsis fa-fw" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 448 512"><use href="#fas-ellipsis"/></svg></button></div><div class=mx-auto><a class=navbar-brand href=/ aria-label=Home><img src=/img/my_logo.svg alt="EV's Blog logo" height=30 width=30></a></div><div class="d-flex align-items-center"><button class="navbar-toggler main-nav-toggler collapsed p-0" type=button data-bs-toggle=collapse data-bs-target=#navbar-0-collapse aria-controls=navbar-0 aria-expanded=false aria-label="Toggle main navigation">
<span class="toggler-icon top-bar emphasis"></span>
<span class="toggler-icon middle-bar emphasis"></span>
<span class="toggler-icon bottom-bar emphasis"></span></button></div></div><div class="navbar-collapse collapse" id=navbar-0-collapse><div class="d-flex flex-fill ms-md-3 mt-4 mt-md-0"><form class="search flex-fill position-relative me-auto"><input class="search-input form-control is-search" type=search placeholder="Search this site" aria-label="Search this site" autocomplete=off name=search-input><div class="search-suggestions shadow bg-body rounded d-none" data-no-results="No results for"></div></form></div><ul class="navbar-nav ms-auto"><li class=nav-item><a class=nav-link data-nav=main data-nav-main=home href=/><span>Home</span>&nbsp;</a></li><li class=nav-item><a class=nav-link data-nav=main data-nav-main=blogs href=/blogs/><span>Blogs</span>&nbsp;</a></li><li class=nav-item><a class=nav-link data-nav=main data-nav-main=tags href=/tags><span>Tags</span>&nbsp;</a></li><li class="d-flex mode-switch align-items-center" id=navbar-mode><input type=checkbox class="checkbox navbar-mode-selector" id=navbar-mode-checkbox aria-label="Toggle theme">
<label class=label for=navbar-mode-checkbox><svg class="svg-inline--fa fas fa-sun fa-fw" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 512 512"><use href="#fas-sun"/></svg><svg class="svg-inline--fa fas fa-moon fa-fw" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 384 512"><use href="#fas-moon"/></svg><div class=ball></div></label></li></ul></div></div></nav></div><div class=main-content></div></div><div class="container-xxl flex-fill p-4 px-xxl-0"><div class="row row-cols-1 row-cols-md-2 row-cols-lg-3"><div class="col col-lg-2 d-none d-lg-block sidebar-overflow sticky-top pt-5"></div><div class="col-12 col-md-9 col-lg-8 mb-5 p-4"><nav aria-label=breadcrumb class=d-sm-none><ol class=breadcrumb><li class=breadcrumb-item><a href=/blogs/><svg class="svg-inline--fa fas fa-angle-left" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 320 512"><use href="#fas-angle-left"/></svg>&nbsp;&nbsp;Blogs</a></li></ol></nav><nav aria-label=breadcrumb class="d-none d-sm-block"><ol class=breadcrumb><li class=breadcrumb-item><a href=/>Home</a></li><li class=breadcrumb-item><a href=/blogs/>Blogs</a></li><li class="breadcrumb-item active" aria-current=page>Transformer</li></ol></nav><p class="display-4 mt-5">Transformer</p><small class="text-body-secondary text-uppercase">Posted on March 12, 2024
&bull;
8&nbsp;min read &bull;
1,681&nbsp;words</small><p class="lead mb-5 mt-3">Transformer 模型</p><div class="d-md-none pb-5"><div class="d-grid gap-2 mx-auto"><a aria-label="On this page" href=#toc-collapse class="btn btn-outline-secondary position-relative toc-button" data-bs-toggle=collapse aria-expanded=false aria-controls=toc-collapse role=button><span class="d-flex justify-content-between"><span class=my-auto>On this page</span><span class="align-self-center ps-1"><svg class="svg-inline--fa fas fa-sort" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 320 512"><use href="#fas-sort"/></svg></span></span></a></div><div class="collapse border bg-body-tertiary rounded p-1 navbar-nav-scroll" id=toc-collapse><small><div class="toc toc-panel text-body p-2"><a class="toc-item toc-level-1" href=/blogs/transformer/#transformer-家族>Transformer 家族 </a><a class="toc-item toc-level-2" href=/blogs/transformer/#encoder-分支>Encoder 分支 </a><a class="toc-item toc-level-2" href=/blogs/transformer/#decoder-分支>Decoder 分支 </a><a class="toc-item toc-level-2" href=/blogs/transformer/#encoder-decoder-分支>Encoder-Decoder 分支 </a><a class="toc-item toc-level-1" href=/blogs/transformer/#transformer-layer>Transformer Layer </a><a class="toc-item toc-level-2" href=/blogs/transformer/#self-attention>Self-Attention </a><a class="toc-item toc-level-2" href=/blogs/transformer/#the-feed-forward-layer-ffn>The Feed-Forward Layer (FFN) </a><a class="toc-item toc-level-1" href=/blogs/transformer/#layer-normalization>Layer Normalization </a><a class="toc-item toc-level-2" href=/blogs/transformer/#post-norm>Post Norm </a><a class="toc-item toc-level-2" href=/blogs/transformer/#pre-norm>Pre Norm </a><a class="toc-item toc-level-2" href=/blogs/transformer/#warmup的作用>Warmup的作用</a></div></small></div></div><div class=content><p>标准的 <strong>Transformer</strong> 模型主要由两个模块构成：</p><center><img style="border-radius:.3125em;box-shadow:0 .2rem .4rem rgba(34,36,38,.12);margin:1rem" width=35% alt src=pics/image-20250312200523538.png></center><blockquote class=blockquote><ul><li><strong>Encoder（左边）</strong>：负责理解输入文本，为每个输入构造对应的语义表示（语义特征）；</li><li><strong>Decoder（右边）</strong>：负责生成输出，使用 Encoder 输出的语义表示结合其他输入来生成目标序列。</li></ul></blockquote><p>这两个模块可以根据任务的需求而单独使用：</p><ul><li><strong>纯 Encoder 模型</strong>：适用于只需要理解输入语义的任务，例如句子分类、命名实体识别；</li><li><strong>纯 Decoder 模型</strong>：适用于生成式任务，例如文本生成；</li><li><strong>Encoder-Decoder 模型</strong> 或 <strong>Seq2Seq 模型</strong>： 适用于需要基于输入的生成式任务，例如翻译、摘要。</li></ul><h2 id=transformer-家族 class=heading>Transformer 家族<a href=#transformer-%e5%ae%b6%e6%97%8f aria-labelledby=transformer-家族><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h2><center><img style="border-radius:.3125em;box-shadow:0 .2rem .4rem rgba(34,36,38,.12);margin:1rem" width=45% alt src=pics/image-20250312200646835.png></center><h3 id=encoder-分支 class=heading>Encoder 分支<a href=#encoder-%e5%88%86%e6%94%af aria-labelledby=encoder-分支><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h3><p>纯 <strong>Encoder 模型</strong>只使用 Transformer 模型中的 Encoder 模块，也被称为<strong>自编码 (auto-encoding) 模型</strong>。在每个阶段，注意力层都可以访问到原始输入句子中的所有词语，即具有 <strong>“双向 (Bi-directional)”注意力</strong>。</p><p>纯 Encoder 模型通常通过破坏给定的句子（例如随机遮盖其中的词语），然后让模型进行重构来进行预训练，最适合处理那些<strong>需要理解整个句子语义的任务</strong>，例如句子分类、命名实体识别（词语分类）、抽取式问答。</p><hr><ul><li><p><strong><a href=https://arxiv.org/abs/1810.04805 class=markdown-link>BERT</a></strong>：通过预测文本中被遮盖的词语和判断一个文本是否跟随另一个来进行预训练</p><ul><li>任务1: 遮盖语言建模 (Masked Language Modeling, <strong>MLM</strong>)</li><li>任务2: 下句预测(Next Sentence Prediction, <strong>NSP</strong>)</li></ul></li><li><p><strong><a href=https://arxiv.org/abs/1910.01108 class=markdown-link>DistilBERT</a></strong>：通过在预训练期间使用<strong>知识蒸馏</strong> (knowledge distillation) 技术，DistilBERT 在内存占用减少 40%、计算速度提高 60% 的情况下，依然可以保持 97% 的性能</p></li><li><p><strong><a href=https://arxiv.org/abs/1907.11692 class=markdown-link>RoBERTa</a></strong>：通过<strong>修改预训练方案</strong>可以进一步提高性能。</p><ul><li>更多的训练数据</li><li>更大的批次</li><li>训练了更长的时间</li><li>放弃了 NSP 任务</li></ul></li><li><p><strong><a href=https://arxiv.org/abs/1901.07291 class=markdown-link>XLM</a></strong>：<strong>跨语言语言模型 (XLM)</strong> 探索了构建多语言模型的多个预训练目标，包括<strong>来自 GPT 的自回归语言建模</strong>和<strong>来自 BERT 的 MLM</strong>，还将 MLM 拓展到多语言输入，提出了<strong>翻译语言建模 (Translation Language Modeling, TLM)</strong>。XLM 在多个多语言 NLU 基准和翻译任务上都取得了最好的性能。</p></li><li><p><strong><a href=https://arxiv.org/abs/1911.02116 class=markdown-link>XLM-RoBERTa</a></strong>：跟随 XLM 和 RoBERTa，XLM-RoBERTa (XLM-R) 通过<strong>升级训练数据</strong>来改进多语言预训练。其基于 Common Crawl 创建了一个 2.5 TB 的语料，然后运用 MLM 训练编码器，由于没有平行对照文本，因此<strong>移除了 XLM 的 TLM 目标</strong>。最终，该模型大幅超越了 XLM 和多语言 BERT 变体。</p></li><li><p><strong><a href=https://arxiv.org/abs/1909.11942 class=markdown-link>ALBERT</a></strong>：ALBERT 通过三处变化使得 Encoder 架构更高效：用更少的参数训练更大的模型，并在 NLU 任务上取得了优异的性能；</p><ul><li>将词嵌入维度与隐藏维度解耦以减少模型参数；</li><li>所有模型层共享参数；</li><li>将 NSP 任务替换为句子排序预测（判断句子顺序是否被交换）</li></ul></li><li><p><strong><a href=https://arxiv.org/abs/2003.10555 class=markdown-link>ELECTRA</a></strong>：MLM 在每个训练步骤中只有被遮盖掉词语的表示会得到更新。ELECTRA 使用了一种<strong>双模型方法</strong>来解决这个问题：</p><ul><li>第一个模型继续按标准 MLM 工作；第二个模型（鉴别器）则预测第一个模型的输出中哪些词语是被遮盖的，这使得训练效率提高了 30 倍。</li><li>下游任务使用时，鉴别器也参与微调；</li></ul></li><li><p><strong><a href=https://arxiv.org/abs/2006.03654 class=markdown-link>DeBERTa</a></strong>：DeBERTa 模型引入了两处<strong>架构变化</strong>。</p><ul><li>将词语的内容与相对位置分离，使得自注意力层 (Self-Attention) 层可以更好地建模邻近词语对的依赖关系；</li><li>在解码头的 softmax 层之前添加了绝对位置嵌入。DeBERTa 是第一个在
<a href=https://arxiv.org/abs/1905.00537 class=markdown-link>SuperGLUE</a> 基准上击败人类的模型</li></ul></li></ul><hr><h3 id=decoder-分支 class=heading>Decoder 分支<a href=#decoder-%e5%88%86%e6%94%af aria-labelledby=decoder-分支><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h3><p>纯 <strong>Decoder 模型</strong>只使用 Transformer 模型中的 Decoder 模块。在每个阶段，对于给定的词语，注意力层只能访问句子中位于它之前的词语，即只能迭代地基于已经生成的词语来逐个预测后面的词语，因此也被称为<strong>自回归 (auto-regressive) 模型</strong>。</p><p>纯 Decoder 模型的预训练通常围绕着预测句子中下一个单词展开。纯 Decoder 模型适合处理那些只涉及<strong>文本生成的任务</strong>。</p><hr><ul><li><p><strong><a href=https://openai.com/blog/language-unsupervised class=markdown-link>GPT</a></strong>：结合了 Transformer Decoder 架构和迁移学习，通过<strong>根据上文预测下一个单词的预训练任务</strong>，在 BookCorpus 数据集上进行了预训练。GPT 模型在分类等下游任务上取得了很好的效果；</p></li><li><p><strong><a href=https://openai.com/blog/better-language-models/ class=markdown-link>GPT-2</a></strong>：受简单且可扩展的预训练方法的启发，OpenAI 通过<strong>扩大原始模型</strong>和<strong>训练集</strong>创造了 GPT-2，它能够生成篇幅较长且语义连贯的文本；</p></li><li><p><strong><a href=https://arxiv.org/abs/1909.05858 class=markdown-link>CTRL</a></strong>：GPT-2 虽然可以根据模板 (prompt) 续写文本，但是几乎无法控制生成序列的风格。条件 Transformer 语言模型 (Conditional Transformer Language, CTRL) 通过<strong>在序列开头添加特殊的“控制符”以控制生成文本的风格</strong>，这样只需要调整控制符就可以生成多样化的文本；</p></li><li><p><strong><a href=https://arxiv.org/abs/2005.14165 class=markdown-link>GPT-3</a></strong>：将 GPT-2 进一步放大 100 倍，GPT-3 具有 1750 亿个参数。除了能生成令人印象深刻的真实篇章之外，还展示了<strong>小样本学习 (few-shot learning)</strong> 的能力。这个模型目前没有开源；</p></li><li><p><strong><a href=https://zenodo.org/record/5297715 class=markdown-link>GPT-Neo</a></strong> / <strong><a href=https://github.com/kingoflolz/mesh-transformer-jax class=markdown-link>GPT-J-6B</a></strong>：由于 GPT-3 没有开源，因此一些旨在重新创建和发布 GPT-3 规模模型的研究人员组成了 EleutherAI，训练出了类似 GPT 的 GPT-Neo 和 GPT-J-6B 。当前公布的模型具有 1.3、2.7、60 亿个参数，在性能上可以媲美较小版本的 GPT-3 模型。</p></li></ul><h3 id=encoder-decoder-分支 class=heading>Encoder-Decoder 分支<a href=#encoder-decoder-%e5%88%86%e6%94%af aria-labelledby=encoder-decoder-分支><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h3><p><strong>Encoder-Decoder 模型</strong>（又称 <strong>Seq2Seq 模型</strong>）同时使用 Transformer 架构的两个模块。在每个阶段，Encoder 的注意力层都可以访问初始输入句子中的所有单词，而 Decoder 的注意力层则只能访问输入中给定词语之前的词语（即已经解码生成的词语）</p><p><strong>Encoder-Decoder 模型</strong>可以使用 Encoder 或 Decoder 模型的目标来完成预训练，但通常会包含一些更复杂的任务</p><p><strong>Encoder-Decoder 模型</strong>适合处理那些需要<strong>根据给定输入来生成新文本</strong>的任务，例如自动摘要、翻译、生成式问答。</p><hr><ul><li><p><strong><a href=https://arxiv.org/abs/1910.10683 class=markdown-link>T5</a></strong>：将所有 NLU 和 NLG 任务都转换为 Seq2Seq 形式统一解决（例如，文本分类就是将文本送入 Encoder，然后 Decoder 生成文本形式的标签）。T5 通过 MLM 及将所有 SuperGLUE 任务转换为 Seq2Seq 任务来进行预训练。最终，具有 110 亿参数的大版本 T5 在多个基准上取得了最优性能。</p></li><li><p><strong><a href=https://arxiv.org/abs/1910.13461 class=markdown-link>BART</a></strong>：同时结合了 BERT 和 GPT 的预训练过程。将输入句子通过遮盖词语、打乱句子顺序、删除词语、文档旋转等方式破坏后传给 Encoder 编码，然后要求 Decoder 能够重构出原始的文本。这使得模型可以灵活地用于 NLU 或 NLG 任务，并且在两者上都实现了最优性能。</p></li><li><p><strong><a href=https://arxiv.org/abs/2010.11125 class=markdown-link>M2M-100</a></strong>：语言对之间可能存在共享知识可以用来处理小众语言之间的翻译。M2M-100 是第一个可以在 100 种语言之间进行翻译的模型，并且对小众的语言也能生成高质量的翻译。该模型使用特殊的前缀标记来指示源语言和目标语言。</p></li><li><p><strong><a href=https://arxiv.org/abs/2007.14062 class=markdown-link>BigBird</a></strong>：由于注意力机制
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n^2)</annotation></semantics></math></span>
</span>的内存要求，Transformer 模型只能处理一定长度内的文本。 BigBird 通过使用线性扩展的稀疏注意力形式，将可处理的文本长度从大多数模型的 512 扩展到 4096，这对于处理文本摘要等需要捕获长距离依赖的任务特别有用</p></li></ul><h2 id=transformer-layer class=heading>Transformer Layer<a href=#transformer-layer aria-labelledby=transformer-layer><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h2><p><strong>Transformer</strong> 的 Encoder 是由多个相同的 <strong>layer</strong> 叠加而成的，每个 Block 都有
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn></mrow><annotation encoding="application/x-tex">2</annotation></semantics></math></span>
</span>个子层（sublayer）。</p><blockquote class=blockquote><p><strong>Block</strong></p><ol><li><strong>多头自注意力</strong>（multi-head self-attention；</li><li><strong>基于位置的前馈网络</strong>（position-wise feed-forward network）</li></ol></blockquote><p>在计算编码器的自注意力时，query、key 和 value 都来自前一个 layer 的输出。</p><blockquote class=blockquote><p><strong>Add & Norm</strong></p><ol><li><p><strong>残差连接</strong>：每个子层都采用了 <strong>残差连接（residual connection）</strong>。</p><p>在 Transformer 中，对于序列中任何位置的任何输入
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">x</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">\mathbf x \in \mathbb R^d</annotation></semantics></math></span>
</span>，都要求满足
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi mathvariant="normal">s</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">b</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">y</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">r</mi></mrow><mo stretchy="false">(</mo><mi mathvariant="bold">x</mi><mo stretchy="false">)</mo><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">\mathrm{sublayer} (\mathbf x )\in \mathbb R^d</annotation></semantics></math></span>
</span>，以便残差连接满足
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">x</mi><mo>+</mo><mrow><mi mathvariant="normal">s</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">b</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">y</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">r</mi></mrow><mo stretchy="false">(</mo><mi mathvariant="bold">x</mi><mo stretchy="false">)</mo><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">\mathbf x +\mathrm{sublayer} (\mathbf x )\in \mathbb R^d</annotation></semantics></math></span>
</span>。</p></li><li><p><strong>层规范化</strong>：在残差连接的加法计算之后，紧接着应用 <strong>层规范化（layer normalization）</strong> (
<a href=https://arxiv.org/abs/1607.06450 class=markdown-link>Layer Normalization</a>)。</p></li><li><p>输入序列对应的每个位置，Transformer 编码器都将输出一个
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span>
</span>维表示向量。</p></li></ol></blockquote><h3 id=self-attention class=heading>Self-Attention<a href=#self-attention aria-labelledby=self-attention><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h3><p><strong>生成查询向量、键向量和值向量</strong></p><ul><li><p><span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi></mrow><annotation encoding="application/x-tex">l</annotation></semantics></math></span>
</span>：输入长度</p></li><li><p><span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span>
</span>: hidden size</p></li><li><p>输入向量
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">X</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>l</mi><mo>×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf X \in \mathbb{R}^{l\times d}</annotation></semantics></math></span>
</span>分别乘以三个不同的权重矩阵
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi mathvariant="bold">W</mi><mi>Q</mi></msup><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>d</mi><mo>×</mo><msub><mi>d</mi><mi>k</mi></msub></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf W^Q \in \mathbb{R}^{d\times d_k} </annotation></semantics></math></span>
</span>，
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi mathvariant="bold">W</mi><mi>K</mi></msup><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>d</mi><mo>×</mo><msub><mi>d</mi><mi>k</mi></msub></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf W^K\in\mathbb{R}^{d\times d_k}</annotation></semantics></math></span>
</span>，
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi mathvariant="bold">W</mi><mi>V</mi></msup><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>d</mi><mo>×</mo><msub><mi>d</mi><mi>v</mi></msub></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf W^V \in \mathbb{R}^{d \times d_v}</annotation></semantics></math></span>
</span>得到
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">Q</mi></mrow><annotation encoding="application/x-tex">\mathbf Q</annotation></semantics></math></span>
</span>、
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">K</mi></mrow><annotation encoding="application/x-tex">\mathbf K</annotation></semantics></math></span>
</span>、
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">V</mi></mrow><annotation encoding="application/x-tex">\mathbf V</annotation></semantics></math></span>
</span>。</p><p>这些权重矩阵为模型的参数，需要通过训练得到（先随机初始化，然后在损失函数中表示出来，最后通过反向传播不断优化学习得出，最终目标是最小化模型的预测误差）。</p></li></ul><center><img style="border-radius:.3125em;box-shadow:0 .2rem .4rem rgba(34,36,38,.12);margin:1rem" width=50% alt src=pics/transformer_self_attention_vectors.png></center><ul><li><p>令
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">X</mi><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi mathvariant="bold">X</mi><mn>1</mn></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi mathvariant="bold">X</mi><mn>2</mn></msub></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mn>2</mn><mo>×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf X = \begin{bmatrix}\mathbf X_1 &amp;\mathbf X_2 \end{bmatrix}\in \mathbb{R}^{2\times d}</annotation></semantics></math></span>
</span>,
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">X</mi><mi>i</mi></msub><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mn>1</mn><mo>×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf X_i \in \mathbb{R}^{1\times d}</annotation></semantics></math></span></span></p><ul><li><span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">X</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">\mathbf X_1</annotation></semantics></math></span>
</span>与
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi mathvariant="bold">W</mi><mi>Q</mi></msup></mrow><annotation encoding="application/x-tex">\mathbf W^{Q}</annotation></semantics></math></span>
</span>权重矩阵相乘得到与这个单词相关的查询向量
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">q</mi><mn>1</mn></msub><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mn>1</mn><mo>×</mo><msub><mi>d</mi><mi>k</mi></msub></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf q_1 \in \mathbb{R}^{1\times d_k}</annotation></semantics></math></span></span></li><li><span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">X</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">\mathbf X_1</annotation></semantics></math></span>
</span>与
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi mathvariant="bold">W</mi><mi>K</mi></msup></mrow><annotation encoding="application/x-tex">\mathbf W^{K}</annotation></semantics></math></span>
</span>权重矩阵相乘得到与这个单词相关的键向量
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">k</mi><mn>1</mn></msub><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mn>1</mn><mo>×</mo><msub><mi>d</mi><mi>k</mi></msub></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf k_1 \in \mathbb{R}^{1\times d_k}</annotation></semantics></math></span></span></li><li><span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">X</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">\mathbf X_1</annotation></semantics></math></span>
</span>与
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi mathvariant="bold">W</mi><mi>V</mi></msup></mrow><annotation encoding="application/x-tex">\mathbf W^{V}</annotation></semantics></math></span>
</span>权重矩阵相乘得到与这个单词相关的值向量
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">v</mi><mn>1</mn></msub><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mn>1</mn><mo>×</mo><msub><mi>d</mi><mi>v</mi></msub></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf v_1 \in \mathbb{R}^{1\times d_v}</annotation></semantics></math></span></span></li></ul><p>最终使得输入序列的每个单词各自创建一个查询向量、一个键向量和一个值向量</p></li></ul><hr><p><strong>矩阵运算</strong></p><center><img style="border-radius:.3125em;box-shadow:0 .2rem .4rem rgba(34,36,38,.12);margin:1rem" width=25% alt src=pics/self-attention-matrix-calculation.png></center><h4 id=scaled-dot-product-attention class=heading>Scaled Dot-product Attention<a href=#scaled-dot-product-attention aria-labelledby=scaled-dot-product-attention><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h4><ul><li><p><strong>查询向量 Query,
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">Q</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>l</mi><mo>×</mo><msub><mi>d</mi><mi>k</mi></msub></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf{Q}\in\mathbb{R}^{l\times d_k}</annotation></semantics></math></span>
</span></strong>：是当前单词的表示形式，用于对所有其他单词(key)进行评分，我们只需要关注当前正在处理的 token 的 query</p></li><li><p><strong>键向量 Key，
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">K</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>l</mi><mo>×</mo><msub><mi>d</mi><mi>k</mi></msub></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf{K}\in\mathbb{R}^{l\times d_k}</annotation></semantics></math></span>
</span></strong>可以看做是序列中所有单词的标签，是在我们找相关单词时候的对照物</p></li><li><p><strong>值向量 Value,
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">V</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>l</mi><mo>×</mo><msub><mi>d</mi><mi>v</mi></msub></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf{V}\in\mathbb{R}^{l\times d_v}</annotation></semantics></math></span>
</span></strong>是单词的实际表示，一旦我们对每个单词的相关度打分之后，我们就要对 value 进行相加表示当前正在处理单词的 value</p><center><img style="border-radius:.3125em;box-shadow:0 .2rem .4rem rgba(34,36,38,.12);margin:1rem" width=40% alt src=pics/self-attention-matrix-calculation-2.png></center></li></ul><div class=mathjax-display><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.25em" columnalign="right" columnspacing=""><mtr><mtd class ="mtr-glue"></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mtext>Attention</mtext><mo stretchy="false">(</mo><mi mathvariant="bold">Q</mi><mo separator="true">,</mo><mi mathvariant="bold">K</mi><mo separator="true">,</mo><mi mathvariant="bold">V</mi><mo stretchy="false">)</mo><mo>=</mo><mtext>softmax</mtext><mrow><mo fence="true">(</mo><mfrac><mrow><mi mathvariant="bold">Q</mi><msup><mi mathvariant="bold">K</mi><mi mathvariant="normal">⊤</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac><mo fence="true">)</mo></mrow><mi mathvariant="bold">V</mi></mrow></mstyle></mtd><mtd class ="mtr-glue"></mtd><mtd class ="mml-eqn-num"></mtd></mtr></mtable><annotation encoding="application/x-tex">
\begin{align}
\text{Attention}(\mathbf{Q},\mathbf{K},\mathbf{V}) = \text{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^{\top}}{\sqrt{d_k}}\right)\mathbf{V} 
\end{align}
</annotation></semantics></math></span></div><ol><li><p><strong>计算注意力权重</strong></p><ul><li><p><strong>注意力分数矩阵:
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">Q</mi><msup><mi mathvariant="bold">K</mi><mi mathvariant="normal">⊤</mi></msup><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>l</mi><mo>×</mo><mi>l</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf{Q}\mathbf{K}^{\top} \in\mathbb{R}^{l\times l}</annotation></semantics></math></span>
</span></strong>：特别地，Scaled Dot-product Attention 使用点积作为相似度函数，这样相似的 queries 和 keys 会具有较大的点积。</p><ol><li>注意力分数需要乘以一个缩放因子
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mrow><annotation encoding="application/x-tex">\sqrt{d_k}</annotation></semantics></math></span>
</span>来标准化它们的方差</li><li>注意力分数需要对第二个维度做 Softmax 标准化</li></ol></li><li><p><strong>最终的注意力权重矩阵：
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi mathvariant="normal">s</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">f</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">x</mi></mrow><mrow><mo fence="true">(</mo><mfrac><mrow><mi mathvariant="bold">Q</mi><msup><mi mathvariant="bold">K</mi><mi mathvariant="normal">⊤</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac><mo fence="true">)</mo></mrow><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>l</mi><mo>×</mo><mi>l</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\mathrm{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^{\top} }{\sqrt{d_k}}\right) \in\mathbb{R}^{l\times l}</annotation></semantics></math></span>
</span></strong>：矩阵中的每一个元素
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">w_{ij}</annotation></semantics></math></span>
</span>表示第
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span>
</span>个 token(query) 向量与第
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>j</mi></mrow><annotation encoding="application/x-tex">j</annotation></semantics></math></span>
</span>个 token(key) 向量之间的关联程度。</p></li></ul></li><li><p><strong>更新token embeddings</strong></p><ul><li>将权重
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">w_{ij}</annotation></semantics></math></span>
</span>与对应的 value 向量
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">v</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>⋯</mo><mtext> </mtext><mo separator="true">,</mo><msub><mi mathvariant="bold">v</mi><mi>l</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{v}_1,\cdots,\mathbf{v}_l</annotation></semantics></math></span>
</span>相乘以获得第
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span>
</span>个 token(query) 向量更新后的语义表示
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi mathvariant="bold">x</mi><mi>i</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msubsup><mo>=</mo><msub><mo>∑</mo><mi>j</mi></msub><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><msub><mi mathvariant="bold">v</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{x}_i^{\prime} = \sum_{j} w_{ij}\mathbf{v}_j</annotation></semantics></math></span></span></li></ul></li></ol><p><strong>对公式进行拆解：</strong></p><div class=mathjax-display><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.25em" columnalign="right" columnspacing=""><mtr><mtd class ="mtr-glue"></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mtext>Attention</mtext><mo stretchy="false">(</mo><msub><mi mathvariant="bold">q</mi><mi>i</mi></msub><mo separator="true">,</mo><mi mathvariant="bold">K</mi><mo separator="true">,</mo><mi mathvariant="bold">V</mi><mo stretchy="false">)</mo><mo>=</mo><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>l</mi></munderover><mfrac><mn>1</mn><mi>Z</mi></mfrac><mi>exp</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><mfrac><mrow><mo stretchy="false">⟨</mo><msub><mi mathvariant="bold">q</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi mathvariant="bold">k</mi><mi>j</mi></msub><mo stretchy="false">⟩</mo></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac><mo fence="true">)</mo></mrow><msub><mi mathvariant="bold">v</mi><mi>j</mi></msub></mrow></mstyle></mtd><mtd class ="mtr-glue"></mtd><mtd class ="mml-eqn-num"></mtd></mtr></mtable><annotation encoding="application/x-tex">
\begin{align}
\text{Attention}(\mathbf{q}_i,\mathbf{K},\mathbf{V}) = \sum_{j=1}^l \frac{1}{Z}\exp\left(\frac{\langle\mathbf{q}_i, \mathbf{k}_j\rangle}{\sqrt{d_k}}\right)\mathbf{v}_j
\end{align}
</annotation></semantics></math></span></div><p>其中
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Z</mi></mrow><annotation encoding="application/x-tex">Z</annotation></semantics></math></span>
</span>是归一化因子，
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">K</mi></mrow><annotation encoding="application/x-tex">\mathbf K</annotation></semantics></math></span>
</span>,
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">V</mi></mrow><annotation encoding="application/x-tex">\mathbf V</annotation></semantics></math></span>
</span>是一一对应的 key 和 value 向量序列。</p><p><code>Scaled Dot-product Attention</code> 就是通过
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">q</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{q}_i</annotation></semantics></math></span>
</span>这个 query 与各个
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">k</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{k}_j</annotation></semantics></math></span>
</span>内积并 softmax 的方式来得到
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">q</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{q}_i</annotation></semantics></math></span>
</span>与各个
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">v</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{v}_j</annotation></semantics></math></span>
</span>的相似度，然后加权求和，得到一个
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mi>v</mi></msub></mrow><annotation encoding="application/x-tex">d_v</annotation></semantics></math></span>
</span>维的向量。
其中因子
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">d_k</annotation></semantics></math></span>
</span>起到调节作用，使得内积不至于太大。</p><hr><p><strong>缩放因子
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mrow><annotation encoding="application/x-tex">\sqrt{d_k}</annotation></semantics></math></span>
</span>的作用分析</strong></p><p><strong>scaled 是指对注意力权重进行缩放，以确保数值的稳定性</strong></p><ol><li><p><strong>控制 softmax 的输入的数量级</strong>：</p><p><strong>queries 和 keys 点积的方差与
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">d_k</annotation></semantics></math></span>
</span>成正比</strong>。方差越大说明点积的数量级越大，<strong>为了将方差稳定为 1，将点积除以
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mrow><annotation encoding="application/x-tex">\sqrt{d_k}</annotation></semantics></math></span>
</span>:</strong></p><p><strong>【证明】</strong>：</p><ul><li><p>考虑两个随机向量
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">u</mi></mrow><annotation encoding="application/x-tex">\mathbf u</annotation></semantics></math></span>
</span>和
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">v</mi></mrow><annotation encoding="application/x-tex">\mathbf v </annotation></semantics></math></span>
</span>，其长度都为
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span>
</span>，且它们的元素都来自均值为
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">0</annotation></semantics></math></span>
</span>、方差为
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn></mrow><annotation encoding="application/x-tex">1</annotation></semantics></math></span>
</span>的独立分布。</p></li><li><p>对于向量的元素
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>u</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">u_i</annotation></semantics></math></span>
</span>和
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>v</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">v_i</annotation></semantics></math></span>
</span>，它们的点积为
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>u</mi><mi>i</mi></msub><msub><mi>v</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">u_iv_i</annotation></semantics></math></span></span></p><ul><li><p>期望值：
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">E</mi><mo stretchy="false">[</mo><msub><mi>u</mi><mi>i</mi></msub><msub><mi>v</mi><mi>i</mi></msub><mo stretchy="false">]</mo><mo>=</mo><mi mathvariant="normal">E</mi><mo stretchy="false">[</mo><msub><mi>u</mi><mi>i</mi></msub><mo stretchy="false">]</mo><mi>E</mi><mo stretchy="false">[</mo><msub><mi>v</mi><mi>i</mi></msub><mo stretchy="false">]</mo><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\mathrm{E}[u_iv_i] = \mathrm{E}[u_i] E [v_i] = 0</annotation></semantics></math></span></span></p></li><li><p>方差：
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi mathvariant="normal">V</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">r</mi></mrow><mo stretchy="false">(</mo><msub><mi>u</mi><mi>i</mi></msub><msub><mi>v</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mi mathvariant="normal">E</mi><mo stretchy="false">[</mo><mo stretchy="false">(</mo><msub><mi>u</mi><mi>i</mi></msub><msub><mi>v</mi><mi>i</mi></msub><msup><mo stretchy="false">)</mo><mn>2</mn></msup><mo stretchy="false">]</mo><mo>−</mo><mo stretchy="false">(</mo><mi mathvariant="normal">E</mi><mo stretchy="false">[</mo><msub><mi>u</mi><mi>i</mi></msub><msub><mi>v</mi><mi>i</mi></msub><mo stretchy="false">]</mo><msup><mo stretchy="false">)</mo><mn>2</mn></msup><mo>=</mo><mi mathvariant="normal">E</mi><mo stretchy="false">[</mo><msubsup><mi>u</mi><mi>i</mi><mn>2</mn></msubsup><mo stretchy="false">]</mo><mi mathvariant="normal">E</mi><mo stretchy="false">[</mo><msubsup><mi>v</mi><mi>i</mi><mn>2</mn></msubsup><mo stretchy="false">]</mo><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\mathrm{Var}(u_iv_i)=\mathrm{E}[(u_iv_i)^2] - (\mathrm{E} [u_iv_i])^2= \mathrm{E}[u_i^2] \mathrm{E}[v_i^2]=1</annotation></semantics></math></span></span></p></li></ul></li><li><p>因此，向量
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">u</mi></mrow><annotation encoding="application/x-tex">\mathbf u</annotation></semantics></math></span>
</span>和
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">v</mi></mrow><annotation encoding="application/x-tex">\mathbf v </annotation></semantics></math></span>
</span>的点积是这些
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>u</mi><mi>i</mi></msub><msub><mi>v</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">u_iv_i</annotation></semantics></math></span>
</span>项的总和：</p><div class=mathjax-display><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.25em" columnalign="right" columnspacing=""><mtr><mtd class ="mtr-glue"></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi mathvariant="bold">u</mi><mo>⋅</mo><mi mathvariant="bold">v</mi><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover><msub><mi>u</mi><mi>i</mi></msub><msub><mi>v</mi><mi>i</mi></msub></mrow></mstyle></mtd><mtd class ="mtr-glue"></mtd><mtd class ="mml-eqn-num"></mtd></mtr></mtable><annotation encoding="application/x-tex">
        \begin{align}
        \mathbf u \cdot \mathbf v = \sum ^d_{i = 1}u_iv_i
        \end{align}
        </annotation></semantics></math></span></div><ul><li><p>期望值：
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">E</mi><mo stretchy="false">[</mo><mi mathvariant="bold">u</mi><mo>⋅</mo><mi mathvariant="bold">v</mi><mo stretchy="false">]</mo><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\mathrm{E}[\mathbf u \cdot \mathbf v] = 0</annotation></semantics></math></span></span></p></li><li><p>方差：
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi mathvariant="normal">V</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">r</mi></mrow><mo stretchy="false">[</mo><mi mathvariant="bold">u</mi><mo>⋅</mo><mi mathvariant="bold">v</mi><mo stretchy="false">]</mo><mo>=</mo><mi>d</mi><mo>⋅</mo><mrow><mi mathvariant="normal">V</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">r</mi></mrow><mo stretchy="false">(</mo><msub><mi>u</mi><mi>i</mi></msub><msub><mi>v</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mi>d</mi></mrow><annotation encoding="application/x-tex">\mathrm{Var}[\mathbf u \cdot \mathbf v] = d \cdot \mathrm{Var}(u_iv_i)= d</annotation></semantics></math></span></span></p></li></ul></li><li><p>因此得出结论</p><div class=mathjax-display><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.25em" columnalign="right" columnspacing=""><mtr><mtd class ="mtr-glue"></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow><mi mathvariant="normal">V</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">r</mi></mrow><mo stretchy="false">(</mo><mfrac><mrow><mi mathvariant="bold">u</mi><mo>⋅</mo><mi mathvariant="bold">v</mi></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac><mo stretchy="false">)</mo><mo>=</mo><mfrac><msub><mi>d</mi><mi>k</mi></msub><mrow><mo stretchy="false">(</mo><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mrow></mfrac><mo>=</mo><mn>1</mn></mrow></mstyle></mtd><mtd class ="mtr-glue"></mtd><mtd class ="mml-eqn-num"></mtd></mtr></mtable><annotation encoding="application/x-tex">
        \begin{align}
        \mathrm{Var}(\frac{\mathbf{u} \cdot \mathbf{v}}{\sqrt{d_k}}) = \frac{d_k}{(\sqrt{d_k})^2} = 1
        \end{align}
        </annotation></semantics></math></span></div></li></ul></li><li><p><strong>防止求导的梯度越趋向于0</strong>：<strong>Softmax 的输入的数量级越大，求导的梯度越会趋向于0</strong>。</p><p>当
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mrow><annotation encoding="application/x-tex">\sqrt{d_k}</annotation></semantics></math></span>
</span>较大时，即两个向量的长度比较长时，两个向量的<strong>相对差距就会变大</strong>。值最大的那个值做出来的 softmax 会更加靠近 1，剩下的会更加靠近 0。<strong>值的分布会更加向两端靠拢</strong>，此时计算梯度的时候，<strong>梯度会趋近于 0</strong>。</p><p>softmax 函数能够将输入中的元素间差距拉大，然后归一化为一个 0~1 之间的分布。</p><ul><li><p>假设输入的
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>=</mo><mo stretchy="false">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>2</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>x</mi><mi>n</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">x = ( x_1 , x_2 , ..., x_n )</annotation></semantics></math></span>
</span>, 其中最大元素为
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">x_k</annotation></semantics></math></span>
</span>，其对应的概率输出为
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">y_k</annotation></semantics></math></span>
</span>，将会呈现
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">x_k</annotation></semantics></math></span>
</span>的数量级越大，
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">y_k</annotation></semantics></math></span>
</span>越趋向于 1 的趋势。</p></li><li><p>输入
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">x</mi><mo>=</mo><mo stretchy="false">[</mo><mi>a</mi><mo separator="true">,</mo><mi>a</mi><mo separator="true">,</mo><mn>2</mn><mi>a</mi><msup><mo stretchy="false">]</mo><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">\mathbf x = [a, a, 2a]^T</annotation></semantics></math></span>
</span>, 不同量级的
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi></mrow><annotation encoding="application/x-tex">a</annotation></semantics></math></span>
</span>对应的
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover accent="true"><mi>y</mi><mo>^</mo></mover><mn>3</mn></msub></mrow><annotation encoding="application/x-tex">\hat y_3</annotation></semantics></math></span>
</span>：</p><div class="mb-3 syntax-highlight"><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-0-1><a class=lnlinks href=#hl-0-1>1</a>
</span><span class=lnt id=hl-0-2><a class=lnlinks href=#hl-0-2>2</a>
</span><span class=lnt id=hl-0-3><a class=lnlinks href=#hl-0-3>3</a>
</span><span class=lnt id=hl-0-4><a class=lnlinks href=#hl-0-4>4</a>
</span><span class=lnt id=hl-0-5><a class=lnlinks href=#hl-0-5>5</a>
</span><span class=lnt id=hl-0-6><a class=lnlinks href=#hl-0-6>6</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>for</span> <span class=n>a</span> <span class=ow>in</span> <span class=p>(</span><span class=mi>1</span><span class=p>,</span><span class=mi>10</span><span class=p>,</span><span class=mi>100</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Softmax</span><span class=p>((</span><span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=n>a</span><span class=p>,</span> <span class=n>a</span><span class=p>,</span> <span class=mi>2</span><span class=o>*</span><span class=n>a</span><span class=p>])</span><span class=o>.</span><span class=n>float</span><span class=p>())))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>tensor</span><span class=p>([</span><span class=mf>0.2119</span><span class=p>,</span> <span class=mf>0.2119</span><span class=p>,</span> <span class=mf>0.5761</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>tensor</span><span class=p>([</span><span class=mf>4.5396e-05</span><span class=p>,</span> <span class=mf>4.5396e-05</span><span class=p>,</span> <span class=mf>9.9991e-01</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>tensor</span><span class=p>([</span><span class=mf>3.7835e-44</span><span class=p>,</span> <span class=mf>3.7835e-44</span><span class=p>,</span> <span class=mf>1.0000e+00</span><span class=p>])</span></span></span></code></pre></td></tr></table></div></div></div></li><li><p>在这种情况下，<strong>如果输入
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span>
</span>的数量级很大</strong>，而假设它的最大值是
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">x_k</annotation></semantics></math></span>
</span>，则经过 softmax 计算得到的
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span>
</span>中，<strong>只有
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">y_k</annotation></semantics></math></span>
</span>趋向于 1，其它概率元素全都趋向于 0</strong>。</p><ul><li><p>当
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mrow><annotation encoding="application/x-tex">\sqrt{d_k}</annotation></semantics></math></span>
</span>较小时，除不除都没事；</p></li><li><p>当
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mrow><annotation encoding="application/x-tex">\sqrt{d_k}</annotation></semantics></math></span>
</span>较大时，<strong>两个向量的长度比较长时，两个向量的相对差距就会变大</strong>。值最大的那个值做出来的 softmax 会更加靠近 1，剩下的会更加靠近 0。值的分布会更加向两端靠拢，此时计算梯度的时候，<strong>梯度会趋近于 0</strong>。</p></li></ul></li></ul></li></ol><hr><p><strong>Attention 代码实现</strong></p><p>将 <code>causal_mask</code> 对应的注意力分数设置为
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><mi mathvariant="normal">∞</mi></mrow><annotation encoding="application/x-tex">-\infin</annotation></semantics></math></span>
</span>，这样 softmax 之后其对应的注意力权重就为
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">0</annotation></semantics></math></span>
</span>了（
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>e</mi><mrow><mo>−</mo><mi mathvariant="normal">∞</mi></mrow></msup><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">e^{−\infin}=0</annotation></semantics></math></span>
</span>）</p><div class="mb-3 syntax-highlight"><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-1-1><a class=lnlinks href=#hl-1-1> 1</a>
</span><span class=lnt id=hl-1-2><a class=lnlinks href=#hl-1-2> 2</a>
</span><span class=lnt id=hl-1-3><a class=lnlinks href=#hl-1-3> 3</a>
</span><span class=lnt id=hl-1-4><a class=lnlinks href=#hl-1-4> 4</a>
</span><span class=lnt id=hl-1-5><a class=lnlinks href=#hl-1-5> 5</a>
</span><span class=lnt id=hl-1-6><a class=lnlinks href=#hl-1-6> 6</a>
</span><span class=lnt id=hl-1-7><a class=lnlinks href=#hl-1-7> 7</a>
</span><span class=lnt id=hl-1-8><a class=lnlinks href=#hl-1-8> 8</a>
</span><span class=lnt id=hl-1-9><a class=lnlinks href=#hl-1-9> 9</a>
</span><span class=lnt id=hl-1-10><a class=lnlinks href=#hl-1-10>10</a>
</span><span class=lnt id=hl-1-11><a class=lnlinks href=#hl-1-11>11</a>
</span><span class=lnt id=hl-1-12><a class=lnlinks href=#hl-1-12>12</a>
</span><span class=lnt id=hl-1-13><a class=lnlinks href=#hl-1-13>13</a>
</span><span class=lnt id=hl-1-14><a class=lnlinks href=#hl-1-14>14</a>
</span><span class=lnt id=hl-1-15><a class=lnlinks href=#hl-1-15>15</a>
</span><span class=lnt id=hl-1-16><a class=lnlinks href=#hl-1-16>16</a>
</span><span class=lnt id=hl-1-17><a class=lnlinks href=#hl-1-17>17</a>
</span><span class=lnt id=hl-1-18><a class=lnlinks href=#hl-1-18>18</a>
</span><span class=lnt id=hl-1-19><a class=lnlinks href=#hl-1-19>19</a>
</span><span class=lnt id=hl-1-20><a class=lnlinks href=#hl-1-20>20</a>
</span><span class=lnt id=hl-1-21><a class=lnlinks href=#hl-1-21>21</a>
</span><span class=lnt id=hl-1-22><a class=lnlinks href=#hl-1-22>22</a>
</span><span class=lnt id=hl-1-23><a class=lnlinks href=#hl-1-23>23</a>
</span><span class=lnt id=hl-1-24><a class=lnlinks href=#hl-1-24>24</a>
</span><span class=lnt id=hl-1-25><a class=lnlinks href=#hl-1-25>25</a>
</span><span class=lnt id=hl-1-26><a class=lnlinks href=#hl-1-26>26</a>
</span><span class=lnt id=hl-1-27><a class=lnlinks href=#hl-1-27>27</a>
</span><span class=lnt id=hl-1-28><a class=lnlinks href=#hl-1-28>28</a>
</span><span class=lnt id=hl-1-29><a class=lnlinks href=#hl-1-29>29</a>
</span><span class=lnt id=hl-1-30><a class=lnlinks href=#hl-1-30>30</a>
</span><span class=lnt id=hl-1-31><a class=lnlinks href=#hl-1-31>31</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn.functional</span> <span class=k>as</span> <span class=nn>F</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>math</span> <span class=kn>import</span> <span class=n>sqrt</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>scaled_dot_product_attention</span><span class=p>(</span><span class=n>query</span><span class=p>,</span> <span class=n>key</span><span class=p>,</span> <span class=n>value</span><span class=p>,</span> <span class=n>causal_mask</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
</span></span><span class=line><span class=cl>     <span class=c1># [B,h,L,d/h]</span>
</span></span><span class=line><span class=cl>    <span class=n>dim_k</span> <span class=o>=</span> <span class=n>query</span><span class=o>.</span><span class=n>size</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>scores</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>bmm</span><span class=p>(</span><span class=n>query</span><span class=p>,</span> <span class=n>key</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=o>-</span><span class=mi>2</span><span class=p>))</span> <span class=o>/</span> <span class=n>sqrt</span><span class=p>(</span><span class=n>dim_k</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>causal_mask</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=c1># -inf</span>
</span></span><span class=line><span class=cl>		<span class=n>mask_value</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>finfo</span><span class=p>(</span><span class=n>attn_weights</span><span class=o>.</span><span class=n>dtype</span><span class=p>)</span><span class=o>.</span><span class=n>min</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=c1># [a11, -inf, ..., -inf]</span>
</span></span><span class=line><span class=cl>        <span class=c1># [a21, a22, ..., -inf]</span>
</span></span><span class=line><span class=cl>        <span class=c1># ...</span>
</span></span><span class=line><span class=cl>        <span class=c1># [an1, an2, ..., ann]</span>
</span></span><span class=line><span class=cl>        <span class=n>scores</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>where</span><span class=p>(</span><span class=n>causal_mask</span><span class=p>,</span> <span class=n>attn_weights</span><span class=p>,</span> <span class=n>mask_value</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># 对每一行求softmax (最后一维)</span>
</span></span><span class=line><span class=cl>    <span class=c1># [b11, 0, ..., 0]</span>
</span></span><span class=line><span class=cl>    <span class=c1># [b21, b22, ..., 0]</span>
</span></span><span class=line><span class=cl>    <span class=c1># ...</span>
</span></span><span class=line><span class=cl>    <span class=c1># [bn1, bn2, ..., bnn]</span>
</span></span><span class=line><span class=cl>    <span class=n>attn_weights</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>scores</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># dropout</span>
</span></span><span class=line><span class=cl>	<span class=n>attn_weights</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>attn_dropout</span><span class=p>(</span><span class=n>attn_weights</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=n>attn_output</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>bmm</span><span class=p>(</span><span class=n>attn_weights</span><span class=p>,</span> <span class=n>value</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>attn_output</span></span></span></code></pre></td></tr></table></div></div></div><p>mask 矩阵的实现</p><div class="mb-3 syntax-highlight"><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-2-1><a class=lnlinks href=#hl-2-1>1</a>
</span><span class=lnt id=hl-2-2><a class=lnlinks href=#hl-2-2>2</a>
</span><span class=lnt id=hl-2-3><a class=lnlinks href=#hl-2-3>3</a>
</span><span class=lnt id=hl-2-4><a class=lnlinks href=#hl-2-4>4</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># causal_mask 下三角矩阵 </span>
</span></span><span class=line><span class=cl><span class=n>causal_mask</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tril</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>((</span><span class=n>query_length</span><span class=p>,</span> <span class=n>query_length</span><span class=p>),</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>bool</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>)</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>query_length</span><span class=p>,</span> <span class=n>query_length</span><span class=p>)</span></span></span></code></pre></td></tr></table></div></div></div><h4 id=multi-headed-attention class=heading>Multi-headed attention<a href=#multi-headed-attention aria-labelledby=multi-headed-attention><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h4><center><img style="border-radius:.3125em;box-shadow:0 .2rem .4rem rgba(34,36,38,.12);margin:1rem" width=70% alt src=pics/transformer_multi-headed_self-attention-recap.png></center><div class=mathjax-display><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.25em" columnalign="right left" columnspacing="0em"><mtr><mtd class ="mtr-glue"></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><msub><mrow><mi mathvariant="normal">h</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">d</mi></mrow><mi>i</mi></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mtext>Attention</mtext><mo stretchy="false">(</mo><mi mathvariant="bold">Q</mi><mo separator="true">,</mo><mi mathvariant="bold">K</mi><mo separator="true">,</mo><mi mathvariant="bold">V</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd class ="mtr-glue"></mtd><mtd class ="mml-eqn-num"></mtd></mtr><mtr><mtd class ="mtr-glue"></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mtext>MultiHead</mtext><mo stretchy="false">(</mo><mi mathvariant="bold">Q</mi><mo separator="true">,</mo><mi mathvariant="bold">K</mi><mo separator="true">,</mo><mi mathvariant="bold">V</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mo>=</mo><mtext>Concat</mtext><mo stretchy="false">(</mo><msub><mrow><mi mathvariant="normal">h</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">d</mi></mrow><mn>1</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mrow><mi mathvariant="normal">h</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">d</mi></mrow><mi>h</mi></msub><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd class ="mtr-glue"></mtd><mtd class ="mml-eqn-num"></mtd></mtr></mtable><annotation encoding="application/x-tex">
\begin{align}
\mathrm{head}_i &amp;= \text{Attention}(\mathbf{Q},\mathbf{K},\mathbf{V})\\[7pt]
\text{MultiHead}(\mathbf{Q},\mathbf{K},\mathbf{V}) &amp;= \text{Concat}(\mathrm{head}_1,...,\mathrm{head}_h)
\end{align}
</annotation></semantics></math></span></div><ol><li>多头的注意力有助于网络捕捉到更丰富的特征/信息</li><li>允许模型在不同的表示子空间里学习到相关的信息</li></ol><hr><p><strong>代码实现</strong></p><p><strong>实现一个注意力头</strong></p><p>每个头都会初始化三个独立的线性层，负责将
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">Q</mi></mrow><annotation encoding="application/x-tex">\mathbf{Q}</annotation></semantics></math></span>
</span>,
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">K</mi></mrow><annotation encoding="application/x-tex">\mathbf{K}</annotation></semantics></math></span>
</span>,
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">V</mi></mrow><annotation encoding="application/x-tex">\mathbf{V}</annotation></semantics></math></span>
</span>序列映射到尺寸为 <code>[batch_size, seq_len, head_dim]</code> 的张量，其中 <code>head_dim</code> 是映射到的向量维度。</p><p>实践中一般将 <code>head_dim</code> 设置为 <code>embed_dim</code> 的因数，这样 token 嵌入式表示的维度就可以保持不变，例如 BERT 有
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>12</mn></mrow><annotation encoding="application/x-tex">12</annotation></semantics></math></span>
</span>个注意力头，因此每个头的维度被设置为
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>768</mn><mi mathvariant="normal">/</mi><mn>12</mn><mo>=</mo><mn>64</mn></mrow><annotation encoding="application/x-tex">768/12=64</annotation></semantics></math></span>
</span>。</p><div class="mb-3 syntax-highlight"><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-3-1><a class=lnlinks href=#hl-3-1> 1</a>
</span><span class=lnt id=hl-3-2><a class=lnlinks href=#hl-3-2> 2</a>
</span><span class=lnt id=hl-3-3><a class=lnlinks href=#hl-3-3> 3</a>
</span><span class=lnt id=hl-3-4><a class=lnlinks href=#hl-3-4> 4</a>
</span><span class=lnt id=hl-3-5><a class=lnlinks href=#hl-3-5> 5</a>
</span><span class=lnt id=hl-3-6><a class=lnlinks href=#hl-3-6> 6</a>
</span><span class=lnt id=hl-3-7><a class=lnlinks href=#hl-3-7> 7</a>
</span><span class=lnt id=hl-3-8><a class=lnlinks href=#hl-3-8> 8</a>
</span><span class=lnt id=hl-3-9><a class=lnlinks href=#hl-3-9> 9</a>
</span><span class=lnt id=hl-3-10><a class=lnlinks href=#hl-3-10>10</a>
</span><span class=lnt id=hl-3-11><a class=lnlinks href=#hl-3-11>11</a>
</span><span class=lnt id=hl-3-12><a class=lnlinks href=#hl-3-12>12</a>
</span><span class=lnt id=hl-3-13><a class=lnlinks href=#hl-3-13>13</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torch</span> <span class=kn>import</span> <span class=n>nn</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>AttentionHead</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>embed_dim</span><span class=p>,</span> <span class=n>head_dim</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>q</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>embed_dim</span><span class=p>,</span> <span class=n>head_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>k</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>embed_dim</span><span class=p>,</span> <span class=n>head_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>v</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>embed_dim</span><span class=p>,</span> <span class=n>head_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>query</span><span class=p>,</span> <span class=n>key</span><span class=p>,</span> <span class=n>value</span><span class=p>,</span> <span class=n>query_mask</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>key_mask</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>attn_outputs</span> <span class=o>=</span> <span class=n>scaled_dot_product_attention</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>q</span><span class=p>(</span><span class=n>query</span><span class=p>),</span> <span class=bp>self</span><span class=o>.</span><span class=n>k</span><span class=p>(</span><span class=n>key</span><span class=p>),</span> <span class=bp>self</span><span class=o>.</span><span class=n>v</span><span class=p>(</span><span class=n>value</span><span class=p>),</span> <span class=n>query_mask</span><span class=p>,</span> <span class=n>key_mask</span><span class=p>,</span> <span class=n>mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>attn_outputs</span></span></span></code></pre></td></tr></table></div></div></div><p><strong>构建 Multi-head Attention 层</strong></p><div class="mb-3 syntax-highlight"><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-4-1><a class=lnlinks href=#hl-4-1> 1</a>
</span><span class=lnt id=hl-4-2><a class=lnlinks href=#hl-4-2> 2</a>
</span><span class=lnt id=hl-4-3><a class=lnlinks href=#hl-4-3> 3</a>
</span><span class=lnt id=hl-4-4><a class=lnlinks href=#hl-4-4> 4</a>
</span><span class=lnt id=hl-4-5><a class=lnlinks href=#hl-4-5> 5</a>
</span><span class=lnt id=hl-4-6><a class=lnlinks href=#hl-4-6> 6</a>
</span><span class=lnt id=hl-4-7><a class=lnlinks href=#hl-4-7> 7</a>
</span><span class=lnt id=hl-4-8><a class=lnlinks href=#hl-4-8> 8</a>
</span><span class=lnt id=hl-4-9><a class=lnlinks href=#hl-4-9> 9</a>
</span><span class=lnt id=hl-4-10><a class=lnlinks href=#hl-4-10>10</a>
</span><span class=lnt id=hl-4-11><a class=lnlinks href=#hl-4-11>11</a>
</span><span class=lnt id=hl-4-12><a class=lnlinks href=#hl-4-12>12</a>
</span><span class=lnt id=hl-4-13><a class=lnlinks href=#hl-4-13>13</a>
</span><span class=lnt id=hl-4-14><a class=lnlinks href=#hl-4-14>14</a>
</span><span class=lnt id=hl-4-15><a class=lnlinks href=#hl-4-15>15</a>
</span><span class=lnt id=hl-4-16><a class=lnlinks href=#hl-4-16>16</a>
</span><span class=lnt id=hl-4-17><a class=lnlinks href=#hl-4-17>17</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>lass</span> <span class=n>MultiHeadAttention</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>config</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>embed_dim</span> <span class=o>=</span> <span class=n>config</span><span class=o>.</span><span class=n>hidden_size</span>
</span></span><span class=line><span class=cl>        <span class=n>num_heads</span> <span class=o>=</span> <span class=n>config</span><span class=o>.</span><span class=n>num_attention_heads</span>
</span></span><span class=line><span class=cl>        <span class=n>head_dim</span> <span class=o>=</span> <span class=n>embed_dim</span> <span class=o>//</span> <span class=n>num_heads</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>heads</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ModuleList</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=p>[</span><span class=n>AttentionHead</span><span class=p>(</span><span class=n>embed_dim</span><span class=p>,</span> <span class=n>head_dim</span><span class=p>)</span> <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_heads</span><span class=p>)]</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>output_linear</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>embed_dim</span><span class=p>,</span> <span class=n>embed_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>query</span><span class=p>,</span> <span class=n>key</span><span class=p>,</span> <span class=n>value</span><span class=p>,</span> <span class=n>query_mask</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>key_mask</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>([</span>
</span></span><span class=line><span class=cl>            <span class=n>h</span><span class=p>(</span><span class=n>query</span><span class=p>,</span> <span class=n>key</span><span class=p>,</span> <span class=n>value</span><span class=p>,</span> <span class=n>query_mask</span><span class=p>,</span> <span class=n>key_mask</span><span class=p>,</span> <span class=n>mask</span><span class=p>)</span> <span class=k>for</span> <span class=n>h</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>heads</span>
</span></span><span class=line><span class=cl>        <span class=p>],</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>output_linear</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span></span></span></code></pre></td></tr></table></div></div></div><hr><p><strong>MHA & MQA & GQA</strong></p><ul><li><p><strong>MHA（Multi-head Attention）</strong>：</p><p>标准的 <strong>多头注意力机制</strong>，
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex">h</annotation></semantics></math></span>
</span>个 Query、Key 和 Value 矩阵</p></li><li><p><strong>MQA</strong>（<strong>Multi-Query Attention</strong>）：</p><p><a href=https://arxiv.org/abs/1911.02150 class=markdown-link>Fast Transformer Decoding: One Write-Head is All You Need</a></p><p><strong>多查询注意力</strong>：<strong>MQA 让所有的头之间共享同一份 Key 和 Value 矩阵，每个头只单独保留了一份 Query 参数</strong>，从而大大减少 Key 和 Value 矩阵的参数量。</p></li><li><p><strong>GQA</strong>（<strong>Grouped-Query Attention</strong>）：</p><p><a href=https://arxiv.org/abs/2305.13245 class=markdown-link>GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints</a></p><p><strong>分组查询注意力</strong>，<strong>GQA 将查询头分成 G 组，每个组共享一个 Key 和 Value 矩阵</strong>。GQA-G 是指具有
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi></mrow><annotation encoding="application/x-tex">G</annotation></semantics></math></span>
</span>组的 grouped-query attention。</p><ul><li><code>GQA-1</code> 具有单个组，因此具有单个 Key 和 Value，等效于 MQA。</li><li><code>GQA-H</code> 具有与头数相等的组，等效于 MHA。</li></ul><p>GQA 介于 MHA 和 MQA 之间。</p></li></ul><h3 id=the-feed-forward-layer-ffn class=heading>The Feed-Forward Layer (FFN)<a href=#the-feed-forward-layer-ffn aria-labelledby=the-feed-forward-layer-ffn><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h3><p>Transformer Encoder/Decoder 中的前馈子层实际上就是<strong>两层全连接神经网络</strong>，它单独地处理序列中的每一个词向量，也被称为 <strong>position-wise feed-forward layer</strong>。</p><p>常见做法是让第一层的维度是词向量大小的 4 倍，然后以 <code>GELU</code> 作为激活函数。</p><p>在下面的实现中，输入
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">x</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>B</mi><mo>×</mo><mi>L</mi><mo>×</mo><mrow><mi mathvariant="normal">h</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">_</mi><mi mathvariant="normal">s</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">z</mi><mi mathvariant="normal">e</mi></mrow></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf x \in \mathbb{R}^{B\times L \times \mathrm{hidden\_size}}</annotation></semantics></math></span>
</span>将被一个两层的 MLP 转换成形状为（B，L, <code>hidden_size</code>）的输出张量。</p><div class="mb-3 syntax-highlight"><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-5-1><a class=lnlinks href=#hl-5-1> 1</a>
</span><span class=lnt id=hl-5-2><a class=lnlinks href=#hl-5-2> 2</a>
</span><span class=lnt id=hl-5-3><a class=lnlinks href=#hl-5-3> 3</a>
</span><span class=lnt id=hl-5-4><a class=lnlinks href=#hl-5-4> 4</a>
</span><span class=lnt id=hl-5-5><a class=lnlinks href=#hl-5-5> 5</a>
</span><span class=lnt id=hl-5-6><a class=lnlinks href=#hl-5-6> 6</a>
</span><span class=lnt id=hl-5-7><a class=lnlinks href=#hl-5-7> 7</a>
</span><span class=lnt id=hl-5-8><a class=lnlinks href=#hl-5-8> 8</a>
</span><span class=lnt id=hl-5-9><a class=lnlinks href=#hl-5-9> 9</a>
</span><span class=lnt id=hl-5-10><a class=lnlinks href=#hl-5-10>10</a>
</span><span class=lnt id=hl-5-11><a class=lnlinks href=#hl-5-11>11</a>
</span><span class=lnt id=hl-5-12><a class=lnlinks href=#hl-5-12>12</a>
</span><span class=lnt id=hl-5-13><a class=lnlinks href=#hl-5-13>13</a>
</span><span class=lnt id=hl-5-14><a class=lnlinks href=#hl-5-14>14</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>FeedForward</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>config</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>linear_1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>hidden_size</span><span class=p>,</span> <span class=n>config</span><span class=o>.</span><span class=n>intermediate_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>linear_2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>intermediate_size</span><span class=p>,</span> <span class=n>config</span><span class=o>.</span><span class=n>hidden_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>gelu</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>GELU</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>hidden_dropout_prob</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>linear_1</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>gelu</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>linear_2</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span></span></span></code></pre></td></tr></table></div></div></div><h2 id=layer-normalization class=heading>Layer Normalization<a href=#layer-normalization aria-labelledby=layer-normalization><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h2><p><strong>Layer Normalization</strong> 对 <strong>单个样本基于特征维度</strong> 进行规范化。</p><p><strong>Skip Connections</strong> 则是将张量直接传递给模型的下一层而不进行处理，并将其添加到处理后的张量中。</p><p>向 Transformer Encoder/Decoder 中添加 Layer Normalization 目前共有两种做法:</p><center><img style="border-radius:.3125em;box-shadow:0 .2rem .4rem rgba(34,36,38,.12);margin:1rem" width=40% alt src=pics/arrangements_of_layer_normalization.png></center><ul><li><p><strong>Post layer normalization</strong>：</p><div class=mathjax-display><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.25em" columnalign="right" columnspacing=""><mtr><mtd class ="mtr-glue"></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mtext>Post Norm: </mtext><mspace width="1em"/><msub><mi mathvariant="bold-italic">x</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mtext>Norm</mtext><mo stretchy="false">(</mo><msub><mi mathvariant="bold-italic">x</mi><mi>t</mi></msub><mo>+</mo><msub><mi>F</mi><mi>t</mi></msub><mo stretchy="false">(</mo><msub><mi mathvariant="bold-italic">x</mi><mi>t</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd class ="mtr-glue"></mtd><mtd class ="mml-eqn-num"></mtd></mtr></mtable><annotation encoding="application/x-tex">
  \begin{align}
  \text{Post Norm: }\quad \boldsymbol{x}_{t+1} = \text{Norm}(\boldsymbol{x}_t + F_t(\boldsymbol{x}_t))
  \end{align}
  </annotation></semantics></math></span></div><p>Transformer 架构中传统的 <strong>Add&amp;Norm</strong> 使用 Post-LN，<strong>将 Layer normalization 放在 Skip Connections 之间</strong>。 但是因为梯度可能会发散，这种做法<strong>很难训练</strong>，还需要结合<strong>学习率预热 (learning rate warm-up)</strong> 等技巧；</p></li><li><p><strong>Pre layer normalization</strong>：</p><div class=mathjax-display><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.25em" columnalign="right" columnspacing=""><mtr><mtd class ="mtr-glue"></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mtext>Pre Norm: </mtext><mspace width="1em"/><msub><mi mathvariant="bold-italic">x</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi mathvariant="bold-italic">x</mi><mi>t</mi></msub><mo>+</mo><msub><mi>F</mi><mi>t</mi></msub><mo stretchy="false">(</mo><mtext>Norm</mtext><mo stretchy="false">(</mo><msub><mi mathvariant="bold-italic">x</mi><mi>t</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd class ="mtr-glue"></mtd><mtd class ="mml-eqn-num"></mtd></mtr></mtable><annotation encoding="application/x-tex">
    \begin{align}
    \text{Pre Norm: } \quad \boldsymbol{x}_{t+1} = \boldsymbol{x}_t + F_t(\text{Norm}(\boldsymbol{x}_t))\\
    \end{align}
    </annotation></semantics></math></span></div><p>目前主流的做法，<strong>将 Layer Normalization 放置于 Skip Connections 的范围内</strong>。同一设置之下，Pre Norm 结构往往更容易训练，并且<strong>不需要学习率预热</strong>，但最终效果通常不如 Post Norm。</p></li></ul><div class="mb-3 syntax-highlight"><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-6-1><a class=lnlinks href=#hl-6-1> 1</a>
</span><span class=lnt id=hl-6-2><a class=lnlinks href=#hl-6-2> 2</a>
</span><span class=lnt id=hl-6-3><a class=lnlinks href=#hl-6-3> 3</a>
</span><span class=lnt id=hl-6-4><a class=lnlinks href=#hl-6-4> 4</a>
</span><span class=lnt id=hl-6-5><a class=lnlinks href=#hl-6-5> 5</a>
</span><span class=lnt id=hl-6-6><a class=lnlinks href=#hl-6-6> 6</a>
</span><span class=lnt id=hl-6-7><a class=lnlinks href=#hl-6-7> 7</a>
</span><span class=lnt id=hl-6-8><a class=lnlinks href=#hl-6-8> 8</a>
</span><span class=lnt id=hl-6-9><a class=lnlinks href=#hl-6-9> 9</a>
</span><span class=lnt id=hl-6-10><a class=lnlinks href=#hl-6-10>10</a>
</span><span class=lnt id=hl-6-11><a class=lnlinks href=#hl-6-11>11</a>
</span><span class=lnt id=hl-6-12><a class=lnlinks href=#hl-6-12>12</a>
</span><span class=lnt id=hl-6-13><a class=lnlinks href=#hl-6-13>13</a>
</span><span class=lnt id=hl-6-14><a class=lnlinks href=#hl-6-14>14</a>
</span><span class=lnt id=hl-6-15><a class=lnlinks href=#hl-6-15>15</a>
</span><span class=lnt id=hl-6-16><a class=lnlinks href=#hl-6-16>16</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>TransformerEncoderLayer</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>config</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>layer_norm_1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LayerNorm</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>hidden_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>layer_norm_2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LayerNorm</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>hidden_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>attention</span> <span class=o>=</span> <span class=n>MultiHeadAttention</span><span class=p>(</span><span class=n>config</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>feed_forward</span> <span class=o>=</span> <span class=n>FeedForward</span><span class=p>(</span><span class=n>config</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># Apply layer normalization and then copy input into query, key, value</span>
</span></span><span class=line><span class=cl>        <span class=n>hidden_state</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>layer_norm_1</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># Apply attention with a skip connection</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>x</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>attention</span><span class=p>(</span><span class=n>hidden_state</span><span class=p>,</span> <span class=n>hidden_state</span><span class=p>,</span> <span class=n>hidden_state</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=n>mask</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># Apply feed-forward layer with a skip connection</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>x</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>feed_forward</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>layer_norm_2</span><span class=p>(</span><span class=n>x</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span></span></span></code></pre></td></tr></table></div></div></div><h3 id=post-norm class=heading>Post Norm<a href=#post-norm aria-labelledby=post-norm><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h3><p>假设初始状态下
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span>
</span>,
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">F(x)</annotation></semantics></math></span>
</span>的方差均为
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn></mrow><annotation encoding="application/x-tex">1</annotation></semantics></math></span>
</span>，那么
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>+</mo><mi>F</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">x+F(x)</annotation></semantics></math></span>
</span>的方差就是
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn></mrow><annotation encoding="application/x-tex">2</annotation></semantics></math></span>
</span>，而 Normalization 操作负责将方差重新降为
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn></mrow><annotation encoding="application/x-tex">1</annotation></semantics></math></span>
</span>，这就说明初始阶段 Post Norm 相当于</p><div class=mathjax-display><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.16em" columnspacing="1em"><mtr><mtd class ="mtr-glue"></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><msub><mi>x</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mfrac><mrow><msub><mi>x</mi><mi>t</mi></msub><mo>+</mo><msub><mi>F</mi><mi>t</mi></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><msqrt><mn>2</mn></msqrt></mfrac></mrow></mstyle></mtd><mtd class ="mtr-glue"></mtd><mtd class ="mml-eqn-num"></mtd></mtr></mtable><annotation encoding="application/x-tex">
\begin{equation}
x_{t+1} = \frac{x_t + F_t(x_t)}{\sqrt{2}}
\end{equation}
</annotation></semantics></math></span></div><p>递归下去，我们得到</p><div class=mathjax-display><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.16em" columnspacing="1em"><mtr><mtd class ="mtr-glue"></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mtable rowspacing="0.25em" columnalign="right left" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><msub><mi>x</mi><mi>l</mi></msub><mo>=</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mtext> </mtext><mfrac><msub><mi>x</mi><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></msub><msqrt><mn>2</mn></msqrt></mfrac><mo>+</mo><mfrac><mrow><msub><mi>F</mi><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow><msqrt><mn>2</mn></msqrt></mfrac></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mo lspace="0em" rspace="0em">=</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mtext> </mtext><mfrac><msub><mi>x</mi><mrow><mi>l</mi><mo>−</mo><mn>2</mn></mrow></msub><mn>2</mn></mfrac><mo>+</mo><mfrac><mrow><msub><mi>F</mi><mrow><mi>l</mi><mo>−</mo><mn>2</mn></mrow></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>l</mi><mo>−</mo><mn>2</mn></mrow></msub><mo stretchy="false">)</mo></mrow><mn>2</mn></mfrac><mo>+</mo><mfrac><mrow><msub><mi>F</mi><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow><msqrt><mn>2</mn></msqrt></mfrac></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mo lspace="0em" rspace="0em">=</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mtext> </mtext><mo>⋯</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mo lspace="0em" rspace="0em">=</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mtext> </mtext><mfrac><msub><mi>x</mi><mn>0</mn></msub><msup><mn>2</mn><mrow><mi>l</mi><mi mathvariant="normal">/</mi><mn>2</mn></mrow></msup></mfrac><mo>+</mo><mfrac><mrow><msub><mi>F</mi><mn>0</mn></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo stretchy="false">)</mo></mrow><msup><mn>2</mn><mrow><mi>l</mi><mi mathvariant="normal">/</mi><mn>2</mn></mrow></msup></mfrac><mo>+</mo><mfrac><mrow><msub><mi>F</mi><mn>1</mn></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo stretchy="false">)</mo></mrow><msup><mn>2</mn><mrow><mo stretchy="false">(</mo><mi>l</mi><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo><mi mathvariant="normal">/</mi><mn>2</mn></mrow></msup></mfrac><mo>+</mo><mfrac><mrow><msub><mi>F</mi><mn>2</mn></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mn>2</mn></msub><mo stretchy="false">)</mo></mrow><msup><mn>2</mn><mrow><mo stretchy="false">(</mo><mi>l</mi><mo>−</mo><mn>2</mn><mo stretchy="false">)</mo><mi mathvariant="normal">/</mi><mn>2</mn></mrow></msup></mfrac><mo>+</mo><mo>⋯</mo><mo>+</mo><mfrac><mrow><msub><mi>F</mi><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow><msup><mn>2</mn><mrow><mn>1</mn><mi mathvariant="normal">/</mi><mn>2</mn></mrow></msup></mfrac></mrow></mstyle></mtd></mtr></mtable></mstyle></mtd><mtd class ="mtr-glue"></mtd><mtd class ="mml-eqn-num"></mtd></mtr></mtable><annotation encoding="application/x-tex">
\begin{equation}\begin{aligned} 
x_l =&amp;\, \frac{x_{l-1}}{\sqrt{2}} + \frac{F_{l-1}(x_{l-1})}{\sqrt{2}} \\ 
=&amp;\, \frac{x_{l-2}}{2} + \frac{F_{l-2}(x_{l-2})}{2} + \frac{F_{l-1}(x_{l-1})}{\sqrt{2}} \\[7pt]
=&amp;\, \cdots \\[7pt]
=&amp;\,\frac{x_0}{2^{l/2}} + \frac{F_0(x_0)}{2^{l/2}} + \frac{F_1(x_1)}{2^{(l-1)/2}} + \frac{F_2(x_2)}{2^{(l-2)/2}} + \cdots + \frac{F_{l-1}(x_{l-1})}{2^{1/2}} 
\end{aligned}\end{equation}
</annotation></semantics></math></span></div><p>本来残差的意思是给前面的层搞一条“绿色通道”，让梯度可以更直接地回传，但是在 Post Norm 中，这条“绿色通道”被严重削弱了，<strong>越靠近前面的通道反而权重越小，残差“名存实亡</strong>”，因此还是不容易训练。</p><h3 id=pre-norm class=heading>Pre Norm<a href=#pre-norm aria-labelledby=pre-norm><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h3><p><strong>Pre Norm</strong>迭代展开之后有：</p><div class=mathjax-display><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.16em" columnspacing="1em"><mtr><mtd class ="mtr-glue"></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><msub><mi>x</mi><mi>l</mi></msub><mo>=</mo><msub><mi>x</mi><mn>0</mn></msub><mo>+</mo><msub><mi>F</mi><mn>0</mn></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo stretchy="false">)</mo><mo>+</mo><msub><mi>F</mi><mn>1</mn></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mn>1</mn></msub><mi mathvariant="normal">/</mi><msqrt><mn>2</mn></msqrt><mo stretchy="false">)</mo><mo>+</mo><msub><mi>F</mi><mn>2</mn></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mn>2</mn></msub><mi mathvariant="normal">/</mi><msqrt><mn>3</mn></msqrt><mo stretchy="false">)</mo><mo>+</mo><mo>⋯</mo><mo>+</mo><msub><mi>F</mi><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></msub><mi mathvariant="normal">/</mi><msqrt><mi>l</mi></msqrt><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd class ="mtr-glue"></mtd><mtd class ="mml-eqn-num"></mtd></mtr></mtable><annotation encoding="application/x-tex">
\begin{equation} 
x_l = x_0 + F_0(x_0) + F_1(x_1/\sqrt{2}) + F_2(x_2/\sqrt{3}) + \cdots + F_{l-1}(x_{l-1}/\sqrt{l})\end{equation}
</annotation></semantics></math></span></div><p>每一条残差通道都是平权的，<strong>残差的作用会比 Post Norm 更加明显，所以它也更好优化</strong>。当然，这样最后的
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>l</mi></msub></mrow><annotation encoding="application/x-tex">x_l</annotation></semantics></math></span>
</span>方差将会很大，所以在接预测层之前
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>l</mi></msub></mrow><annotation encoding="application/x-tex">x_l</annotation></semantics></math></span>
</span>也还要加个 Normalization。</p><p><strong>一个
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi></mrow><annotation encoding="application/x-tex">L</annotation></semantics></math></span>
</span>层 Pre Norm 的模型，其实际等效层数不如
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi></mrow><annotation encoding="application/x-tex">L</annotation></semantics></math></span>
</span>层的 Post Norm 模型，而层数少了导致效果变差了</strong></p><div class=mathjax-display><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.25em" columnalign="right left" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><msub><mi mathvariant="bold">x</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mtext> </mtext><msub><mi mathvariant="bold">x</mi><mi>t</mi></msub><mo>+</mo><msub><mi>F</mi><mi>t</mi></msub><mo stretchy="false">(</mo><mtext>Norm</mtext><mo stretchy="false">(</mo><msub><mi mathvariant="bold">x</mi><mi>t</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mo lspace="0em" rspace="0em">=</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mtext> </mtext><msub><mi mathvariant="bold">x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>+</mo><msub><mi>F</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">(</mo><mtext>Norm</mtext><mo stretchy="false">(</mo><msub><mi mathvariant="bold">x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>+</mo><msub><mi>F</mi><mi>t</mi></msub><mo stretchy="false">(</mo><mtext>Norm</mtext><mo stretchy="false">(</mo><msub><mi mathvariant="bold">x</mi><mi>t</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mo lspace="0em" rspace="0em">=</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mtext> </mtext><mo>⋯</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mo lspace="0em" rspace="0em">=</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mtext> </mtext><msub><mi mathvariant="bold">x</mi><mn>0</mn></msub><mo>+</mo><msub><mi>F</mi><mn>0</mn></msub><mo stretchy="false">(</mo><mtext>Norm</mtext><mo stretchy="false">(</mo><msub><mi mathvariant="bold">x</mi><mn>0</mn></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>+</mo><mo>⋯</mo><mo>+</mo><msub><mi>F</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">(</mo><mtext>Norm</mtext><mo stretchy="false">(</mo><msub><mi mathvariant="bold">x</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>+</mo><msub><mi>F</mi><mi>t</mi></msub><mo stretchy="false">(</mo><mtext>Norm</mtext><mo stretchy="false">(</mo><msub><mi mathvariant="bold">x</mi><mi>t</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">
\begin{aligned} 
\mathbf{x}_{t+1} =&amp;\,\mathbf{x}_t + F_t(\text{Norm}(\mathbf{x}_t)) \\[7pt]
=&amp;\, \mathbf{x}_{t-1} + F_{t-1}(\text{Norm}(\mathbf{x}_{t-1})) +  F_t(\text{Norm}(\mathbf{x}_t)) \\[7pt]
=&amp;\, \cdots \\[7pt]
=&amp;\, \mathbf{x}_0 + F_0 (\text{Norm}(\mathbf{x}_0)) + \cdots + F_{t-1}(\text{Norm}(\mathbf{x}_{t-1})) +  F_t(\text{Norm}(\mathbf{x}_t)) 
\end{aligned}
</annotation></semantics></math></span></div><p>其中每一项都是同一量级的，那么有
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">x</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mi mathvariant="script">O</mi><mo stretchy="false">(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex"> \mathbf{x}_{t+1}=\mathscr{O}(t+1)</annotation></semantics></math></span>
</span>，当
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span>
</span>较大时，两者的相对差别很小，因此</p><div class=mathjax-display><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.25em" columnalign="right left" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mtext> </mtext><msub><mi>F</mi><mi>t</mi></msub><mo stretchy="false">(</mo><mtext>Norm</mtext><mo stretchy="false">(</mo><msub><mi mathvariant="bold">x</mi><mi>t</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>+</mo><msub><mi>F</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">(</mo><mtext>Norm</mtext><mo stretchy="false">(</mo><msub><mi mathvariant="bold">x</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mo lspace="0em" rspace="0em">≈</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mtext> </mtext><msub><mi>F</mi><mi>t</mi></msub><mo stretchy="false">(</mo><mtext>Norm</mtext><mo stretchy="false">(</mo><msub><mi mathvariant="bold">x</mi><mi>t</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>+</mo><msub><mi>F</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">(</mo><mtext>Norm</mtext><mo stretchy="false">(</mo><msub><mi mathvariant="bold">x</mi><mi>t</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mo lspace="0em" rspace="0em">=</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mrow></mrow><mtext> </mtext><mo stretchy="false">(</mo><msub><mi>F</mi><mi>t</mi></msub><mo>⊕</mo><msub><mi>F</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo><mo stretchy="false">(</mo><mtext>Norm</mtext><mo stretchy="false">(</mo><msub><mi mathvariant="bold">x</mi><mi>t</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">
\begin{aligned} 
&amp;\, F_t(\text{Norm}(\mathbf{x}_t)) + F_{t+1}(\text{Norm}(\mathbf{x}_{t+1})) \\[7pt]
\approx&amp;\, F_t(\text{Norm}(\mathbf{x}_t)) + F_{t+1}(\text{Norm}(\mathbf{x}_t)) \\[7pt] 
=&amp;\, (F_t\oplus F_{t+1})(\text{Norm}(\mathbf{x}_t)) 
\end{aligned}
</annotation></semantics></math></span></div><p>即当
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span>
</span>比较大时，
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">x</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{x}_t</annotation></semantics></math></span>
</span>，
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">x</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">\mathbf{x}_{t+1}</annotation></semantics></math></span>
</span>相差较小，所以
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>F</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">(</mo><mtext>Norm</mtext><mo stretchy="false">(</mo><msub><mi mathvariant="bold">x</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">F_{t+1}(\text{Norm}(\mathbf{x}_{t+1}))</annotation></semantics></math></span>
</span>与
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>F</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">(</mo><mtext>Norm</mtext><mo stretchy="false">(</mo><msub><mi mathvariant="bold">x</mi><mi>t</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">F_{t+1}(\text{Norm}(\mathbf{x}_{t}))</annotation></semantics></math></span>
</span>很接近，因此原本一个
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span>
</span>层的模型与
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">t+1</annotation></semantics></math></span>
</span>层之和，近似等效于一个更宽的
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span>
</span>层模型，<strong>所以在 Pre Norm 中多层叠加的结果更多是增加宽度而不是深度</strong>，层数越多，这个层就越“虚”。</p><p><a href=https://papers.cool/arxiv/2203.00555 class=markdown-link>DeepNet</a>:</p><blockquote class=blockquote><p>However, the gradients of Pre-LN at bottom layers tend to be larger than at top layers, leading to a degradation in performance compared with Post-LN.</p></blockquote><p><strong>Pre Norm</strong> 结构会过度倾向于<strong>恒等分支（bottom layers）</strong>，从而使得 <strong>Pre Norm</strong> 倾向于退化（degradation）为一个“浅而宽”的模型，最终不如同一深度的 Post Norm.</p><h3 id=warmup的作用 class=heading>Warmup的作用<a href=#warmup%e7%9a%84%e4%bd%9c%e7%94%a8 aria-labelledby=warmup的作用><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h3><p>根据泰勒展开式：</p><div class=mathjax-display><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.16em" columnspacing="1em"><mtr><mtd class ="mtr-glue"></mtd><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo>+</mo><mi mathvariant="normal">Δ</mi><mi>x</mi><mo stretchy="false">)</mo><mo>≈</mo><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>+</mo><mo stretchy="false">⟨</mo><msub><mi mathvariant="normal">∇</mi><mi>x</mi></msub><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mi mathvariant="normal">Δ</mi><mi>x</mi><mo stretchy="false">⟩</mo></mrow></mstyle></mtd><mtd class ="mtr-glue"></mtd><mtd class ="mml-eqn-num"></mtd></mtr></mtable><annotation encoding="application/x-tex">
\begin{equation}f(x+\Delta x) \approx f(x) + \langle\nabla_x f(x), \Delta x\rangle\end{equation}
</annotation></semantics></math></span></div><p><strong>即增量
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo>+</mo><mi mathvariant="normal">Δ</mi><mi>x</mi><mo stretchy="false">)</mo><mo>−</mo><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(x+\Delta x) - f(x)</annotation></semantics></math></span>
</span>正比于梯度</strong>，换句话说，梯度衡量了输出对输入的依赖程度。如果梯度消失，那么意味着模型的输出对输入的依赖变弱了。</p><p><strong>Warmup 是在训练开始阶段，将学习率从 0 缓增到指定大小，而不是一开始从指定大小训练。</strong></p><p>如果不进行 Wamrup，那么模型一开始就快速地学习，由于梯度消失，<strong>模型对越靠后的层越敏感，也就是越靠后的层学习得越快</strong>，然后后面的层是以前面的层的输出为输入的，前面的层根本就没学好，所以后面的层虽然学得快，但却是建立在糟糕的输入基础上的。</p><p>很快地，后面的层以糟糕的输入为基础到达了一个糟糕的局部最优点，此时它的学习开始放缓（因为已经到达了它认为的最优点附近），同时反向传播给前面层的梯度信号进一步变弱，这就导致了前面的层的梯度变得不准。但Adam 的更新量是常数量级的，梯度不准，但更新量依然是数量级，意味着可能就是一个常数量级的随机噪声了，于是学习方向开始不合理，前面的输出开始崩盘，导致后面的层也一并崩盘。</p><ul><li>如果 Post Norm 结构的模型不进行 Wamrup，我们能观察到的现象往往是：loss 快速收敛到一个常数附近，然后再训练一段时间，loss 开始发散，直至 NAN。</li><li>如果进行 Wamrup，那么留给模型足够多的时间进行“预热”，在这个过程中，<strong>主要是抑制了后面的层的学习速度，并且给了前面的层更多的优化时间，以促进每个层的同步优化</strong>。</li></ul><p>这里的讨论前提是梯度消失，如果是 Pre Norm 之类的结果，没有明显的梯度消失现象，那么不加 Warmup 往往也可以成功训练。</p><h1 id=reference class=heading>Reference<a href=#reference aria-labelledby=reference><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h1><ul><li><a href=https://arxiv.org/abs/1706.03762 class=markdown-link>Attention Is All You Need</a></li><li><a href=https://transformers.run/ class=markdown-link>Transformers快速入门</a></li><li><a href=https://jalammar.github.io/illustrated-transformer/ class=markdown-link>The Illustrated Transformer</a></li><li><a href=https://nlp.seas.harvard.edu/2018/04/03/attention.html class=markdown-link>The Annotated Transformer</a></li><li><a href=https://spaces.ac.cn/archives/8620 class=markdown-link>浅谈Transformer的初始化、参数化与标准化</a></li><li><a href=https://spaces.ac.cn/archives/9009 class=markdown-link>为什么Pre Norm的效果不如Post Norm？</a></li><li><a href=https://spaces.ac.cn/archives/8747 class=markdown-link>模型优化漫谈：BERT的初始标准差为什么是0.02？</a></li></ul></div><div class="row row-cols-2 mt-5 mb-3"><div class=col><a class=next href=/blogs/flashattention/><svg class="svg-inline--fa fas fa-arrow-left" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 448 512"><use href="#fas-arrow-left"/></svg>&nbsp;FlashAttention</a></div><div class="col text-end"><a class=previous href=/blogs/agents/>Agents 框架&nbsp;<svg class="svg-inline--fa fas fa-arrow-right" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 448 512"><use href="#fas-arrow-right"/></svg></a></div></div><a href=# id=back-to-top class=back-to-top><span>▲</span>
</a><a href=# id=scroll-to-bottom class=back-to-top><span>▼</span>
</a><script>document.addEventListener("DOMContentLoaded",function(){var e=document.getElementById("back-to-top"),t=document.getElementById("scroll-to-bottom");e.addEventListener("click",function(e){e.preventDefault(),window.scrollTo({top:0,behavior:"smooth"})}),t.addEventListener("click",function(e){e.preventDefault(),window.scrollTo({top:document.body.scrollHeight,behavior:"smooth"})})})</script></div><div class="col col-md-3 col-lg-2 d-none d-md-block pt-5"><div class="toc toc-sidebar mb-5 my-md-0 mb-lg-5 p-3 text-body-secondary sticky-top"><strong class="d-block h6 my-2 pt-4">On this page:</strong><nav class=toc><a class="toc-item toc-level-1" href=/blogs/transformer/#transformer-家族>Transformer 家族 </a><a class="toc-item toc-level-2" href=/blogs/transformer/#encoder-分支>Encoder 分支 </a><a class="toc-item toc-level-2" href=/blogs/transformer/#decoder-分支>Decoder 分支 </a><a class="toc-item toc-level-2" href=/blogs/transformer/#encoder-decoder-分支>Encoder-Decoder 分支 </a><a class="toc-item toc-level-1" href=/blogs/transformer/#transformer-layer>Transformer Layer </a><a class="toc-item toc-level-2" href=/blogs/transformer/#self-attention>Self-Attention </a><a class="toc-item toc-level-2" href=/blogs/transformer/#the-feed-forward-layer-ffn>The Feed-Forward Layer (FFN) </a><a class="toc-item toc-level-1" href=/blogs/transformer/#layer-normalization>Layer Normalization </a><a class="toc-item toc-level-2" href=/blogs/transformer/#post-norm>Post Norm </a><a class="toc-item toc-level-2" href=/blogs/transformer/#pre-norm>Pre Norm </a><a class="toc-item toc-level-2" href=/blogs/transformer/#warmup的作用>Warmup的作用</a></nav></div></div></div></div><footer class="container-fluid footer text-center p-3"><div class="container-xxl text-center"><small>Copyright © 2025 EV's Blog All rights reserved.
|
Powered by
<a href=https://gethinode.com class="link-bg-footer markdown-link">Hinode</a>.</small></div></footer></div><div id=toast-container class="toast-container position-fixed bottom-0 end-0 p-3"><div id=toast-copied-code-message class=toast role=alert aria-live=assertive aria-atomic=true><div class=toast-header><strong class=me-auto>EV's Blog</strong>
<button type=button class=btn-close data-bs-dismiss=toast aria-label=Close></button></div><div class=toast-body>Code copied to clipboard</div></div></div><svg xmlns:xlink="http://www.w3.org/1999/xlink" display="none"><symbol id="fas-ellipsis"><path d="M8 256a56 56 0 11112 0A56 56 0 118 256zm160 0a56 56 0 11112 0 56 56 0 11-112 0zm216-56a56 56 0 110 112 56 56 0 110-112z"/></symbol><symbol id="fas-sun"><path d="M361.5 1.2c5 2.1 8.6 6.6 9.6 11.9L391 121l107.9 19.8c5.3 1 9.8 4.6 11.9 9.6s1.5 10.7-1.6 15.2L446.9 256l62.3 90.3c3.1 4.5 3.7 10.2 1.6 15.2s-6.6 8.6-11.9 9.6L391 391 371.1 498.9c-1 5.3-4.6 9.8-9.6 11.9s-10.7 1.5-15.2-1.6L256 446.9l-90.3 62.3c-4.5 3.1-10.2 3.7-15.2 1.6s-8.6-6.6-9.6-11.9L121 391 13.1 371.1c-5.3-1-9.8-4.6-11.9-9.6s-1.5-10.7 1.6-15.2L65.1 256 2.8 165.7c-3.1-4.5-3.7-10.2-1.6-15.2s6.6-8.6 11.9-9.6L121 121 140.9 13.1c1-5.3 4.6-9.8 9.6-11.9s10.7-1.5 15.2 1.6L256 65.1 346.3 2.8c4.5-3.1 10.2-3.7 15.2-1.6zM160 256a96 96 0 11192 0 96 96 0 11-192 0zm224 0a128 128 0 10-256 0 128 128 0 10256 0z"/></symbol><symbol id="fas-moon"><path d="M223.5 32C1e2 32 0 132.3.0 256S1e2 480 223.5 480c60.6.0 115.5-24.2 155.8-63.4 5-4.9 6.3-12.5 3.1-18.7s-10.1-9.7-17-8.5c-9.8 1.7-19.8 2.6-30.1 2.6-96.9.0-175.5-78.8-175.5-176 0-65.8 36-123.1 89.3-153.3 6.1-3.5 9.2-10.5 7.7-17.3s-7.3-11.9-14.3-12.5c-6.3-.5-12.6-.8-19-.8z"/></symbol><symbol id="fas-angle-left"><path d="M41.4 233.4c-12.5 12.5-12.5 32.8.0 45.3l160 160c12.5 12.5 32.8 12.5 45.3.0s12.5-32.8.0-45.3L109.3 256 246.6 118.6c12.5-12.5 12.5-32.8.0-45.3s-32.8-12.5-45.3.0l-160 160z"/></symbol><symbol id="fas-sort"><path d="M137.4 41.4c12.5-12.5 32.8-12.5 45.3.0l128 128c9.2 9.2 11.9 22.9 6.9 34.9S301 224.1 288 224.1L32 224c-12.9.0-24.6-7.8-29.6-19.8s-2.2-25.7 6.9-34.9l128-128zm0 429.3-128-128c-9.2-9.2-11.9-22.9-6.9-34.9S19.1 288 32.1 288h256c12.9.0 24.6 7.8 29.6 19.8s2.2 25.7-6.9 34.9l-128 128c-12.5 12.5-32.8 12.5-45.3.0z"/></symbol><symbol id="fas-arrow-left"><path d="M9.4 233.4c-12.5 12.5-12.5 32.8.0 45.3l160 160c12.5 12.5 32.8 12.5 45.3.0s12.5-32.8.0-45.3L109.2 288H416c17.7.0 32-14.3 32-32s-14.3-32-32-32H109.3L214.6 118.6c12.5-12.5 12.5-32.8.0-45.3s-32.8-12.5-45.3.0l-160 160z"/></symbol><symbol id="fas-arrow-right"><path d="M438.6 278.6c12.5-12.5 12.5-32.8.0-45.3l-160-160c-12.5-12.5-32.8-12.5-45.3.0s-12.5 32.8.0 45.3L338.8 224H32c-17.7.0-32 14.3-32 32s14.3 32 32 32h306.7L233.4 393.4c-12.5 12.5-12.5 32.8.0 45.3s32.8 12.5 45.3.0l160-160z"/></symbol></svg>
<script src=/js/core.bundle-analytics.en.min.ceb6a67c169a28031391976dac91e1e2f460951862201b6249516a55d0fd6109.js data-category=analytics integrity="sha256-zramfBaaKAMTkZdtrJHh4vRglRhiIBtiSVFqVdD9YQk=" crossorigin=anonymous async></script><script src=/js/core.bundle.en.min.82868b7252e09bfbfda9583f6e99ff753228964cf66772429255a8d5ff982f48.js integrity="sha256-goaLclLgm/v9qVg/bpn/dTIolkz2Z3JCklWo1f+YL0g=" crossorigin=anonymous async></script></body></html>
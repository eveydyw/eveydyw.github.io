<!doctype html><html lang=en class=no-js><head><script src=/js/critical.bundle.min.85a7f5f23dc031d38877ecdb71b00d49e8a62c54f1ba98e75a734706ea09f30a.js integrity="sha256-haf18j3AMdOId+zbcbANSeimLFTxupjnWnNHBuoJ8wo=" crossorigin=anonymous></script><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=generator content="Hugo 0.143.1"><meta name=theme content="Hinode 0.29.3"><link rel=stylesheet href="/css/main.min.87520833af13a26c9eab0db4b8b9e6c7afddf5dd51d32505d34d673a925cb9af.css" integrity="sha256-h1IIM68Tomyeqw20uLnmx6/d9d1R0yUF001nOpJcua8=" crossorigin=anonymous><link rel=preload href=/fonts/inter-v12-latin-regular.woff2 as=font type=font/woff2 crossorigin><meta name=robots content="index, follow"><meta name=googlebot content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1"><meta name=bingbot content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1"><title>EV's Blog - SearchEngine-2-相关性</title>
<meta name=description content="【笔记】wangshusen-搜索引擎技术：相关性"><meta property="og:locale" content="en_US"><meta property="og:type" content="article"><meta property="og:title" content="SearchEngine-2-相关性"><meta property="og:description" content="【笔记】wangshusen-搜索引擎技术：相关性"><meta property="og:url" content="https://eveydyw.github.io/blogs/searchengine/rel/"><meta property="og:site_name" content="EV's Blog"><meta property="article:published_time" content="2024-10-17T20:37:39+08:00"><meta property="article:modified_time" content="2024-10-17T20:37:39+08:00"><meta property="og:image" content="https://eveydyw.github.io/img/logo1280x640.png"><meta property="og:image:alt" content="SearchEngine-2-相关性"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="SearchEngine-2-相关性"><meta name=twitter:description content="【笔记】wangshusen-搜索引擎技术：相关性"><meta name=twitter:image content="https://eveydyw.github.io/img/logo1280x640.png"><meta name=twitter:image:alt content="SearchEngine-2-相关性"><script type=application/ld+json>{"@context":"https://schema.org","@graph":[{"@type":"Organization","@id":"https://eveydyw.github.io/#/schema/organization/1","name":"Hinode","url":"https://eveydyw.github.io/","sameAs":[null,"https://github.com/gethinode/hinode"],"logo":{"@type":"ImageObject","@id":"https://eveydyw.github.io/#/schema/image/1","url":"https://eveydyw.github.io/img/logo512x512.png","width":512,"height":512,"caption":"Hinode"},"image":{"@id":"https://eveydyw.github.io/#/schema/image/1"}},{"@type":"WebSite","@id":"https://eveydyw.github.io/#/schema/website/1","url":"https://eveydyw.github.io/","name":"EV\u0027s Blog","description":"Hinode is a clean documentation and blog theme for your Hugo site based on Bootstrap 5.","publisher":{"@id":"https://eveydyw.github.io/#/schema/organization/1"}},{"@type":"WebPage","@id":"https://eveydyw.github.io/blogs/searchengine/rel/","url":"https://eveydyw.github.io/blogs/searchengine/rel/","name":"SearchEngine-2-相关性","description":"【笔记】wangshusen-搜索引擎技术：相关性","isPartOf":{"@id":"https://eveydyw.github.io/#/schema/website/1"},"about":{"@id":"https://eveydyw.github.io/#/schema/organization/1"},"datePublished":"2024-10-17T20:37:39CET","dateModified":"2024-10-17T20:37:39CET","breadcrumb":{"@id":"https://eveydyw.github.io/blogs/searchengine/rel/#/schema/breadcrumb/1"},"primaryImageOfPage":{"@id":"https://eveydyw.github.io/blogs/searchengine/rel/#/schema/image/2"},"inLanguage":"en-US","potentialAction":[{"@type":"ReadAction","target":["https://eveydyw.github.io/blogs/searchengine/rel/"]}]},{"@type":"BreadcrumbList","@id":"https://eveydyw.github.io/blogs/searchengine/rel/#/schema/breadcrumb/1","name":"Breadcrumbs","itemListElement":[{"@type":"ListItem","position":1,"item":{"@type":"WebPage","@id":"https://eveydyw.github.io/","url":"https://eveydyw.github.io/","name":"Home"}},{"@type":"ListItem","position":2,"item":{"@type":"WebPage","@id":"https://eveydyw.github.io/blogs/","url":"https://eveydyw.github.io/blogs/","name":"Blogs"}},{"@type":"ListItem","position":3,"item":{"@type":"WebPage","@id":"https://eveydyw.github.io/blogs/searchengine/","url":"https://eveydyw.github.io/blogs/searchengine/","name":"Searchengine"}},{"@type":"ListItem","position":4,"item":{"@id":"https://eveydyw.github.io/blogs/searchengine/rel/"}}]},{"@context":"https://schema.org","@graph":[{"@type":"ImageObject","@id":"https://eveydyw.github.io/blogs/searchengine/rel/#/schema/image/2","url":"https://eveydyw.github.io/img/logo1280x640.png","contentUrl":"https://eveydyw.github.io/img/logo1280x640.png","caption":"SearchEngine-2-相关性"}]}]}</script><link rel=icon type=image/png sizes=16x16 href=/img/my_logo_hu_10729e67805acfd8.png><link rel=icon type=image/png sizes=32x32 href=/img/my_logo_hu_f267cd4735c3fb50.png><link rel=icon type=image/png sizes=48x48 href=/img/my_logo_hu_bc348718a5ba4099.png><link rel=apple-touch-icon sizes=180x180 href=/img/my_logo_hu_a8f52fe79483cdab.png><script>MathJax={loader:{load:["[tex]/html","[tex]/ams","[tex]/amscd"]},tex:{inlineMath:[["\\(","\\)"],["$","$"]],displayMath:[["\\[","\\]"],["$$","$$"]],processEscapes:!0,packages:["base","ams","amscd"],tags:"ams"},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"],renderActions:{addMenu:[0,"",""],checkLoading:[0,"",""]}},chtml:{scale:1,displayAlign:"center",displayIndent:"0em",lineWidth:"container"},svg:{fontCache:"global"}}</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-svg.js></script></head><body><div class="d-flex flex-column min-vh-100"><div class="d-flex flex-column"><div class="container-fluid fixed-top p-0"><nav class="navbar p-4 bg-body navbar-fixed-top navbar-expand-md"><div class="container-xxl p-0"><div class="d-flex navbar-container justify-content-center"><div class="d-flex align-items-center"><button class="navbar-toggler collapsed p-0 mx-auto invisible fw-30" type=button><svg class="svg-inline--fa fas fa-ellipsis fa-fw" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 448 512"><use href="#fas-ellipsis"/></svg></button></div><div class=mx-auto><a class=navbar-brand href=/ aria-label=Home><img src=/img/my_logo.svg alt="EV's Blog logo" height=30 width=30></a></div><div class="d-flex align-items-center"><button class="navbar-toggler main-nav-toggler collapsed p-0" type=button data-bs-toggle=collapse data-bs-target=#navbar-0-collapse aria-controls=navbar-0 aria-expanded=false aria-label="Toggle main navigation">
<span class="toggler-icon top-bar emphasis"></span>
<span class="toggler-icon middle-bar emphasis"></span>
<span class="toggler-icon bottom-bar emphasis"></span></button></div></div><div class="navbar-collapse collapse" id=navbar-0-collapse><div class="d-flex flex-fill ms-md-3 mt-4 mt-md-0"><form class="search flex-fill position-relative me-auto"><input class="search-input form-control is-search" type=search placeholder="Search this site" aria-label="Search this site" autocomplete=off name=search-input><div class="search-suggestions shadow bg-body rounded d-none" data-no-results="No results for"></div></form></div><ul class="navbar-nav ms-auto"><li class=nav-item><a class=nav-link data-nav=main data-nav-main=home href=/><span>Home</span>&nbsp;</a></li><li class=nav-item><a class=nav-link data-nav=main data-nav-main=blogs href=/blogs/><span>Blogs</span>&nbsp;</a></li><li class=nav-item><a class=nav-link data-nav=main data-nav-main=tags href=/tags><span>Tags</span>&nbsp;</a></li><li class="d-flex mode-switch align-items-center" id=navbar-mode><input type=checkbox class="checkbox navbar-mode-selector" id=navbar-mode-checkbox aria-label="Toggle theme">
<label class=label for=navbar-mode-checkbox><svg class="svg-inline--fa fas fa-sun fa-fw" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 512 512"><use href="#fas-sun"/></svg><svg class="svg-inline--fa fas fa-moon fa-fw" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 384 512"><use href="#fas-moon"/></svg><div class=ball></div></label></li></ul></div></div></nav></div><div class=main-content></div></div><div class="container-xxl flex-fill p-4 px-xxl-0"><div class="row row-cols-1 row-cols-md-2 row-cols-lg-3"><div class="col col-lg-2 d-none d-lg-block sidebar-overflow sticky-top pt-5"></div><div class="col-12 col-md-9 col-lg-8 mb-5 p-4"><nav aria-label=breadcrumb class=d-sm-none><ol class=breadcrumb><li class=breadcrumb-item><a href=/blogs/searchengine/><svg class="svg-inline--fa fas fa-angle-left" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 320 512"><use href="#fas-angle-left"/></svg>&nbsp;&nbsp;SearchEngine</a></li></ol></nav><nav aria-label=breadcrumb class="d-none d-sm-block"><ol class=breadcrumb><li class=breadcrumb-item><a href=/>Home</a></li><li class=breadcrumb-item><a href=/blogs/>Blogs</a></li><li class=breadcrumb-item><a href=/blogs/searchengine/>SearchEngine</a></li><li class="breadcrumb-item active" aria-current=page>SearchEngine-2-相关性</li></ol></nav><p class="display-4 mt-5">SearchEngine-2-相关性</p><small class="text-body-secondary text-uppercase">Posted on October 17, 2024
&bull;
11&nbsp;min read &bull;
2,264&nbsp;words</small><p class="lead mb-5 mt-3">【笔记】wangshusen-搜索引擎技术：相关性</p><div class="d-md-none pb-5"><div class="d-grid gap-2 mx-auto"><a aria-label="On this page" href=#toc-collapse class="btn btn-outline-secondary position-relative toc-button" data-bs-toggle=collapse aria-expanded=false aria-controls=toc-collapse role=button><span class="d-flex justify-content-between"><span class=my-auto>On this page</span><span class="align-self-center ps-1"><svg class="svg-inline--fa fas fa-sort" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 320 512"><use href="#fas-sort"/></svg></span></span></a></div><div class="collapse border bg-body-tertiary rounded p-1 navbar-nav-scroll" id=toc-collapse><small><div class="toc toc-panel text-body p-2"><a class="toc-item toc-level-1" href=/blogs/searchengine/rel/#相关性定义与分档>相关性：定义与分档 </a><a class="toc-item toc-level-2" href=/blogs/searchengine/rel/#相关-vs-不相关>相关 V.S. 不相关 </a><a class="toc-item toc-level-2" href=/blogs/searchengine/rel/#档位细分>档位细分 </a><a class="toc-item toc-level-2" href=/blogs/searchengine/rel/#标注流程>标注流程 </a><a class="toc-item toc-level-1" href=/blogs/searchengine/rel/#评价指标auc正逆序比dcg>评价指标（AUC、正逆序比、DCG） </a><a class="toc-item toc-level-2" href=/blogs/searchengine/rel/#pointwise-评价指标>Pointwise 评价指标 </a><a class="toc-item toc-level-2" href=/blogs/searchengine/rel/#pairwise-评价指标>Pairwise 评价指标 </a><a class="toc-item toc-level-2" href=/blogs/searchengine/rel/#listwise-评价指标>Listwise 评价指标 </a><a class="toc-item toc-level-2" href=/blogs/searchengine/rel/#总结-2>总结 </a><a class="toc-item toc-level-1" href=/blogs/searchengine/rel/#文本匹配tf-idfbm25词距>文本匹配（TF-IDF、BM25、词距） </a><a class="toc-item toc-level-2" href=/blogs/searchengine/rel/#链路上的相关性模型>链路上的相关性模型 </a><a class="toc-item toc-level-2" href=/blogs/searchengine/rel/#词匹配分数>词匹配分数 </a><a class="toc-item toc-level-2" href=/blogs/searchengine/rel/#词距分数-term-proximity>词距分数 (Term Proximity) </a><a class="toc-item toc-level-2" href=/blogs/searchengine/rel/#总结-3>总结 </a><a class="toc-item toc-level-1" href=/blogs/searchengine/rel/#bert-模型>BERT 模型 </a><a class="toc-item toc-level-2" href=/blogs/searchengine/rel/#模型结构线上推理>模型结构、线上推理 </a><a class="toc-item toc-level-2" href=/blogs/searchengine/rel/#bert-模型的训练>BERT 模型的训练</a></div></small></div></div><div class=content><h2 id=相关性定义与分档 class=heading>相关性：定义与分档<a href=#%e7%9b%b8%e5%85%b3%e6%80%a7%e5%ae%9a%e4%b9%89%e4%b8%8e%e5%88%86%e6%a1%a3 aria-labelledby=相关性定义与分档><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h2><p><strong>工业界标准流程：</strong></p><div class=mathjax-display><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>制定标注规则</mtext><mo>→</mo><mtext>标注数据 </mtext><mo>→</mo><mtext> 训练模型</mtext><mo>→</mo><mtext>线上推理 </mtext></mrow><annotation encoding="application/x-tex">
\text{制定标注规则} \rightarrow \text{标注数据 } \rightarrow \text{ 训练模型} \rightarrow \text{线上推理 }
</annotation></semantics></math></span></div><ul><li><p>搜索产品和搜索算法团队定义相关性标注规则。</p><ul><li><p>人为将
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>q</mi><mo separator="true">,</mo><mi>d</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(q,d)</annotation></semantics></math></span>
</span>的相关性划分为
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn></mrow><annotation encoding="application/x-tex">4</annotation></semantics></math></span>
</span>个或
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>5</mn></mrow><annotation encoding="application/x-tex">5</annotation></semantics></math></span>
</span>个（如百度）档位。后以
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn></mrow><annotation encoding="application/x-tex">4</annotation></semantics></math></span>
</span>个为例。</p></li><li><p>相关性分档规则非常重要，假如日后有大幅变动，需要重新标注数据，丢弃积累的数据。</p></li></ul></li><li><p>产品和算法团队监督指导标注团队的工作，累积数十万、数百万条
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>q</mi><mo separator="true">,</mo><mi>d</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(q,d)</annotation></semantics></math></span>
</span>样本。</p></li><li><p>算法团队用人工标注的数据训练相关性模型。</p></li></ul><center><img style="border-radius:.3125em;box-shadow:0 .2rem .4rem rgba(34,36,38,.12);margin:1rem" src=pics/image-20250330204551517.png width=55% alt></center><h3 id=相关-vs-不相关 class=heading>相关 V.S. 不相关<a href=#%e7%9b%b8%e5%85%b3-vs-%e4%b8%8d%e7%9b%b8%e5%85%b3 aria-labelledby=相关-vs-不相关><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h3><h4 id=字面匹配-vs-需求匹配 class=heading>字面匹配 vs 需求匹配<a href=#%e5%ad%97%e9%9d%a2%e5%8c%b9%e9%85%8d-vs-%e9%9c%80%e6%b1%82%e5%8c%b9%e9%85%8d aria-labelledby=字面匹配-vs-需求匹配><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h4><p><strong>相关性不是字面上的匹配，而是需求匹配。</strong></p><p>相关性是指
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span>
</span>能满足
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span>
</span>的需求或回答
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span>
</span>提出的问题。</p><ul><li><p>哪怕
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span>
</span>和
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span>
</span><strong>字面上完全不匹配，两者也可以被判定为相关</strong>。</p><ul><li><p><span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span>
</span>= 谁掌握芯片制造的尖端技术</p></li><li><p><span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span>
</span>= 全球最先进的光刻机都由荷兰 ASML 公司制造</p></li></ul></li><li><p>即便
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span>
</span>和
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span>
</span><strong>字面匹配，两者也可能不相关</strong>。</p><ul><li><p><span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span>
</span>= 巴伦西亚旅游</p></li><li><p><span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span>
</span>= 我去巴伦西亚旅游，吃到了最好最正宗的西班牙海鲜饭，回来研究了一番，这个视频给大家介绍西班牙海鲜饭的做法···</p></li></ul></li></ul><hr><h4 id=相关性标注只考虑相关性 class=heading>相关性标注只考虑相关性<a href=#%e7%9b%b8%e5%85%b3%e6%80%a7%e6%a0%87%e6%b3%a8%e5%8f%aa%e8%80%83%e8%99%91%e7%9b%b8%e5%85%b3%e6%80%a7 aria-labelledby=相关性标注只考虑相关性><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h4><p><strong>相关性标注的重点问题：相关性标注只考虑相关性，不考虑内容质量、时效性等因素</strong>。</p><ul><li><p>满足相关性，但内容质量低，OK</p><ul><li><p><span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span>
</span>= 什么药物可以治愈新冠？</p></li><li><p><span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span>
</span>= 一则虚假广告，声称某种草药可以治愈新冠，并用阴阳调和原理解释该草药克制新冠病毒</p></li></ul></li><li><p>满足相关性，但时效性低，OK</p><ul><li><p><span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span>
</span>= 上海落户政策</p></li><li><p><span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span>
</span>= 一篇过时的文章，介绍 2015 年的上海落户政策。</p></li></ul></li></ul><p><strong>相关性数据训练的模型只负责判断相关性</strong> （内容质量/时效性由别的模型判别）</p><hr><h4 id=多意图 class=heading>多意图<a href=#%e5%a4%9a%e6%84%8f%e5%9b%be aria-labelledby=多意图><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h4><p>查询词
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span>
</span>可能有多种意图，文档
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span>
</span>只需命中一种意图就算相关</p><p>【例】</p><ul><li><p>黑寡妇：黑寡妇蜘蛛、漫威电影黑寡妇角色、车臣黑寡妇组织。</p></li><li><p>用户搜 =“黑寡妇”，不论用户的意图是什么，黑寡妇蜘蛛黑寡妇角色、黑寡妇组织的文档都满足相关性。</p></li></ul><p><strong>越短的查询词就越可能有多意图。</strong></p><p>若没有命中用户的真实意图，是个性化的问题，搜索引擎没有根据用户画像做好查询词理解和排序。</p><hr><h4 id=上位词下位词 class=heading>上位词&下位词<a href=#%e4%b8%8a%e4%bd%8d%e8%af%8d%e4%b8%8b%e4%bd%8d%e8%af%8d aria-labelledby=上位词下位词><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h4><p><span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>下位词</mtext><mo>∈</mo><mtext>上位词</mtext></mrow><annotation encoding="application/x-tex">\text{下位词} \in \text{上位词}</annotation></semantics></math></span></span></p><p><strong>搜上位词，出下位词，判定为 相关</strong>。</p><ul><li><p>搜
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span>
</span>=“广东菜”，出
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span>
</span>= “潮汕美食”。</p></li><li><p>搜
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span>
</span>=“红色口红”，出
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span>
</span>= “玫红色口红”</p></li></ul><p><strong>搜下位词，出上位词，通常判定为 不相关</strong>。</p><ul><li><p>搜
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span>
</span>= “潮汕美食”，出
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span>
</span>=“经典广东菜’</p></li><li><p>搜
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span>
</span>= “玫红色口红”，出
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span>
</span>=“红色口红”</p></li></ul><hr><h4 id=丢词的判定 class=heading>丢词的判定<a href=#%e4%b8%a2%e8%af%8d%e7%9a%84%e5%88%a4%e5%ae%9a aria-labelledby=丢词的判定><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h4><p><strong>丢词即文档不能完全满足查询词的需求，丢失了一部分需求</strong>。</p><ul><li><p>丢弃 <strong>核心词</strong>，判定为不相关</p><ul><li><p>搜
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span>
</span>=“情人节餐厅”，出
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span>
</span>= “情人节礼物”。</p></li><li><p>搜
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span>
</span>=“黄晓明”，出
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span>
</span>=“杨颖拍过的电影”。</p></li></ul></li><li><p>丢失 <strong>重要限定词</strong>（<strong>主观性较强</strong>），判定为不相关。</p><ul><li>搜
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span>
</span>= “初二物理考点”，出
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span>
</span>= “初三物理考点”。</li><li></li><li>搜
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span>
</span>= “黄石公园春季旅游”，出
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span>
</span>= “黄石公园秋季旅游 &ldquo;。</li></ul></li><li><p>丢失 <strong>不重要限定词</strong>，判定为相关</p><ul><li><p>搜
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span>
</span>=“精彩的好莱坞动作片”，出
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span>
</span>= “好莱坞动作片 top 10”。</p></li><li><p>搜
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span>
</span>= “东南亚十大旅游景点”，出
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span>
</span>= “东南亚热门旅游景点”。</p></li></ul></li></ul><p><strong>具体要看
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span>
</span>能否满足
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span>
</span>的主要需求或回答
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span>
</span>提出的问题。</strong></p><hr><h4 id=总结 class=heading>总结<a href=#%e6%80%bb%e7%bb%93 aria-labelledby=总结><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h4><ul><li><p>相关性是指
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span>
</span>能满足
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span>
</span>的需求或回答
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span>
</span>提出的问题，而非字面上的匹配。</p></li><li><p>相关性标注只考虑相关性，不考虑内容质量、时效性</p></li><li><p>如果
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span>
</span>有多种意图，只要命中一种意图，就判定为相关。</p></li><li><p>搜上位词出下位词，判定为相关；搜下位词出上位词，通常判定为不相关。</p></li><li><p>丢核心词、重要限定词，判定为不相关；丢不重要的限定词，判定为相关。</p></li></ul><h3 id=档位细分 class=heading>档位细分<a href=#%e6%a1%a3%e4%bd%8d%e7%bb%86%e5%88%86 aria-labelledby=档位细分><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h3><h4 id=根据内容占比划分高中档位 class=heading>根据内容占比划分高、中档位<a href=#%e6%a0%b9%e6%8d%ae%e5%86%85%e5%ae%b9%e5%8d%a0%e6%af%94%e5%88%92%e5%88%86%e9%ab%98%e4%b8%ad%e6%a1%a3%e4%bd%8d aria-labelledby=根据内容占比划分高中档位><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h4><p>如果
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>q</mi><mo separator="true">,</mo><mi>d</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(q,d)</annotation></semantics></math></span>
</span>相关，则进一步划分为<strong>高（超过
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>50</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">50\%</annotation></semantics></math></span>
</span>）</strong>、**中（不超过
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>50</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">50\%</annotation></semantics></math></span>
</span>）**两档。</p><p>细分规则：<strong>满足需求的内容的篇幅占比是否超过 50%</strong></p><p>【例 1】</p><ul><li><p>搜索
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span>
</span>= “泰坦尼克号”，出
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span>
</span>= 演员莱昂纳多关于他的代表作的访谈，其中重点谈了《泰坦尼克号》电影。</p></li><li><p>文档
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span>
</span>满足查询词
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span>
</span>的需求，判定为相关。</p></li><li><p>如果访谈内容中《泰坦尼克号》篇幅占比大于
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>50</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">50\%</annotation></semantics></math></span>
</span>，判定为高档位，否则判定为中档位。</p></li></ul><p>【例 2】</p><ul><li><p>搜索
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span>
</span>= “小米手机测评”，出
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span>
</span>= 几款安卓手机的测评，其中包括几款小米手机。</p></li><li><p>文档
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span>
</span>满足查询词
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span>
</span>的需求，判定为相关。</p></li><li><p>如果文档中小米手机篇幅占比大于
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>50</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">50\%</annotation></semantics></math></span>
</span>，判定为高档位，否则判定为中档位。</p></li></ul><h4 id=根据参考价值划分低无档位 class=heading>根据参考价值划分低、无档位<a href=#%e6%a0%b9%e6%8d%ae%e5%8f%82%e8%80%83%e4%bb%b7%e5%80%bc%e5%88%92%e5%88%86%e4%bd%8e%e6%97%a0%e6%a1%a3%e4%bd%8d aria-labelledby=根据参考价值划分低无档位><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h4><p>如果
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>q</mi><mo separator="true">,</mo><mi>d</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(q,d)</annotation></semantics></math></span>
</span>不相关，则进一步划分为<strong>低（有参考价值）</strong>、**无（无参考价值）**两档。</p><p>细分规则：<strong>文档是否具有参考价值（用户可能愿意看这篇文档，代表有参考价值）</strong></p><p>【例 1】</p><ul><li><p>搜索
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span>
</span>= “初二下册物理考点”，出
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span>
</span>= “中考物理考点”。</p></li><li><p>丢失重要限定词，导致文档
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span>
</span>无法满足查询词
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span>
</span>的需求，判定为不相关。</p></li><li><p>“中考物理考点”有一定参考价值，档位为“低”</p></li></ul><p>【例 2】</p><ul><li><p>搜索
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span>
</span>= “初二下册物理考点”，出
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span>
</span>= “初一数学考点”。</p></li><li><p>丢失重要限定词，导致文档
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span>
</span>无法满足查询词
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span>
</span>的需求，判定为不相关</p></li><li><p>“初一数学考点 " 没有参考价值，档位为“无”。</p></li></ul><hr><h4 id=总结-1 class=heading>总结<a href=#%e6%80%bb%e7%bb%93-1 aria-labelledby=总结-1><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h4><ol><li><p><strong>相关性</strong>是指
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span>
</span>能满足
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span>
</span>的需求或回答
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span>
</span>提出的问题。</p></li><li><p>先判断
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span>
</span>与
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span>
</span>是否相关，划分为
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn></mrow><annotation encoding="application/x-tex">2</annotation></semantics></math></span>
</span>大档位：</p><ul><li><p>判断是否相关<strong>只考虑相关性本身</strong>，不要考虑内容质量、时效性、个性化等其他因素。</p></li><li><p><span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span>
</span>可能有多种意图，只要
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span>
</span>命中其中一种意图，就算相关。</p></li><li><p>搜上位词出下位词，判定为相关；反之，通常判定为不相关</p></li><li><p>如果
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span>
</span>丢弃了
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span>
</span>中的词，需要判断
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span>
</span>能否满足
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span>
</span>的需求，从而判断是否相关。</p></li></ul></li><li><p>将大档位细分为
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn></mrow><annotation encoding="application/x-tex">4</annotation></semantics></math></span>
</span>个小档位。</p><ul><li><p>根据所占篇幅，将 &ldquo;相关&rdquo; 细分为高、中
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn></mrow><annotation encoding="application/x-tex">2</annotation></semantics></math></span>
</span>个小档位。</p></li><li><p>根据文档是否有参考价值，将 &ldquo;不相关&rdquo; 细分为低、无
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn></mrow><annotation encoding="application/x-tex">2</annotation></semantics></math></span>
</span>个小档位</p></li><li><p>相关性细分为高、中、低、无
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn></mrow><annotation encoding="application/x-tex">4</annotation></semantics></math></span>
</span>个小档位。</p></li><li><p>有的公司将 &ldquo;相关&rdquo; 细分为
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn></mrow><annotation encoding="application/x-tex">3</annotation></semantics></math></span>
</span>个小档位，“不相关 " 细分为
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn></mrow><annotation encoding="application/x-tex">2</annotation></semantics></math></span>
</span>个小档位，一共
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>5</mn></mrow><annotation encoding="application/x-tex">5</annotation></semantics></math></span>
</span>个小档位。</p></li></ul></li></ol><h3 id=标注流程 class=heading>标注流程<a href=#%e6%a0%87%e6%b3%a8%e6%b5%81%e7%a8%8b aria-labelledby=标注流程><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h3><p><strong>由算法团队抽取待标注样本</strong></p><ul><li><p>从搜索日志中随机抽取
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span>
</span>条查询词，
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span>
</span>的大小取决于标注人力。既有高频查询词，也有中、低频查询词（假如中低频查询词占比太小，训练出的模型在中低频查询词上的表现会比较差）</p></li><li><p>给定
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span>
</span>，从搜索结果中抽取
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span>
</span>篇文档，组成二元组
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>q</mi><mo separator="true">,</mo><msub><mi>d</mi><mn>1</mn></msub><mo stretchy="false">)</mo><mo separator="true">,</mo><mo>⋯</mo><mtext> </mtext><mo separator="true">,</mo><mo stretchy="false">(</mo><mi>q</mi><mo separator="true">,</mo><msub><mi>d</mi><mi>k</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(q, d_1) , \cdots, (q, d_k)</annotation></semantics></math></span>
</span>，
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn></mrow><annotation encoding="application/x-tex">4</annotation></semantics></math></span>
</span>个相关性档位的 <strong>样本数量尽可能平衡</strong>。</p></li><li><p><strong>不能直接取搜索结果页排名 <code>Topk</code> 的文档</strong>，否则高档位（即高相关）文档过多，低档位文档过少。样本有偏不利于训练。</p></li></ul><hr><p><strong>由产品团队和算法团队监督标注过程和验收结果</strong></p><ul><li><p>遇到难以界定档位的
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>q</mi><mo separator="true">,</mo><mi>d</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(q, d)</annotation></semantics></math></span>
</span>，由产品和算法团队做界定和解释。</p></li><li><p>一条样本由至少两人标注，两人标注的结果需要有一致性。如果标注不一致，样本会被直接丢掉，或找第三人标注。</p></li><li><p>一致率指两个人的标注结果有多大比例是相同的。一致率大于某个阈值，如
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>80</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">80\%</annotation></semantics></math></span>
</span>，才会被接受。</p></li><li><p>若一致率合格，再由产品团队抽查标注结果（可以认为产品团队的标注结果是正确的，为 ground truth），要求准确率高于某个阈值。</p></li><li><p>只靠抽查还是不够（数量太少），可以事先往数据中埋雷（如往
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>10000</mn></mrow><annotation encoding="application/x-tex">10000</annotation></semantics></math></span>
</span>条待标注样本中掺杂
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>200</mn></mrow><annotation encoding="application/x-tex">200</annotation></semantics></math></span>
</span>条产品团队自己标注的样本），考察埋雷样本的标注准确率。</p></li></ul><h2 id=评价指标auc正逆序比dcg class=heading>评价指标（<code>AUC</code>、正逆序比、<code>DCG</code>）<a href=#%e8%af%84%e4%bb%b7%e6%8c%87%e6%a0%87auc%e6%ad%a3%e9%80%86%e5%ba%8f%e6%af%94dcg aria-labelledby=评价指标auc正逆序比dcg><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h2><blockquote class=blockquote><ul><li><p><strong><code>Pointwise</code></strong> 评价指标：<strong><code>AUC</code></strong>（Area Under the Curve）</p></li><li><p><strong><code>Pairwise</code></strong> 评价指标：<strong><code>PNR</code></strong>（正逆序比，Positive to Negative Ratio）</p></li><li><p><strong><code>Listwise</code></strong> 评价指标：<strong><code>DCG</code></strong>（Discounted Cumulative Gain）</p></li></ul></blockquote><ul><li><p><strong>离线</strong>: <code>AUC</code> 和 <code>PNR</code> （看相关性模型在测试集上的表现够不够好）</p></li><li><p><strong>线上排序</strong>：<code>DCG</code> （线上直接搜索结果页存在搜索日志里，事后做抽样，让人工标注相关性，然后计算 <code>DCG</code>，评价排序是否合理）</p></li></ul><h3 id=pointwise-评价指标 class=heading>Pointwise 评价指标<a href=#pointwise-%e8%af%84%e4%bb%b7%e6%8c%87%e6%a0%87 aria-labelledby=pointwise-评价指标><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h3><h4 id=二分类评价指标 class=heading>二分类评价指标<a href=#%e4%ba%8c%e5%88%86%e7%b1%bb%e8%af%84%e4%bb%b7%e6%8c%87%e6%a0%87 aria-labelledby=二分类评价指标><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h4><p>把 <strong>相关性</strong> 看作 <strong>二分类问题</strong>，<strong>独立对待</strong> 每一对
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>q</mi><mo separator="true">,</mo><mi>d</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(q, d)</annotation></semantics></math></span>
</span>二元组。</p><p>训练集还是
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn></mrow><annotation encoding="application/x-tex">4</annotation></semantics></math></span>
</span>个小档位（高中低无），但测试集只用
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn></mrow><annotation encoding="application/x-tex">2</annotation></semantics></math></span>
</span>个大档位（把测试集相关性档位转化为 0/1）</p><ul><li>高、中两档合并，作为标签
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">y = 1</annotation></semantics></math></span></span></li><li>低、无两档合并，作为标签
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">y = 0</annotation></semantics></math></span></span></li></ul><p>相关性模型输出预测值
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>∈</mo><mo stretchy="false">[</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">p\in [0,1]</annotation></semantics></math></span>
</span>，<strong>
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span>
</span>的值越大，表示模型认为查询词和文档越有可能相关</strong></p><hr><p><strong>二分类评价指标</strong>：</p><p>准确率、召回率、<code>F1</code>、<code>AUC</code>（工业界最常用 AUC 来评价搜索相关性）</p><center><img style="border-radius:.3125em;box-shadow:0 .2rem .4rem rgba(34,36,38,.12);margin:.2rem" src=pics/image-20250330210613826.png width=23% alt>
<img style="border-radius:.3125em;box-shadow:0 .2rem .4rem rgba(34,36,38,.12);margin:.2rem" src=pics/image-20250330210641149.png width=23% alt>
<img style="border-radius:.3125em;box-shadow:0 .2rem .4rem rgba(34,36,38,.12);margin:.2rem" src=pics/image-20250330210712566.png width=23% alt>
<img style="border-radius:.3125em;box-shadow:0 .2rem .4rem rgba(34,36,38,.12);margin:.2rem" src=pics/image-20250330210729619.png width=23% alt></center><p>调整二分类的阈值，获得很多组 假阳性率&真阳性率 的二元组，获得 <code>ROC</code> 曲线</p><p>用 **<code>AUC</code> （Area Under the Curve）**评价模型的预测是否准确。</p><ul><li><p><code>AUC</code>
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>=</mo><mn>0.5</mn></mrow><annotation encoding="application/x-tex">= 0.5</annotation></semantics></math></span>
</span>说明模型没有学到东西 （没有比随机猜测更好）</p></li><li><p>比较好的相关性模型的 <code>AUC</code> 在 <code>0.8 ~ 0.95</code> 之间</p></li></ul><h3 id=pairwise-评价指标 class=heading>Pairwise 评价指标<a href=#pairwise-%e8%af%84%e4%bb%b7%e6%8c%87%e6%a0%87 aria-labelledby=pairwise-评价指标><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h3><p>每次取两个二元组做对比。问题：只考虑文档两两之间的序，不考虑整体的序。</p><center><img style="border-radius:.3125em;box-shadow:0 .2rem .4rem rgba(34,36,38,.12);margin:1rem" src=pics/image-20250330211115589.png width=25% alt></center><ul><li><p>根据模型估计的相关性分数
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span>
</span>对文档做排序（不知道真实相关性分数)</p><ul><li>例子中有
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>6</mn></mrow><annotation encoding="application/x-tex">6</annotation></semantics></math></span>
</span>篇文档，它们的分数满足
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mn>1</mn></msub><mo>≥</mo><msub><mi>p</mi><mn>2</mn></msub><mo>≥</mo><mo>⋯</mo><mo>≥</mo><msub><mi>p</mi><mn>6</mn></msub></mrow><annotation encoding="application/x-tex">p_1 \ge p_2 \ge \cdots \ge p_6</annotation></semantics></math></span></span></li></ul></li><li><p>有
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span>
</span>篇文档，则有
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo fence="true">(</mo><mtable rowspacing="0.16em" columnalign="center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mi>k</mi></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>2</mn></mstyle></mtd></mtr></mtable><mo fence="true">)</mo></mrow><mo>=</mo><mfrac><mrow><mi>k</mi><mo stretchy="false">!</mo></mrow><mrow><mn>2</mn><mo stretchy="false">!</mo><mo>×</mo><mo stretchy="false">(</mo><mi>k</mi><mo>−</mo><mn>2</mn><mo stretchy="false">)</mo><mo stretchy="false">!</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">\begin{pmatrix} k\\ 2  \end{pmatrix}= \frac{k!}{2!\times (k-2)!} </annotation></semantics></math></span>
</span>种方式将文档两两组合</p><ul><li><p><span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>=</mo><mn>6</mn></mrow><annotation encoding="application/x-tex">k=6</annotation></semantics></math></span>
</span>，有
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo fence="true">(</mo><mtable rowspacing="0.16em" columnalign="center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>6</mn></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>2</mn></mstyle></mtd></mtr></mtable><mo fence="true">)</mo></mrow><mo>=</mo><mn>15</mn></mrow><annotation encoding="application/x-tex">\begin{pmatrix} 6\\ 2  \end{pmatrix} = 15</annotation></semantics></math></span>
</span>种组合</p></li><li><p>有
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn></mrow><annotation encoding="application/x-tex">2</annotation></semantics></math></span>
</span>个逆序对
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mn>2</mn><mo separator="true">,</mo><mn>3</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(2,3)</annotation></semantics></math></span>
</span>,
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mn>2</mn><mo separator="true">,</mo><mn>4</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(2,4)</annotation></semantics></math></span>
</span>，
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>13</mn></mrow><annotation encoding="application/x-tex">13</annotation></semantics></math></span>
</span>个正序对</p></li><li><p>正逆序比为：
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>PNR</mtext><mo>=</mo><mfrac><mn>13</mn><mn>2</mn></mfrac></mrow><annotation encoding="application/x-tex">\text{PNR}=\frac{13}{2}</annotation></semantics></math></span></span></p></li></ul></li><li><p><span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">⟨</mo><mtext>高</mtext><mo separator="true">,</mo><mtext>高</mtext><mo stretchy="false">⟩</mo></mrow><annotation encoding="application/x-tex">\langle\text{高},\text{高}\rangle </annotation></semantics></math></span>
</span>这样的 pair，可以看成正序对，也可以忽略掉这样的二元组。</p></li></ul><hr><p>以下两种情况正逆序比相同，都是
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>13</mn><mo>:</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">13:2</annotation></semantics></math></span>
</span>，在 <code>Pairwise</code> 评价指标下是等价的。但实际上，右边的情况要优于左边。（因为错误发生在后面，用户浏览搜索结果页时通常是从上到下浏览，要保证前面的结果都是高相关性）</p><center><img style="border-radius:.3125em;box-shadow:0 .2rem .4rem rgba(34,36,38,.12);margin:1rem" src=pics/image-20250330212456443.png width=60% alt></center><hr><h3 id=listwise-评价指标 class=heading>Listwise 评价指标<a href=#listwise-%e8%af%84%e4%bb%b7%e6%8c%87%e6%a0%87 aria-labelledby=listwise-评价指标><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h3><p><strong>更看重排在前面的文档，给更大的权重</strong></p><h4 id=pairwise-指标-vs-listwise-指标 class=heading>Pairwise 指标 v.s. Listwise 指标<a href=#pairwise-%e6%8c%87%e6%a0%87-vs-listwise-%e6%8c%87%e6%a0%87 aria-labelledby=pairwise-指标-vs-listwise-指标><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h4><p>有
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span>
</span>篇候选文档，根据模型打分做降序排列，把文档记作
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>⋯</mo><mtext> </mtext><mo separator="true">,</mo><msub><mi>d</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">d_1, \cdots, d_n</annotation></semantics></math></span>
</span>。（此时不知道真实相关性分数）</p><p><span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>⋯</mo><mtext> </mtext><mo separator="true">,</mo><msub><mi>d</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">d_1, \cdots, d_n</annotation></semantics></math></span>
</span>的真实相关性分数为
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>⋯</mo><mtext> </mtext><mo separator="true">,</mo><msub><mi>y</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">y_1, \cdots, y_n</annotation></semantics></math></span>
</span>。（人工标注相关性档位，档位映射到
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[0,1]</annotation></semantics></math></span>
</span>区间上的实数）</p><p>理想的排序为
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mn>1</mn></msub><mo>≥</mo><msub><mi>y</mi><mn>2</mn></msub><mo>≥</mo><mo>⋯</mo><mo>≥</mo><msub><mi>y</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">y_1 \ge y_2 \ge \cdots \ge y_n</annotation></semantics></math></span>
</span>，即模型打分的序与真实相关性分数的序一致，此时 <code>pairwise</code> 和 <code>listwise</code> 指标都最大化。</p><p>逆序对会导致 <code>pairwise</code> 和 <code>listwise</code> 指标减小。</p><ul><li><p><strong><code>pairwise</code> 指标</strong>: 逆序对出现的位置不影响。</p></li><li><p><strong><code>listwise</code> 指标</strong>：逆序对越靠前，对 <code>listwise</code> 指标造成的损失越大</p></li></ul><h4 id=cgcumulative-gain class=heading>CG（Cumulative Gain）<a href=#cgcumulative-gain aria-labelledby=cgcumulative-gain><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h4><p>有
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span>
</span>篇候选文档，根据模型打分做降序排列；它们的真实相关性分数为
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>⋯</mo><mtext> </mtext><mo separator="true">,</mo><msub><mi>y</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">y_1, \cdots, y_n</annotation></semantics></math></span>
</span>。</p><p>只关注排在前
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span>
</span><span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>k</mi><mo>≪</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(k\ll n)</annotation></semantics></math></span>
</span>的文档，它们最可能获得曝光，对用户体验的影响最大。</p><p><strong>Cumulative Gain <code>CG@k</code>:</strong></p><div class=mathjax-display><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>C</mi><mi>G</mi><mi mathvariant="normal">@</mi><mi>k</mi><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><msub><mi>y</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">
CG@k = \sum^k_{i = 1}y_i
</annotation></semantics></math></span></div><center><img style="border-radius:.3125em;box-shadow:0 .2rem .4rem rgba(34,36,38,.12);margin:1rem" src=pics/image-20250330213416163.png width=60% alt></center><p><code>CG@k</code> 何时最大化？</p><ul><li><p>真实相关性分数
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span>
</span>最高的
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span>
</span>篇文档被模型排在前
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span>
</span>。</p></li><li><p>前
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span>
</span>篇文档的序不重要，它们之间可以存在逆序对。</p></li></ul><h4 id=dcgdiscounted-cumulative-gain class=heading>DCG（Discounted Cumulative Gain）<a href=#dcgdiscounted-cumulative-gain aria-labelledby=dcgdiscounted-cumulative-gain><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h4><p>有
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span>
</span>篇候选文档，根据模型打分做降序排列，它们的真实相关性分数为
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>⋯</mo><mtext> </mtext><mo separator="true">,</mo><msub><mi>y</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">y_1, \cdots, y_n</annotation></semantics></math></span></span></p><p><strong>Discounted Cumulative Gain <code>DCG@k</code>:</strong></p><div class=mathjax-display><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mrow><mi mathvariant="normal">D</mi><mi mathvariant="normal">C</mi><mi mathvariant="normal">G</mi><mi mathvariant="normal">@</mi></mrow><mi>k</mi><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mfrac><msub><mi>y</mi><mi>i</mi></msub><mrow><msub><mrow><mi>log</mi><mo>⁡</mo></mrow><mn>2</mn></msub><mo stretchy="false">(</mo><mi>i</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">
\mathrm{DCG@}k =\sum_{i = 1}^k\frac{y_i}{\log_2(i+1)}
</annotation></semantics></math></span></div><center><img style="border-radius:.3125em;box-shadow:0 .2rem .4rem rgba(34,36,38,.12);margin:.5rem" src=pics/image-20250330213612091.png width=40% alt><br><img style="border-radius:.3125em;box-shadow:0 .2rem .4rem rgba(34,36,38,.12);margin:.5rem" src=pics/image-20250330213848927.png width=40% alt>
<img style="border-radius:.3125em;box-shadow:0 .2rem .4rem rgba(34,36,38,.12);margin:.5rem" src=pics/image-20250330213704785.png width=40% alt></center><p><code>DCG@k</code> 何时最大化?</p><ul><li><p>真实相关性分数
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span>
</span>最高的
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span>
</span>篇文档被模型排在前
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span>
</span>。</p></li><li><p>前
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span>
</span>篇文档不存在逆序对。</p></li></ul><h3 id=总结-2 class=heading>总结<a href=#%e6%80%bb%e7%bb%93-2 aria-labelledby=总结-2><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h3><p>相关性有 <code>pointwise</code>、<code>pairwise</code>、<code>listwise</code> 评价指标</p><blockquote class=blockquote><ul><li><p><strong><code>pointwise</code></strong>：单独评价每一个
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>q</mi><mo separator="true">,</mo><mi>d</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(q, d)</annotation></semantics></math></span>
</span>二元组，判断预测的相关性分数与真实标签的相似度。不考虑二元组之间的关系；</p></li><li><p><strong><code>pairwise</code></strong>：对比
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>q</mi><mo separator="true">,</mo><msub><mi>d</mi><mn>1</mn></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(q, d_1)</annotation></semantics></math></span>
</span>和
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>q</mi><mo separator="true">,</mo><msub><mi>d</mi><mn>2</mn></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(q, d_2)</annotation></semantics></math></span>
</span>，判断两者的序是否正确（<strong>正序对或逆序对</strong>）</p></li><li><p><strong><code>listwise</code></strong>：对比
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>q</mi><mo separator="true">,</mo><msub><mi>d</mi><mn>1</mn></msub><mo stretchy="false">)</mo><mo separator="true">,</mo><mo stretchy="false">(</mo><mi>q</mi><mo separator="true">,</mo><msub><mi>d</mi><mn>2</mn></msub><mo stretchy="false">)</mo><mo separator="true">,</mo><mo>⋯</mo><mtext> </mtext><mo separator="true">,</mo><mo stretchy="false">(</mo><mi>q</mi><mo separator="true">,</mo><msub><mi>d</mi><mi>n</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(q, d_1),(q, d_2), \cdots, (q,d_n)</annotation></semantics></math></span>
</span>，判断整体的序关系的正确程度</p></li></ul></blockquote><h4 id=离线评价指标-pointpair-wise class=heading>离线评价指标 (point&amp;pair-wise)<a href=#%e7%a6%bb%e7%ba%bf%e8%af%84%e4%bb%b7%e6%8c%87%e6%a0%87-pointpair-wise aria-labelledby=离线评价指标-pointpair-wise><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h4><p>事先准备人工标注的数据，划分为训练集和测试集。</p><p>完成训练之后，计算测试集上的 <code>AUC</code> 和 <code>PNR</code>（分别是 <code>pointwise</code> 和 <code>pairwise</code> 评价指标）。</p><p>相关性有
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn></mrow><annotation encoding="application/x-tex">4</annotation></semantics></math></span>
</span>个档位，为什么用 <code>AUC</code>（<code>AUC</code> 是评价二分类的指标），而不用多分类的评价指标（<code>Macro F1</code> 和 <code>Micro F1</code>）？</p><ul><li><p>简而言之，相关性虽然有
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn></mrow><annotation encoding="application/x-tex">4</annotation></semantics></math></span>
</span>个档位，但不是多分类问题。</p></li><li><p>相关性的标签存在序关系：
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>高</mtext><mo>&gt;</mo><mtext>中</mtext><mo>&gt;</mo><mtext>低</mtext><mo>&gt;</mo><mtext>无</mtext></mrow><annotation encoding="application/x-tex">\text{高} &gt; \text{中} &gt; \text{低} &gt;\text{无}</annotation></semantics></math></span></span></p></li><li><p>多分类把
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn></mrow><annotation encoding="application/x-tex">4</annotation></semantics></math></span>
</span>种标签看作
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn></mrow><annotation encoding="application/x-tex">4</annotation></semantics></math></span>
</span>个类别，忽略其中的序关系。</p></li><li><p>把 &ldquo;高&rdquo; 错判为 &ldquo;中&rdquo;，或错判为 &ldquo;无&rdquo;，错误严重程度不同，但被多分类视为同等的分类错误。</p></li></ul><h4 id=线上评价指标-listwise class=heading>线上评价指标 (listwise)<a href=#%e7%ba%bf%e4%b8%8a%e8%af%84%e4%bb%b7%e6%8c%87%e6%a0%87-listwise aria-labelledby=线上评价指标-listwise><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h4><p><strong>一个搜索 session</strong>: 用户实际做过的一次搜索，保存在搜索日志中：用户搜索
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span>
</span>，搜索结果页上按顺序展示文档
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>⋯</mo><mtext> </mtext><mo separator="true">,</mo><msub><mi>d</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">d_1, \cdots, d_n</annotation></semantics></math></span></span></p><p>从搜索日志中抽取一批 session，覆盖高、中、低频查询词。</p><p>对于每个 session，取排序最高的
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span>
</span>篇文档
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>⋯</mo><mtext> </mtext><mo separator="true">,</mo><msub><mi>d</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">d_1, \cdots, d_k</annotation></semantics></math></span></span></p><ul><li><p><span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span>
</span>的设定取决于用户浏览深度，比如
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>=</mo><mn>20</mn></mrow><annotation encoding="application/x-tex">k = 20</annotation></semantics></math></span>
</span>。</p></li><li><p>高频查询词前
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>20</mn></mrow><annotation encoding="application/x-tex">20</annotation></semantics></math></span>
</span>篇文档几乎都是高相关，指标过高。</p></li><li><p>高频查询词的
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span>
</span>设置的较大（比如
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>=</mo><mn>40</mn></mrow><annotation encoding="application/x-tex">k = 40</annotation></semantics></math></span>
</span>），低频查询词的
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span>
</span>设置的较小（比如
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>=</mo><mn>20</mn></mrow><annotation encoding="application/x-tex">k = 20</annotation></semantics></math></span>
</span>）</p></li></ul><p>人工标注相关性分数，记作
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>⋯</mo><mtext> </mtext><mo separator="true">,</mo><msub><mi>y</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">y_1, \cdots, y_n</annotation></semantics></math></span>
</span>。</p><p>想要对线上实际结果做评估，没办法事先准备一个测试集，只能在做评估的时候标注，每做一次评估就要标一批数据，通常一个月做一次，故叫<strong>月度评估</strong>。</p><p>计算
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi mathvariant="normal">D</mi><mi mathvariant="normal">C</mi><mi mathvariant="normal">G</mi><mi mathvariant="normal">@</mi></mrow><mi>k</mi><mo>=</mo><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></msubsup><mfrac><msub><mi>y</mi><mi>i</mi></msub><mrow><msub><mrow><mi>log</mi><mo>⁡</mo></mrow><mn>2</mn></msub><mo stretchy="false">(</mo><mi>i</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">\mathrm{DCG@}k=\sum_{i=1}^k\frac{y_i}{\log_2(i+1)}</annotation></semantics></math></span>
</span>，作为该 session 的评价指标。</p><p>对 <code>DCG@k</code> 关于所有 session 取平均，评价线上相关性模型。</p><hr><p><strong>【思考题】</strong></p><p><span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi mathvariant="sans-serif">N</mi><mi mathvariant="sans-serif">D</mi><mi mathvariant="sans-serif">C</mi><mi mathvariant="sans-serif">G</mi><mi mathvariant="sans-serif">@</mi></mrow><mi>k</mi><mo>=</mo><mfrac><mrow><mrow><mi mathvariant="sans-serif">D</mi><mi mathvariant="sans-serif">C</mi><mi mathvariant="sans-serif">G</mi><mi mathvariant="sans-serif">@</mi></mrow><mi>k</mi></mrow><mrow><mrow><mi mathvariant="sans-serif">I</mi><mi mathvariant="sans-serif">D</mi><mi mathvariant="sans-serif">C</mi><mi mathvariant="sans-serif">G</mi><mi mathvariant="sans-serif">@</mi></mrow><mi>k</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\mathsf{NDCG@}k=\frac{\mathsf{DCG@}k}{\mathsf{IDCG@}k}</annotation></semantics></math></span>
</span>是教科书中经典的评价指标，<code>NDCG</code> 是归一化的 <code>DCG</code>（N 表示 Normalized）</p><ul><li>其中 <code>IDCG@k</code> 是 <code>DCG@k</code> 的最优值，对应最优的排序。</li><li>因此 <code>NDCG@k</code> 的值介于
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">0</annotation></semantics></math></span>
</span>和
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn></mrow><annotation encoding="application/x-tex">1</annotation></semantics></math></span>
</span>之间</li></ul><p>【问题】<code>NDCG</code> 可否代替 <code>DCG</code> 用作线上评价指标？<code>NDCG</code> 有什么缺陷？</p><p>【提示】</p><ul><li><p>先做召回，再做排序。假设召回的结果全是低相关文档。</p></li><li><p><code>DCG</code> 是高是低？<code>NDCG</code> 是高是低？<code>DCG</code> 与 <code>NDCG</code> 谁更合理？</p></li></ul><p>指标得到了正向提升，而用户体验实际大幅下降，会诱导系统进行负向优化。</p><p>比如，在召回环节的一次上线，实验方案相比原方案，相关性大幅降低，但线上 <code>NDCG</code> 指标有可能会涨，这是因为
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>NDCG</mtext><mo>=</mo><mtext>DCG</mtext><mi mathvariant="normal">/</mi><mtext>IDCG</mtext></mrow><annotation encoding="application/x-tex">\text{NDCG}= \text{DCG}/ \text{IDCG}</annotation></semantics></math></span>
</span>，虽然 <code>DCG</code> 降低了，但是 <code>IDCG</code> 降低的幅度远大于 <code>DCG</code> 降低幅度，所以一次失败的优化却得到了正向效果。</p><p><code>DCG</code> 低，但 <code>NDCG</code> 不一定低，故 <code>DCG</code> 更合理。</p><h2 id=文本匹配tf-idfbm25词距 class=heading>文本匹配（TF-IDF、BM25、词距）<a href=#%e6%96%87%e6%9c%ac%e5%8c%b9%e9%85%8dtf-idfbm25%e8%af%8d%e8%b7%9d aria-labelledby=文本匹配tf-idfbm25词距><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h2><p>在深度学习成熟之前，搜索引擎主要靠 <strong>文本匹配</strong> 来判断相关性。</p><ul><li><p>传统的搜索引擎使用几十种人工设计的文本匹配分数，作为线性模型或树模型的特征，模型预测相关性分数。</p></li><li><p>词匹配分数（<code>TF-IDF</code>、<code>BM25</code>）、词距分数（<code>OkaTP</code>、<code>BM25TP</code>）。</p></li><li><p>其他分数（类目匹配、核心词匹配）。</p></li><li><p>2020 年后，搜索排序普遍放弃文本匹配，改用 <code>BERT</code> 模型，仅剩文本召回使用文本匹配做海选</p></li></ul><h3 id=链路上的相关性模型 class=heading>链路上的相关性模型<a href=#%e9%93%be%e8%b7%af%e4%b8%8a%e7%9a%84%e7%9b%b8%e5%85%b3%e6%80%a7%e6%a8%a1%e5%9e%8b aria-labelledby=链路上的相关性模型><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h3><p><strong>召回海选：</strong></p><ul><li>打分量：数万</li><li>模型：文本匹配分数+线性模型，或双塔 <code>BERT</code> 模型</li></ul><p><strong>粗排：</strong></p><ul><li>打分量：数千</li><li>模型：双塔 <code>BERT</code> 模型，或单塔 <code>BERT</code> 模型（又叫交叉 <code>BERT</code> 模型）</li></ul><p><strong>精排</strong>：</p><ul><li>打分量：数百</li><li>模型：单塔 <code>BERT</code> 模型（
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn></mrow><annotation encoding="application/x-tex">4</annotation></semantics></math></span>
</span>/
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>6</mn></mrow><annotation encoding="application/x-tex">6</annotation></semantics></math></span>
</span>/￥ 层）</li></ul><h3 id=词匹配分数 class=heading>词匹配分数<a href=#%e8%af%8d%e5%8c%b9%e9%85%8d%e5%88%86%e6%95%b0 aria-labelledby=词匹配分数><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h3><p>反映查询词和文档的相关性</p><ul><li><p>中文分词：将查询词、文档切成多个字符串</p><ul><li>查询词：
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi><mo>=</mo><mtext>好莱坞电影推荐</mtext></mrow><annotation encoding="application/x-tex">q = \text{好莱坞电影推荐}</annotation></semantics></math></span></span></li><li>分词得到：
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><mo>=</mo><mo stretchy="false">{</mo><mtext>好莱坞</mtext><mo separator="true">,</mo><mtext>电影</mtext><mo separator="true">,</mo><mtext>推荐</mtext><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">Q = \{\text{好莱坞}, \text{电影}, \text{推荐}\}</annotation></semantics></math></span></span></li></ul></li><li><p><span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span>
</span>中的词在文档
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span>
</span>中出现次数越多，则
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span>
</span>与
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span>
</span>越可能相关。</p></li><li><p><code>TF-IDF</code> 和 <code>BM25</code> 都是基于上述想法</p></li></ul><h4 id=tf-idf class=heading>TF-IDF<a href=#tf-idf aria-labelledby=tf-idf><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h4><h5 id=term-frequency-tf-词频 class=heading>Term Frequency (TF) 词频<a href=#term-frequency-tf-%e8%af%8d%e9%a2%91 aria-labelledby=term-frequency-tf-词频><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h5><ul><li><p><span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span>
</span>: 分词结果记作集合 ，【例】：
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><mo>=</mo><mo stretchy="false">{</mo><mtext>好莱坞</mtext><mo separator="true">,</mo><mtext>电影</mtext><mo separator="true">,</mo><mtext>推荐</mtext><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">Q = \{\text{好莱坞}, \text{电影}, \text{推荐}\}</annotation></semantics></math></span>
</span>。</p></li><li><p><span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo>∈</mo><mi>Q</mi></mrow><annotation encoding="application/x-tex">t \in Q</annotation></semantics></math></span>
</span>：一个词（term），【例】：
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo>=</mo><mtext>电影</mtext></mrow><annotation encoding="application/x-tex">t = \text{电影}</annotation></semantics></math></span>
</span>。</p></li><li><p><span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext>tf</mtext><mrow><mi>t</mi><mo separator="true">,</mo><mi>d</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\text{tf}_{t,d}</annotation></semantics></math></span>
</span>：词
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span>
</span>在文档
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span>
</span>中出现次数叫做词频。</p><ul><li><p><span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext>tf</mtext><mrow><mi>t</mi><mo separator="true">,</mo><mi>d</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\text{tf}_{t,d}</annotation></semantics></math></span>
</span>越大，说明
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span>
</span>与
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span>
</span>越可能相关</p></li><li><p><span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mo>∑</mo><mrow><mi>t</mi><mo>∈</mo><mi>Q</mi></mrow></msub><msub><mtext>tf</mtext><mrow><mi>t</mi><mo separator="true">,</mo><mi>d</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\sum_{t \in Q}\text{tf}_{t,d}</annotation></semantics></math></span>
</span>越大，则
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span>
</span>与
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span>
</span>越可能相关</p></li></ul></li></ul><hr><p>用
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext>tf</mtext><mrow><mi>t</mi><mo separator="true">,</mo><mi>d</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\text{tf}_{t,d}</annotation></semantics></math></span>
</span>衡量相关性的<strong>缺陷</strong>：文档
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span>
</span>越长，则
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext>tf</mtext><mrow><mi>t</mi><mo separator="true">,</mo><mi>d</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\text{tf}_{t,d}</annotation></semantics></math></span>
</span>越大。</p><ul><li><p>把文档
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span>
</span>重复两遍，得到
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>d</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>=</mo><mi>d</mi><mo>+</mo><mi>d</mi></mrow><annotation encoding="application/x-tex">d^{\prime}=d+d</annotation></semantics></math></span>
</span>。</p></li><li><p><code>TF</code> 变成的原先两倍：
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext>tf</mtext><mrow><mi>t</mi><mo separator="true">,</mo><msup><mi>d</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow></msub><mo>=</mo><mn>2</mn><mo>⋅</mo><msub><mtext>tf</mtext><mrow><mi>t</mi><mo separator="true">,</mo><mi>d</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\text{tf}_{t,d^{\prime}} =2\cdot \text{tf}_{t,d}</annotation></semantics></math></span></span></p></li><li><p>文档
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>d</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow><annotation encoding="application/x-tex">d^{\prime}</annotation></semantics></math></span>
</span>和
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span>
</span>的信息量相同，算出的相关性分数应当相等。</p></li></ul><hr><p><strong>解决方法</strong>：用文档
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span>
</span>的长度（记作
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>l</mi><mi>d</mi></msub></mrow><annotation encoding="application/x-tex">l_d</annotation></semantics></math></span>
</span>）对词频做归一化，即<strong>归一化的词频</strong>。用
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mo>∑</mo><mrow><mi>t</mi><mo>∈</mo><mi>Q</mi></mrow></msub><mfrac><msub><mtext>tf</mtext><mrow><mi>t</mi><mo separator="true">,</mo><mi>d</mi></mrow></msub><msub><mi>l</mi><mi>d</mi></msub></mfrac></mrow><annotation encoding="application/x-tex">\sum_{t \in Q}\frac{\text{tf}_{t,d}}{l_d}</annotation></semantics></math></span>
</span>消除文档长度影响。</p><ul><li><span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>l</mi><mi>d</mi></msub></mrow><annotation encoding="application/x-tex">l_d</annotation></semantics></math></span>
</span>：文档
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span>
</span>的长度</li></ul><p>用
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mo>∑</mo><mrow><mi>t</mi><mo>∈</mo><mi>Q</mi></mrow></msub><mfrac><msub><mtext>tf</mtext><mrow><mi>t</mi><mo separator="true">,</mo><mi>d</mi></mrow></msub><msub><mi>l</mi><mi>d</mi></msub></mfrac></mrow><annotation encoding="application/x-tex">\sum_{t \in Q}\frac{\text{tf}_{t,d}}{l_d}</annotation></semantics></math></span>
</span>衡量相关性仍然有<strong>缺陷</strong>：加和同等对待所有
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span>
</span>。</p><hr><p>词的重要性各不相同，不该同等对待。如何设定词的权重?</p><p><strong>语义重要性</strong>（<code>term weight</code>）：语义重要性在查询词处理环节计算，需要用深度学习计算。</p><ul><li><p><span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>电影</mtext><mo>&gt;</mo><mtext>好莱坞 </mtext><mo>&gt;</mo><mtext>推荐</mtext></mrow><annotation encoding="application/x-tex">\text{电影} &gt; \text{好莱坞 }&gt; \text{推荐}</annotation></semantics></math></span></span></p></li><li><p><span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo>=</mo><mtext>电影</mtext></mrow><annotation encoding="application/x-tex">t =\text{电影}</annotation></semantics></math></span>
</span>是核心词</p></li><li><p><span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo>=</mo><mtext>好莱坞</mtext></mrow><annotation encoding="application/x-tex">t =\text{好莱坞}</annotation></semantics></math></span>
</span>是重要的限定词。</p></li><li><p><span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo>=</mo><mtext>推荐</mtext></mrow><annotation encoding="application/x-tex">t =\text{推荐}</annotation></semantics></math></span>
</span>是不重要的词。</p></li></ul><p>在深度学习成熟之前，词匹配算不了 <code>term weight</code>，有一个简单的方法给词设定权重：一个 term 在越多文档中出现，它的判别能力就越弱，给它设定的权重就越低。</p><p>有多少篇文档包含
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span>
</span>？
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>好莱坞 </mtext><mo>&lt;</mo><mtext>电影 </mtext><mo>&lt;</mo><mtext>推荐</mtext></mrow><annotation encoding="application/x-tex">\text{好莱坞 }&lt; \text{电影 }&lt; \text{推荐}</annotation></semantics></math></span>
</span>。</p><hr><h5 id=document-frequency-df class=heading>Document Frequency (DF)<a href=#document-frequency-df aria-labelledby=document-frequency-df><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h5><ul><li><p><span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext>df</mtext><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\text{df}_t</annotation></semantics></math></span>
</span>：词
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span>
</span>在多少文档中出现过（数据集一共有
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span>
</span>篇文档）。</p></li><li><p><span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn><mo>≤</mo><msub><mtext>df</mtext><mi>t</mi></msub><mo>≤</mo><mi>N</mi></mrow><annotation encoding="application/x-tex">0\le \text{df}_t \le N</annotation></semantics></math></span></span></p></li><li><p><span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext>df</mtext><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\text{df}_t</annotation></semantics></math></span>
</span>大，说明词
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span>
</span>判别能力弱，应当设置较小权重。</p><ul><li><p>“你”、“的”、“是”这样的停用词（stop word）的 <code>DF</code> 接近
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span>
</span>，对判断相关性几乎不起作用。</p></li><li><p>“好莱坞”、“强化学习”、“王者荣耀”的 <code>DF</code> 都很小，判别能力强。</p></li></ul></li></ul><hr><h5 id=inverse-document-frequencyidf class=heading>Inverse Document Frequency（IDF）<a href=#inverse-document-frequencyidf aria-labelledby=inverse-document-frequencyidf><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h5><p><strong>Inverse Document Frequency (<code>IDF</code>）</strong> 定义为:</p><div class=mathjax-display><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mrow><mi mathvariant="normal">i</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">f</mi></mrow><mi>t</mi></msub><mo>=</mo><mrow><mi mathvariant="normal">l</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">g</mi></mrow><mfrac><mi>N</mi><msub><mrow><mi mathvariant="normal">d</mi><mi mathvariant="normal">f</mi></mrow><mi>t</mi></msub></mfrac></mrow><annotation encoding="application/x-tex">
\mathrm{idf}_t =\mathrm{log}\frac N{\mathrm{df}_t}
</annotation></semantics></math></span></div><ul><li><p><code>IDF</code> 只取决于文档的数据集</p><ul><li><p>对于人工智能论文数据集，“深度学习 " 的 <code>IDF</code> 很小</p></li><li><p>对于维基百科数据集，“深度学习 " 的 <code>IDF</code> 很大</p></li></ul></li><li><p><span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mi mathvariant="normal">i</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">f</mi></mrow><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\mathrm{idf}_t</annotation></semantics></math></span>
</span>可以衡量词
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span>
</span>的判别能力；
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mi mathvariant="normal">i</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">f</mi></mrow><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\mathrm{idf}_t</annotation></semantics></math></span>
</span>越大，词
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span>
</span>越重要。</p></li></ul><p>用加权和
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mo>∑</mo><mrow><mi>t</mi><mo>∈</mo><mi>Q</mi></mrow></msub><mfrac><msub><mtext>tf</mtext><mrow><mi>t</mi><mo separator="true">,</mo><mi>d</mi></mrow></msub><msub><mi>l</mi><mi>d</mi></msub></mfrac><mo>⋅</mo><msub><mrow><mi mathvariant="normal">i</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">f</mi></mrow><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\sum_{t \in Q}\frac{\text{tf}_{t,d}}{l_d} \cdot \mathrm{idf}_t</annotation></semantics></math></span>
</span>衡量相关性</p><hr><h5 id=term-frequency-inverse-document-frequencytf-idf class=heading>Term Frequency-Inverse Document Frequency（TF-IDF）<a href=#term-frequency-inverse-document-frequencytf-idf aria-labelledby=term-frequency-inverse-document-frequencytf-idf><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h5><p>查询词
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span>
</span>的分词结果记作
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span>
</span>，它与文档
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span>
</span>的相关性可以用 <strong><code>TF-IDF</code></strong> 衡量</p><div class=mathjax-display><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mrow><mi mathvariant="normal">T</mi><mi mathvariant="normal">F</mi><mi mathvariant="normal">I</mi><mi mathvariant="normal">D</mi><mi mathvariant="normal">F</mi></mrow><mo stretchy="false">(</mo><mi>Q</mi><mo separator="true">,</mo><mi>d</mi><mo stretchy="false">)</mo><mo>=</mo><munder><mo>∑</mo><mrow><mi>t</mi><mo>∈</mo><mi mathvariant="script">Q</mi></mrow></munder><mfrac><msub><mrow><mi mathvariant="normal">t</mi><mi mathvariant="normal">f</mi></mrow><mrow><mi>t</mi><mo separator="true">,</mo><mi>d</mi></mrow></msub><msub><mi>l</mi><mi>d</mi></msub></mfrac><mo>⋅</mo><msub><mrow><mtext> </mtext><mi mathvariant="normal">i</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">f</mi></mrow><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">
\mathrm{TFIDF}(Q, d)=\sum_{t\in\mathcal{Q}}\frac{\mathrm{tf}_{t, d}}{l_d}\cdot\mathrm{~idf}_t
</annotation></semantics></math></span></div><p><code>TF-IDF</code> 有很多变种，例如</p><div class=mathjax-display><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mrow><mi mathvariant="normal">T</mi><mi mathvariant="normal">F</mi><mi mathvariant="normal">I</mi><mi mathvariant="normal">D</mi><mi mathvariant="normal">F</mi></mrow><mo stretchy="false">(</mo><mi>Q</mi><mo separator="true">,</mo><mi>d</mi><mo stretchy="false">)</mo><mo>=</mo><munder><mo>∑</mo><mrow><mi>t</mi><mo>∈</mo><mi mathvariant="script">Q</mi></mrow></munder><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>1</mn><mo>+</mo><msub><mrow><mi mathvariant="normal">t</mi><mi mathvariant="normal">f</mi></mrow><mrow><mi>t</mi><mo separator="true">,</mo><mi>d</mi></mrow></msub><mo stretchy="false">)</mo><mo>⋅</mo><msub><mrow><mtext> </mtext><mi mathvariant="normal">i</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">f</mi></mrow><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">
\mathrm{TFIDF}(Q, d)=\sum_{t\in\mathcal{Q}}\log(1+\mathrm{tf}_{t, d}) \cdot\mathrm{~idf}_t
</annotation></semantics></math></span></div><hr><h4 id=okapi-best-match-25-bm25 class=heading>Okapi Best Match 25 (BM25)<a href=#okapi-best-match-25-bm25 aria-labelledby=okapi-best-match-25-bm25><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h4><p><strong><code>BM25</code></strong> 可以看做 <strong><code>TF-IDF</code></strong> 的一种变体：</p><div class=mathjax-display><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><munder><mo>∑</mo><mrow><mi>t</mi><mo>∈</mo><mi mathvariant="script">Q</mi></mrow></munder><mfrac><mrow><msub><mrow><mi mathvariant="normal">t</mi><mi mathvariant="normal">f</mi></mrow><mrow><mi>t</mi><mo separator="true">,</mo><mi>d</mi></mrow></msub><mo>⋅</mo><mo stretchy="false">(</mo><mi>k</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><mrow><msub><mrow><mi mathvariant="normal">t</mi><mi mathvariant="normal">f</mi></mrow><mrow><mi>t</mi><mo separator="true">,</mo><mi>d</mi></mrow></msub><mo>+</mo><mi>k</mi><mo>⋅</mo><mrow><mo fence="true">(</mo><mn>1</mn><mo>−</mo><mi>b</mi><mo>+</mo><mi>b</mi><mo>⋅</mo><mfrac><msub><mi>l</mi><mi>d</mi></msub><mrow><mrow><mi mathvariant="normal">m</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">n</mi></mrow><mo stretchy="false">(</mo><msub><mi>l</mi><mi>d</mi></msub><mo stretchy="false">)</mo></mrow></mfrac><mo fence="true">)</mo></mrow></mrow></mfrac><mo>⋅</mo><mi>ln</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><mn>1</mn><mo>+</mo><mfrac><mrow><mi>N</mi><mo>−</mo><msub><mrow><mi mathvariant="normal">d</mi><mi mathvariant="normal">f</mi></mrow><mi>t</mi></msub><mo>+</mo><mn>0.5</mn></mrow><mrow><msub><mrow><mi mathvariant="normal">d</mi><mi mathvariant="normal">f</mi></mrow><mi>t</mi></msub><mo>+</mo><mn>0.5</mn></mrow></mfrac><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">
\sum_{t\in\mathcal{Q}}\frac{\mathrm{tf}_{t, d}\cdot(k+1)}{\mathrm{tf}_{t, d}+k\cdot\left(1-b+b\cdot\frac{l_d}{\mathrm{mean}(l_d)}\right)} \cdot \ln\left(1+\frac{N-\mathrm{df}_t+0.5}{\mathrm{df}_t+0.5}\right)
</annotation></semantics></math></span></div><p><span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span>
</span>和
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span>
</span>是参数，通常设置
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>∈</mo><mo stretchy="false">[</mo><mn>1.2</mn><mo separator="true">,</mo><mn>2</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">k \in[1.2,2]</annotation></semantics></math></span>
</span>和
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi><mo>=</mo><mn>0.75</mn></mrow><annotation encoding="application/x-tex">b=0.75</annotation></semantics></math></span>
</span>。</p><ul><li><p><code>BM25</code> 有很多种变体</p></li><li><p>在所有 <strong>词匹配分数</strong> 中，<code>BM25</code> 是最强的。如果学一个线性模型或者树模型预测相关性，<code>BM25</code> 的特征权重是最高的</p></li></ul><h4 id=词袋模型-bag-of-words class=heading>词袋模型 (bag of words)<a href=#%e8%af%8d%e8%a2%8b%e6%a8%a1%e5%9e%8b-bag-of-words aria-labelledby=词袋模型-bag-of-words><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h4><p><code>TF-IDF</code> 和 <code>BM25</code> 都属于 <strong>词袋模型</strong>，隐含了词袋模型假设：<strong>只考虑词频，不考虑词的顺序和上下文</strong>。</p><p>【例 1】：</p><ul><li>男朋友 / 送 / 的 / 礼物</li><li>送 / 男朋友 / 的 / 礼物</li></ul><p>【例 2】：</p><ul><li>白 / 衬衫 / 灰 / 裤子</li><li>灰 / 衬衫 / 白 / 裤子</li></ul><p><strong>缺点：词袋模型忽略词序和上下文，丢失了语义，不利于准确计算相关性。</strong></p><hr><p>前深度学习时代有很多词袋模型，例如 Latent Semantic Analysis (<code>LSA</code>)、Latent Dirichlet Allocation (<code>LDA</code>) 等都可以将查询词和文档映射为向量。</p><hr><p><code>RNN</code>、<code>BERT</code>、<code>GPT</code> 都不是词袋模型，会考虑词的顺序和上下文，更好地理解查询词和文档的语义。</p><h3 id=词距分数-term-proximity class=heading>词距分数 (Term Proximity)<a href=#%e8%af%8d%e8%b7%9d%e5%88%86%e6%95%b0-term-proximity aria-labelledby=词距分数-term-proximity><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h3><p>从另一个角度反映相关性。</p><p>【例】</p><ul><li><p>查询词
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><mo>=</mo><mo stretchy="false">{</mo><mtext>亚马逊</mtext><mo separator="true">,</mo><mtext>雨林</mtext><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">Q =\{\text{亚马逊}, \text{雨林}\}</annotation></semantics></math></span></span></p></li><li><p>文档
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mo>=</mo><mrow><mtext>我在亚马逊上网购了一本书，介绍东南亚热带雨林的植物群落</mtext><mo>…</mo><mo>…</mo></mrow></mrow><annotation encoding="application/x-tex">d= \text{我在亚马逊上网购了一本书，介绍东南亚热带雨林的植物群落……}</annotation></semantics></math></span></span></p></li></ul><p>如果用 <code>TF-IDF</code> 或 <code>BM25</code> 计算相关性，会得出错误结论。</p><p>想要避免这类错误，需要用到 <strong>词距</strong>。</p><blockquote class=blockquote><ul><li><strong>词距</strong>:
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span>
</span>中的两个词出现在文档
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span>
</span>中，两者间隔多少词。</li></ul></blockquote><p>词距 <strong>越小</strong>，
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span>
</span>与
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span>
</span><strong>越可能相关</strong>。</p><h4 id=okatp class=heading><code>OkaTP</code><a href=#okatp aria-labelledby=okatp><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h4><p>既考虑了词频，也考虑了词距:</p><p>词
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span>
</span>在文档
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span>
</span>中出现的位置记作集合
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>t</mi><mo separator="true">,</mo><mi>d</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(t,d)</annotation></semantics></math></span>
</span>。</p><ul><li><p>出现在文档
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span>
</span>中第
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>27</mn></mrow><annotation encoding="application/x-tex">27</annotation></semantics></math></span>
</span>、
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>84</mn></mrow><annotation encoding="application/x-tex">84</annotation></semantics></math></span>
</span>、
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>98</mn></mrow><annotation encoding="application/x-tex">98</annotation></semantics></math></span>
</span>位置上。</p></li><li><p>那么
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>t</mi><mo separator="true">,</mo><mi>d</mi><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">{</mo><mn>27</mn><mo separator="true">,</mo><mn>84</mn><mo separator="true">,</mo><mn>98</mn><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">O(t,d)=\{27,84,98\}</annotation></semantics></math></span>
</span>。</p></li><li><p>集合
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>t</mi><mo separator="true">,</mo><mi>d</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(t,d)</annotation></semantics></math></span>
</span>的大小等于词频：
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∣</mi><mi mathvariant="script">O</mi><mo stretchy="false">(</mo><mi>t</mi><mo separator="true">,</mo><mi>d</mi><mo stretchy="false">)</mo><mi mathvariant="normal">∣</mi><mo>=</mo><msub><mrow><mi mathvariant="normal">t</mi><mi mathvariant="normal">f</mi></mrow><mrow><mi>t</mi><mo separator="true">,</mo><mi>d</mi></mrow></msub></mrow><annotation encoding="application/x-tex">|\mathcal{O}(t,d)|=\mathrm{tf}_{t,d}</annotation></semantics></math></span>
</span>。</p></li></ul><p><span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span>
</span>和
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>t</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow><annotation encoding="application/x-tex">t^{\prime}</annotation></semantics></math></span>
</span>是查询词
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span>
</span>中的两个词，它们的 <strong>词距分数</strong> <code>TP</code></p><div class=mathjax-display><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mrow><mi mathvariant="normal">t</mi><mi mathvariant="normal">p</mi></mrow><mo stretchy="false">(</mo><mi>t</mi><mo separator="true">,</mo><msup><mi>t</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo separator="true">,</mo><mi>d</mi><mo stretchy="false">)</mo><mo>=</mo><munder><mo>∑</mo><mrow><mi>o</mi><mo>∈</mo><mi>O</mi><mo stretchy="false">(</mo><mi>t</mi><mo separator="true">,</mo><mi>d</mi><mo stretchy="false">)</mo></mrow></munder><munder><mo>∑</mo><mrow><msup><mi>o</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>∈</mo><mi>O</mi><mo stretchy="false">(</mo><msup><mi>t</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo separator="true">,</mo><mi>d</mi><mo stretchy="false">)</mo></mrow></munder><mfrac><mn>1</mn><mrow><mo stretchy="false">(</mo><mi>o</mi><mo>−</mo><msup><mi>o</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mrow></mfrac><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">
\mathrm{tp}(t, t&#x27;, d)=\sum_{o\in O(t, d)}\sum_{o&#x27;\in O(t&#x27;, d)}\frac1{(o-o&#x27;)^2}.
</annotation></semantics></math></span></div><p>查询词中的
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo separator="true">,</mo><msup><mi>t</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>∈</mo><mi>Q</mi></mrow><annotation encoding="application/x-tex">t,t^{\prime} \in Q</annotation></semantics></math></span>
</span>在文档
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span>
</span>中出现 <strong>次数越多、距离越近</strong>，则
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi mathvariant="normal">t</mi><mi mathvariant="normal">p</mi></mrow><mo stretchy="false">(</mo><mi>t</mi><mo separator="true">,</mo><msup><mi>t</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo separator="true">,</mo><mi>d</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathrm{tp}(t,t^{\prime},d)</annotation></semantics></math></span>
</span><strong>越大</strong>。</p><p><strong><code>OkaTP</code> :</strong></p><div class=mathjax-display><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><munder><mo>∑</mo><mrow><mi>t</mi><mo separator="true">,</mo><msup><mi>t</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>∈</mo><mi mathvariant="script">Q</mi><mo separator="true">,</mo><mi>t</mi><mo mathvariant="normal">≠</mo><mi>t</mi></mrow></munder><mfrac><mrow><mi mathvariant="normal">tp</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>t</mi><mo separator="true">,</mo><msup><mi>t</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo separator="true">,</mo><mi>d</mi><mo stretchy="false">)</mo><mo>⋅</mo><mo stretchy="false">(</mo><mi>k</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><mrow><mi mathvariant="normal">tp</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>t</mi><mo separator="true">,</mo><msup><mi>t</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo separator="true">,</mo><mi>d</mi><mo stretchy="false">)</mo><mo>+</mo><mi>k</mi><mo>⋅</mo><mrow><mo fence="true">(</mo><mn>1</mn><mo>−</mo><mi>b</mi><mo>+</mo><mi>b</mi><mo>⋅</mo><mfrac><msub><mi>l</mi><mi>d</mi></msub><mrow><mi mathvariant="normal">mean</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>l</mi><mi>d</mi></msub><mo stretchy="false">)</mo></mrow></mfrac><mo fence="true">)</mo></mrow></mrow></mfrac><mo>⋅</mo><mi>min</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mrow><mi mathvariant="normal">i</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">f</mi></mrow><mi>t</mi></msub><mo separator="true">,</mo><msub><mrow><mi mathvariant="normal">i</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">f</mi></mrow><msup><mi>t</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">
\sum_{t, t^{\prime}\in\mathcal{Q}, t\neq t}\frac{\operatorname{tp}(t, t^{\prime}, d)\cdot(k+1)}{\operatorname{tp}(t, t^{\prime}, d)+k\cdot\left(1-b+b\cdot\frac{l_d}{\operatorname{mean}(l_d)}\right)} \cdot \min(\mathrm{idf}_t,\mathrm{idf}_{t^{\prime}})
</annotation></semantics></math></span></div><ul><li><p>第一项跟 <code>BM25</code> 的区别是，把 <code>Term Frequency</code>
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mi mathvariant="normal">t</mi><mi mathvariant="normal">f</mi></mrow><mrow><mi>t</mi><mo separator="true">,</mo><mi>d</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\mathrm{tf}_{t, d}</annotation></semantics></math></span>
</span>换成了词距分数 <code>TP</code>
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi mathvariant="normal">t</mi><mi mathvariant="normal">p</mi></mrow><mo stretchy="false">(</mo><mi>t</mi><mo separator="true">,</mo><msup><mi>t</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo separator="true">,</mo><mi>d</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathrm{tp}(t,t&#x27;,d)</annotation></semantics></math></span>
</span>。<code>TP</code> 越大，该项就越大。</p></li><li><p>第二项两个 term 各有一个 <code>IDF</code>，取两者中 <strong>较小</strong> 的那一项。</p></li><li><p>对两个 term 的组合求加和</p></li></ul><h3 id=总结-3 class=heading>总结<a href=#%e6%80%bb%e7%bb%93-3 aria-labelledby=总结-3><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h3><p><strong>词匹配分数</strong>包括 <code>TF-IDF</code>、<code>BM25</code> 等</p><ul><li><p><code>TF</code>：词在文档中出现次数越多越好。</p></li><li><p><code>IDF</code>：词在较少的文档中出现，则给词较高的权重。</p></li><li><p><strong>基于词袋模型，只考虑词频，不考虑词序和上下文</strong>。</p></li></ul><p><strong>词距分数</strong>包括 <code>OkaTP</code> 等</p><ul><li><p>查询词
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span>
</span>中的词在文档中出现次数 <strong>越多越好</strong>。</p></li><li><p>查询词
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span>
</span>中的任意两个词在文档中 <strong>越近越好</strong>。</p></li></ul><p>传统搜索相关性会用很多 <strong>人工设计的文本匹配分数</strong>，如词匹配分数、词距分数等一共几十种分数，<strong>把它们作为特征</strong>，用 <strong>线性模型</strong> 或 <strong>树模型</strong> 预测相关性。</p><p>基于文本匹配的传统方法没有真正理解查询词和文档的语义，效果远不如深度学习。现在只有召回海选还用这种方法，用较小代价排序海量文档。</p><h2 id=bert-模型 class=heading>BERT 模型<a href=#bert-%e6%a8%a1%e5%9e%8b aria-labelledby=bert-模型><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h2><p>现代搜索引擎普遍使用 <code>BERT</code> 模型计算查询词
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span>
</span>和文档
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span>
</span>的相关性，文本匹配的方法正在逐步被淘汰。</p><ul><li><p><strong>交叉/单塔 <code>BERT</code> 模型</strong>：把查询词和文档拼成一个序列输入 <code>BERT</code>，准确性好，但推理代价大，通常用于 <strong>搜索链路的下游（精排、粗排）。</strong></p></li><li><p><strong>双塔 BERT 模型</strong>：不够准确，但是推理代价小，给上万篇文档打分也没有问题，通常用在 <strong>链路上游（粗排、召回海选）</strong>。</p></li><li><p>训练相关性 <code>BERT</code> 模型的 <strong>4 个步骤</strong>：<strong>预训练、后预训练、微调、蒸馏。</strong></p></li></ul><p><strong>粗排</strong> 给几千篇文档打分，可能用<strong>交叉 <code>BERT</code></strong>（此时模型应较小，只有
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn></mrow><annotation encoding="application/x-tex">2</annotation></semantics></math></span>
</span>层或
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn></mrow><annotation encoding="application/x-tex">4</annotation></semantics></math></span>
</span>层），也可能用双塔 <code>BERT</code>。</p><p><strong>召回海选</strong> 给几千或者几万篇文档打分。</p><h3 id=模型结构线上推理 class=heading>模型结构、线上推理<a href=#%e6%a8%a1%e5%9e%8b%e7%bb%93%e6%9e%84%e7%ba%bf%e4%b8%8a%e6%8e%a8%e7%90%86 aria-labelledby=模型结构线上推理><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h3><h4 id=交叉-bert-模型 class=heading>交叉 BERT 模型<a href=#%e4%ba%a4%e5%8f%89-bert-%e6%a8%a1%e5%9e%8b aria-labelledby=交叉-bert-模型><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h4><p>交叉 <code>BERT</code> 模型的 self attention 层对查询和文档做了交叉。</p><p>输入的查询词、标题、正文（有可能会包含更多字段，如摘要、Anchor Query）会被切分为 token，每个 token 可以是汉字、词、拉丁字母、英文单词或其他字符串。token 会被 Embedding 层表征为向量。</p><center><img style="border-radius:.3125em;box-shadow:0 .2rem .4rem rgba(34,36,38,.12);margin:1rem" src=pics/image-20250330221718965.png width=60% alt></center><p><strong>每个 token 被表征为
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn></mrow><annotation encoding="application/x-tex">3</annotation></semantics></math></span>
</span>个向量，取加和作为 token 的表征</strong></p><ul><li><p><strong>token embedding</strong>：表征 token 本身。</p></li><li><p><strong>position embedding</strong>：位置编码，表征 token 的序。</p></li><li><p><strong>segment embedding</strong>：用于区分查询词、标题、正文 （非必要，模型可以通过 position embedding 和 <code>[SEP]</code> 分隔符区分查询词、标题、正文
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn></mrow><annotation encoding="application/x-tex">3</annotation></semantics></math></span>
</span>个字段）。</p></li></ul><p>模型的输入有
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span>
</span>个 token，被表征为
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span>
</span>个向量；然后经过很多自注意力层和全连接层，<strong>最终模型输出一个
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">0</annotation></semantics></math></span>
</span>~
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn></mrow><annotation encoding="application/x-tex">1</annotation></semantics></math></span>
</span>之间的实数作为相关性分数</strong>，指示输入句子对的相似性。</p><p><strong>交叉编码器不会产生句子嵌入。</strong></p><hr><h4 id=分词粒度字粒度-vs-字词混合粒度 class=heading>分词粒度：字粒度 v.s. 字词混合粒度<a href=#%e5%88%86%e8%af%8d%e7%b2%92%e5%ba%a6%e5%ad%97%e7%b2%92%e5%ba%a6-vs-%e5%ad%97%e8%af%8d%e6%b7%b7%e5%90%88%e7%b2%92%e5%ba%a6 aria-labelledby=分词粒度字粒度-vs-字词混合粒度><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h4><p>需要把查询词和文档切分成很多 token，具体的分词方法对相关性有很大影响。</p><p>对于 <strong>中文</strong>，有
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn></mrow><annotation encoding="application/x-tex">2</annotation></semantics></math></span>
</span>种分词粒度：</p><ul><li><p><strong>字粒度</strong>：将每个汉字/字符作为一个 token。</p><ul><li><p>词表较小（几千），只包含汉字、字母、常用字符。词表较小则 Embedding Table 也较小。</p></li><li><p>优点：实现简单，无需做分词。</p></li><li><p>第一个版本的相关性 <code>BERT </code>最好用字粒度，实现简单，效果也还行，可以作为好的 baseline。</p></li></ul></li><li><p><strong>字词混合粒度</strong>：做分词，将分词结果作为 tokens。</p><ul><li><p>词表较大（几万、十几万），包含汉字、字母、常用符号、常用中文词语、常用英文单词</p></li><li><p>与字粒度相比，字词混合粒度得到的序列长度更短（即 token 数量更少，可以少一半左右）。</p></li><li><p>参考
<a href=https://github.com/ZhuiyiTechnology/WoBERT class=markdown-link>WoBERT</a>，字词混合粒度更复杂，效果更好。</p></li></ul></li></ul><hr><p><strong>序列更短（token 数量更少）有什么好处？</strong></p><ul><li><p><code>BERT </code>推理的计算量是序列长度/token 数量的超线性函数，介于线性和平方时间复杂度之间。<strong>自注意力层是平方时间复杂度，全连接层是线性时间复杂度</strong>。序列越长，推理代价越大。</p></li><li><p>为了控制推理成本，会限定 token 数量，例如
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>128</mn></mrow><annotation encoding="application/x-tex">128</annotation></semantics></math></span>
</span>或
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>256</mn></mrow><annotation encoding="application/x-tex">256</annotation></semantics></math></span>
</span>。</p></li><li><p>如果文档超出 token 数量上限，会被截断，或者做抽取式摘要（准确性降低）。</p></li><li><p><strong>使用字词混合粒度，token 数量更少，推理成本降低</strong>。（字词混合粒度的序列长度可以比字粒度少一半左右，如字粒度需要
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>256</mn></mrow><annotation encoding="application/x-tex">256</annotation></semantics></math></span>
</span>tokens，字词混合粒度只需要
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>128</mn></mrow><annotation encoding="application/x-tex">128</annotation></semantics></math></span>
</span>tokens）</p></li></ul><hr><h4 id=推理降本 class=heading>推理降本<a href=#%e6%8e%a8%e7%90%86%e9%99%8d%e6%9c%ac aria-labelledby=推理降本><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h4><p>对每个
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>q</mi><mo separator="true">,</mo><mi>d</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(q,d)</annotation></semantics></math></span>
</span>二元组计算相关性分数 score，精排有几百个
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>q</mi><mo separator="true">,</mo><mi>d</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(q,d)</annotation></semantics></math></span>
</span>二元组，粗排有几千个，由于交叉 <code>BERT</code> 模型推理代价很大，代价很大。</p><p><strong>【降本方案】</strong></p><ul><li><p><strong>用内存换计算</strong>，用 Redis 这样的 KV 数据库缓存
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">⟨</mo><mi>q</mi><mo separator="true">,</mo><mi>d</mi><mo separator="true">,</mo><mi>s</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi><mo stretchy="false">⟩</mo></mrow><annotation encoding="application/x-tex">\langle q, d, score\rangle</annotation></semantics></math></span>
</span>。</p><ul><li><p>把查询词和文档 id
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>q</mi><mo separator="true">,</mo><mi>d</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(q,d)</annotation></semantics></math></span>
</span>作为 key，相关性分数 (score) 作为 value。</p></li><li><p>线上做排序时要计算相关性时，如果
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>q</mi><mo separator="true">,</mo><mi>d</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(q,d)</annotation></semantics></math></span>
</span>命中缓存，则避免计算。用户的搜索大多集中在很少的高频查询词上，由于
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>q</mi><mo separator="true">,</mo><mi>d</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(q,d)</annotation></semantics></math></span>
</span>重复率很高，这种缓存机制可以避免一半以上的计算。</p></li><li><p>如果超出内存上限（通常几个 TB 的内存），按照 <code>LRU</code>（Least Recently Used，最近最少使用）清理缓存。</p></li></ul></li><li><p><strong>模型量化技术</strong>，例如将 float32 转化成 int8。</p><p>神经网络的参数都是用浮点数表示的，通常为 float32，也叫单精度浮点数，占 32bits 的存储。把 float32 压缩成 int8 这样的低精度整数。</p><ul><li><p><strong>训练后量化（post-training quantization，<code>PTQ</code>）</strong>：训练不变，量化和训练互不影响，训练完了再做量化，把 float32 压缩成 int8。</p></li><li><p><strong>训练中量化（quantization-aware training，<code>QAT</code>）</strong>：训练和量化是结合在一起做的，训练模型时要做前向传播和反向传播，前向传播使用量化后的低精度整数做计算，反向传播仍使用原始的浮点数权重和浮点数梯度。</p></li></ul></li><li><p><strong>文本摘要技术</strong>，使用文本摘要降低 token 数量。</p><ul><li><p>如果文档长度超出上限，则用摘要替换文档，优于直接截断文档。</p></li><li><p><strong>在文档发布时计算摘要</strong>。可以是抽取式（取一些关键的句子和段落作为摘要），也可以是生成式（用大语言模型生成摘要）。</p></li><li><p>如果摘要效果好，可以将 token 数量上限降低，比如从
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>128</mn></mrow><annotation encoding="application/x-tex">128</annotation></semantics></math></span>
</span>降低到
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>96</mn></mrow><annotation encoding="application/x-tex">96</annotation></semantics></math></span></span></p></li></ul></li></ul><h4 id=双塔-bert-模型 class=heading>双塔 BERT 模型<a href=#%e5%8f%8c%e5%a1%94-bert-%e6%a8%a1%e5%9e%8b aria-labelledby=双塔-bert-模型><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h4><p>准确性不好，但推理代价小。</p><p><strong>双塔 <code>BERT </code>模型既可以用于召回，也可以用于排序。</strong></p><center><img style="border-radius:.3125em;box-shadow:0 .2rem .4rem rgba(34,36,38,.12);margin:1rem" src=pics/image-20250330222645527.png width=60% alt></center><p>左右的神经网络分别把<strong>查询词</strong>和<strong>文档</strong>映射成向量</p><ul><li><p>左边的神经网络（<strong>查询词塔</strong>）在 <strong>线上实时做推理</strong>，用户做一次搜索，只有一个查询词，左塔做一次推理，代价很小。</p><p>查询词类目等特征由查询词处理环节提供。</p></li><li><p>右边的神经网络（<strong>文档塔</strong>）不会在线上做推理，而是在 <strong>文档发布时离线做一次推理</strong>，把算出的文档向量表征存入哈希表（key 是文档 id，value 是向量表征），线上计算相关性时就不用再计算文档的向量表征了。</p></li></ul><p><strong>因此双塔模型在线上的推理代价较小</strong></p><p>计算查询词的向量表征和文档的向量表征的向量相似度，把向量相似度作为估计的相关性分数</p><h4 id=总结-4 class=heading>总结<a href=#%e6%80%bb%e7%bb%93-4 aria-labelledby=总结-4><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h4><p><strong>交叉/单塔 BERT 模型：</strong></p><ul><li><p>准确性高，计算量大，适用于 <strong>精排、粗排。</strong></p></li><li><p>字词混合粒度分词降低序列长度，即 token 数量。</p></li><li><p>用 KV 内存数据库缓存
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">⟨</mo><mi>q</mi><mo separator="true">,</mo><mi>d</mi><mo separator="true">,</mo><mi>s</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi><mo stretchy="false">⟩</mo></mrow><annotation encoding="application/x-tex">\langle q, d, score\rangle</annotation></semantics></math></span>
</span>，可以避免大部分计算。</p></li><li><p>用模型量化技术，把 float32 转化成 int8，降低推理成本。</p></li><li><p>设置较小的 token 数量上限，将长文档替换成摘要。</p></li></ul><hr><p><strong>双塔 BERT 模型：</strong></p><ul><li><p>准确性低，计算量小，适用于 <strong>粗排、召回海选</strong>。</p></li><li><p>事先 离线 计算每篇 文档
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span>
</span>的向量表征
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mi>d</mi></msub></mrow><annotation encoding="application/x-tex">z_d</annotation></semantics></math></span>
</span>，将
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>d</mi><mo separator="true">,</mo><msub><mi>z</mi><mi>d</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(d, z_d)</annotation></semantics></math></span>
</span>存入哈希表。</p></li><li><p>线上计算
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>q</mi><mo separator="true">,</mo><mi>d</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(q,d)</annotation></semantics></math></span>
</span>的相关性时，给定候选文档
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span>
</span>，从哈希表中读取它的向量表征
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mi>d</mi></msub></mrow><annotation encoding="application/x-tex">z_d</annotation></semantics></math></span>
</span>,（key 是文档 id，value 是向量表征）。</p></li><li><p>线上计算查询词
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span>
</span>的向量表征
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>q</mi></msub></mrow><annotation encoding="application/x-tex">x_q</annotation></semantics></math></span>
</span>，然后计算内积
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">⟨</mo><msub><mi>x</mi><mi>q</mi></msub><mo separator="true">,</mo><msub><mi>z</mi><mi>d</mi></msub><mo stretchy="false">⟩</mo></mrow><annotation encoding="application/x-tex">\langle x_q, z_d \rangle</annotation></semantics></math></span>
</span>(可能做个 sigmoid 内积)，结果作为
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>q</mi><mo separator="true">,</mo><mi>d</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(q,d)</annotation></semantics></math></span>
</span>相关性分数。</p></li></ul><h3 id=bert-模型的训练 class=heading><code>BERT</code> 模型的训练<a href=#bert-%e6%a8%a1%e5%9e%8b%e7%9a%84%e8%ae%ad%e7%bb%83 aria-labelledby=bert-模型的训练><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h3><p>不论是交叉 <code>BERT </code>还是双塔 <code>BERT</code>，如果用在排序中计算相关性，两者的训练方法相同。</p><p>训练分
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn></mrow><annotation encoding="application/x-tex">4</annotation></semantics></math></span>
</span>个步骤：</p><ul><li><p><strong>预训练（pretrain）</strong>：用 <code>MLM</code>（Mask Language Model）等任务预训练模型。(直接用开源的模型效果不差，但最好自己用搜索引擎的文档库做预训练，效果会更好)</p></li><li><p><strong>后预训练（post pretrain）</strong>：利用用户的点击、交互 数据 训练模型，因为相关性越好的文档越有可能被点击和交互。</p></li><li><p><strong>微调（fine tuning）</strong>：用人工标注的相关性数据训练模型。</p></li><li><p><strong>蒸馏（distillation）</strong>：得到更小的模型，加速线上的推理（先训练大模型再蒸馏小模型，效果远好于直接训练小模型）。</p></li></ul><h4 id=微调fine-tuning class=heading>微调（fine tuning）<a href=#%e5%be%ae%e8%b0%83fine-tuning aria-labelledby=微调fine-tuning><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h4><p>微调用监督学习训练模型，模型估计
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span>
</span>和
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span>
</span>的相关性。</p><p>人工标注数十万、数百万条样本，每条样本为
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>q</mi><mo separator="true">,</mo><mi>d</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(q,d,y)</annotation></semantics></math></span>
</span>，(查询词，文档，人工标注的相关性分数）</p><p>可以把 <strong>估计相关性</strong> 看作 <strong>回归任务</strong>，也可以看作 <strong>排序任务</strong>。</p><ul><li><p><strong>回归任务</strong>：让预测的值
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span>
</span>拟合
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span>
</span>，起到 <strong>“保值”</strong> 的作用</p><ul><li><p>给定
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>q</mi><mo separator="true">,</mo><mi>d</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(q,d)</annotation></semantics></math></span>
</span>，模型估计相关性为
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span>
</span>。</p></li><li><p><span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span>
</span>越接近真实标签
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span>
</span>越好</p></li></ul></li><li><p><strong>排序任务</strong>：让
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span>
</span>的序拟合
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span>
</span>的序，起到 <strong>“保序”</strong> 的作用。只在乎预测的序是否正确，不在乎预测的值离
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span>
</span>是远是近。</p><ul><li><p>给定两条样本
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>q</mi><mo separator="true">,</mo><msub><mi>d</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>y</mi><mn>1</mn></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(q,d_1,y_1)</annotation></semantics></math></span>
</span>和
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>q</mi><mo separator="true">,</mo><msub><mi>d</mi><mn>2</mn></msub><mo separator="true">,</mo><msub><mi>y</mi><mn>2</mn></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(q,d_2,y_2)</annotation></semantics></math></span>
</span>，相同的查询词
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span>
</span>，不同的文档
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span>
</span>，真实相关性分数满足
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mn>1</mn></msub><mo>&gt;</mo><msub><mi>y</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">y_1&gt;y_2</annotation></semantics></math></span>
</span>。</p></li><li><p>模型预测的相关性分数
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">p_1</annotation></semantics></math></span>
</span>和
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">p_2</annotation></semantics></math></span>
</span>应当满足
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mn>1</mn></msub><mo>&gt;</mo><msub><mi>p</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">p_1&gt;p_2</annotation></semantics></math></span>
</span>（正序对），反之
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mn>1</mn></msub><mo>&lt;</mo><msub><mi>p</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">p_1&lt;p_2</annotation></semantics></math></span>
</span>为逆序对。</p></li></ul></li></ul><hr><h5 id=回归任务 class=heading>回归任务<a href=#%e5%9b%9e%e5%bd%92%e4%bb%bb%e5%8a%a1 aria-labelledby=回归任务><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h5><p><strong>数据</strong>：
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>q</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>d</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>y</mi><mn>1</mn></msub><mo stretchy="false">)</mo><mo separator="true">,</mo><mo>⋯</mo><mo stretchy="false">(</mo><msub><mi>q</mi><mi>n</mi></msub><mo separator="true">,</mo><msub><mi>d</mi><mi>n</mi></msub><mo separator="true">,</mo><msub><mi>y</mi><mi>n</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(q_1,d_1,y_1) ,\cdots (q_n,d_n,y_n)</annotation></semantics></math></span>
</span>，其中
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">y_i</annotation></semantics></math></span>
</span>是归一化后的相关性分数，
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>∈</mo><mo stretchy="false">[</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">y_i \in [0,1]</annotation></semantics></math></span>
</span>。</p><p>模型预测
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>q</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>d</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(q_i,d_i)</annotation></semantics></math></span>
</span>的 <strong>相关性</strong> 为
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">p_i</annotation></semantics></math></span>
</span>。</p><p><strong>最小化损失函数</strong>
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>1</mn><mi>n</mi></mfrac><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>p</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\frac1n\sum_{i=1}^n\log(y_i,p_i)</annotation></semantics></math></span>
</span>，使得
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">p_i</annotation></semantics></math></span>
</span>尽量接近
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">y_i</annotation></semantics></math></span>
</span>。</p><ul><li><p><strong>均方差损失函数</strong>：</p><div class=mathjax-display><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mrow><mi mathvariant="normal">M</mi><mi mathvariant="normal">S</mi><mi mathvariant="normal">E</mi><mi mathvariant="normal">_</mi><mi mathvariant="normal">L</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">s</mi><mi mathvariant="normal">s</mi></mrow><mo stretchy="false">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>p</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mo stretchy="false">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><msub><mi>p</mi><mi>i</mi></msub><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">
  \mathrm{MSE\_Loss}(y_i, p_i)=\frac12(y_i-p_i)^2
  </annotation></semantics></math></span></div></li><li><p><strong>交叉熵损失函数</strong>（类似二分类，用 soft label 更好)</p><div class=mathjax-display><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mrow><mi mathvariant="normal">C</mi><mi mathvariant="normal">E</mi><mi mathvariant="normal">_</mi><mi mathvariant="normal">L</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">s</mi><mi mathvariant="normal">s</mi></mrow><mo stretchy="false">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>p</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><msub><mi>y</mi><mi>i</mi></msub><mo>⋅</mo><mi>ln</mi><mo>⁡</mo><msub><mi>p</mi><mi>i</mi></msub><mo>−</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>⋅</mo><mi>ln</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><msub><mi>p</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">
  \mathrm{CE\_Loss}(y_i, p_i)=-y_i\cdot\ln p_i-(1-y_i)\cdot\ln(1-p_i)
  </annotation></semantics></math></span></div><p>用 <code>CE Loss</code> 时，标签
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span>
</span>不止是
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">0</annotation></semantics></math></span>
</span>（最低相关性档位）和
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn></mrow><annotation encoding="application/x-tex">1</annotation></semantics></math></span>
</span>（最高相关性档位），还可以是
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">0</annotation></semantics></math></span>
</span>~
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn></mrow><annotation encoding="application/x-tex">1</annotation></semantics></math></span>
</span>之间的小数（其余相关性档位）。</p></li></ul><hr><h5 id=排序任务 class=heading>排序任务<a href=#%e6%8e%92%e5%ba%8f%e4%bb%bb%e5%8a%a1 aria-labelledby=排序任务><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h5><p><strong>数据</strong>：一条样本包含一条查询词
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span>
</span>和
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span>
</span>篇文档
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>⋯</mo><mtext> </mtext><mo separator="true">,</mo><msub><mi>d</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">d_1, \cdots, d_k</annotation></semantics></math></span>
</span>。</p><p>对于
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>q</mi><mo separator="true">,</mo><msub><mi>d</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(q,d_i)</annotation></semantics></math></span>
</span>，<strong>真实相关性分数</strong>记作
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">y_i</annotation></semantics></math></span>
</span>，<strong>模型预测的相关性</strong>记作
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">p_i</annotation></semantics></math></span>
</span>。</p><ul><li><p>两种排序方式：按照
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">y_i</annotation></semantics></math></span>
</span>排序、按照
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">p_i</annotation></semantics></math></span>
</span>排序。</p></li><li><p>排序任务不在乎
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">p_i</annotation></semantics></math></span>
</span>和
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">y_i</annotation></semantics></math></span>
</span>的值是否接近，只在乎两种排序是否接近。</p></li></ul><p>设
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>&gt;</mo><msub><mi>y</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">y_i&gt;y_j</annotation></semantics></math></span>
</span>，损失函数应当鼓励
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mi>i</mi></msub><mo>−</mo><msub><mi>p</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">p_i-p_j</annotation></semantics></math></span>
</span>尽量大。</p><ul><li><p>如果
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mi>i</mi></msub><mo>≥</mo><msub><mi>p</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">p_i \ge p_j</annotation></semantics></math></span>
</span>（模型预测正确），则称
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>i</mi><mo separator="true">,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(i,j)</annotation></semantics></math></span>
</span>为 <strong>正序对</strong>。</p></li><li><p>如果
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mi>i</mi></msub><mo>&lt;</mo><msub><mi>p</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">p_i&lt;p_j</annotation></semantics></math></span>
</span>（模型预测错误），则称
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>i</mi><mo separator="true">,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(i,j)</annotation></semantics></math></span>
</span>为 <strong>逆序对</strong>。</p></li></ul><ul><li>损失函数应当惩罚逆序对，鼓励正序对 —> 鼓励
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mi>i</mi></msub><mo>−</mo><msub><mi>p</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">p_i-p_j</annotation></semantics></math></span>
</span>尽量大。</li></ul><p><strong>Pairwise logistic 损失函数：</strong></p><div class=mathjax-display><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><munder><mo>∑</mo><mrow><mo stretchy="false">(</mo><mi>i</mi><mo separator="true">,</mo><mi>j</mi><mo stretchy="false">)</mo><mo>:</mo><msub><mi>y</mi><mi>i</mi></msub><mo>&gt;</mo><msub><mi>y</mi><mi>j</mi></msub></mrow></munder><mi>ln</mi><mo>⁡</mo><mrow><mo fence="true">[</mo><mn>1</mn><mo>+</mo><mi>exp</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><mo>−</mo><mi>γ</mi><mo>⋅</mo><mo stretchy="false">(</mo><msub><mi>p</mi><mi>i</mi></msub><mo>−</mo><msub><mi>p</mi><mi>j</mi></msub><mo stretchy="false">)</mo><mo fence="true">)</mo></mrow><mo fence="true">]</mo></mrow><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">
\sum_{(i, j): y_i &gt; y_j}\ln\left [1+\exp\left(-\gamma\cdot(p_i-p_j)\right)\right].
</annotation></semantics></math></span></div><ul><li><p><span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mi>i</mi></msub><mo>−</mo><msub><mi>p</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">p_i-p_j</annotation></semantics></math></span>
</span>越大，则损失函数越小。最小化损失函数，可以鼓励
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>i</mi><mo separator="true">,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(i,j)</annotation></semantics></math></span>
</span>成为正序对。</p></li><li><p><span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi></mrow><annotation encoding="application/x-tex">\gamma</annotation></semantics></math></span>
</span>是一个
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">&gt;0</annotation></semantics></math></span>
</span>的超参数，控制 logistic 函数的形状。</p></li><li><p>一条样本包含
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span>
</span>篇文档，对
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span>
</span>篇文档两两组合，最小化加和，会鼓励模型给
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span>
</span>篇文档的排序，接近按
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span>
</span>做的排序。</p></li></ul><hr><h5 id=总结-5 class=heading>总结<a href=#%e6%80%bb%e7%bb%93-5 aria-labelledby=总结-5><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h5><ul><li><p><strong>回归任务</strong>，使用 <strong>均方差损失（<code>MSE</code>）</strong> 或 <strong>交叉熵损失（<code>CE</code>）</strong>，有利于提升 <code>AUC</code> 指标。</p></li><li><p><strong>排序任务</strong>，使用 <strong>pairwise logistic 损失</strong>，有利于提升正逆序比指标。</p></li><li><p><strong>不要把估计相关性看作多分类任务</strong>：相关性有多个档位，但档位之间是有序的，而不是无序的类别名称。</p></li><li><p>如果同时用 <code>AUC</code> 和 <strong>正逆序比</strong> 作为离线评价指标，则同时使用 <code>CE</code> 和 pairwise logistic 损失，取两者的加权和作为优化的目标函数。</p></li></ul><hr><h4 id=后预训练post-pretrain class=heading>后预训练（post pretrain）<a href=#%e5%90%8e%e9%a2%84%e8%ae%ad%e7%bb%83post-pretrain aria-labelledby=后预训练post-pretrain><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h4><ol><li><a href=https://arxiv.org/abs/2105.11108 class=markdown-link>Zou et al. Pre-trained language model based ranking in Baidu search. In KDD , 2021.</a></li><li><a href=https://dl.acm.org/doi/10.1145/3568681 class=markdown-link>Zou et al. Pre-trained language model-based retrieval and ranking for web search.
ACM Transactions on the Web, 2022.</a></li></ol><p><strong>训练相关性模型：预训练 -> 后预训练 -> 微调 -> 蒸馏</strong></p><hr><p><strong>后预训练的步骤</strong></p><ol><li><p>从搜索日志中挑选十亿对
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>q</mi><mo separator="true">,</mo><mi>d</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(q,d)</annotation></semantics></math></span>
</span>。对于
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>48</mn></mrow><annotation encoding="application/x-tex">48</annotation></semantics></math></span>
</span>层 BERT，数据量超过 10 亿的话，边际效益会越来越小，更多数据对指标的提升不大。</p></li><li><p>自动生成标签：将用户行为
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">x</mi></mrow><annotation encoding="application/x-tex">\mathbf x</annotation></semantics></math></span>
</span>映射到相关性分数
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>y</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat{y}</annotation></semantics></math></span>
</span>。自动生成的标签
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>y</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat{y}</annotation></semantics></math></span>
</span>噪声很大，但仍然有很多信息量，这就是为什么后预训练有效 (技巧很多，对后续结果影响最大)。</p></li><li><p>用自动生成的
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>q</mi><mo separator="true">,</mo><mi>d</mi><mo separator="true">,</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(q,d, \hat{y})</annotation></semantics></math></span>
</span>训练模型。方法与微调类似，都是监督学习。<strong>但额外加上预训练的 MLM 任务，避免预训练的结果被清洗掉</strong>。</p></li></ol><center><img style="border-radius:.3125em;box-shadow:0 .2rem .4rem rgba(34,36,38,.12);margin:1rem" src=pics/image-20250330225134100.png width=60% alt></center><hr><h5 id=步骤-1挑选-q-d class=heading>步骤 1：挑选
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>q</mi><mo separator="true">,</mo><mi>d</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(q, d)</annotation></semantics></math></span>
</span><a href=#%e6%ad%a5%e9%aa%a4-1%e6%8c%91%e9%80%89-q-d aria-labelledby=步骤-1挑选-q-d><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h5><ul><li><p>搜索日志记录用户每次搜索的查询词
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span>
</span>和搜索引擎返回的文档。</p></li><li><p>根据搜索日志抽取查询词
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span>
</span>，需要覆盖高、中、低频的
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span>
</span>。（不是均匀抽样，否则抽到的几乎都是高频查询词）</p></li><li><p>给定
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span>
</span>，搜索日志记录搜到的文档
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>⋯</mo><mtext> </mtext><mo separator="true">,</mo><msub><mi>d</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex"> d_1, \cdots, d_n</annotation></semantics></math></span>
</span>，以及模型估计的相关性分数。（精排相关性模型的打分，非人工标注。不需要很准确，只是用来筛选文档而已。）</p></li><li><p>根据线上模型估计的相关性分数，选取
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span>
</span>篇文档的一个子集，均匀覆盖各相关性档位。</p></li></ul><hr><h5 id=步骤-2自动生成相关性分数 class=heading>步骤 2：自动生成相关性分数<a href=#%e6%ad%a5%e9%aa%a4-2%e8%87%aa%e5%8a%a8%e7%94%9f%e6%88%90%e7%9b%b8%e5%85%b3%e6%80%a7%e5%88%86%e6%95%b0 aria-labelledby=步骤-2自动生成相关性分数><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h5><ul><li><p>步骤 1 根据搜索日志选出十亿对
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>q</mi><mo separator="true">,</mo><mi>d</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(q,d)</annotation></semantics></math></span>
</span>。</p></li><li><p>对搜索日志做统计，得出
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>q</mi><mo separator="true">,</mo><mi>d</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(q,d)</annotation></semantics></math></span>
</span>的点击率和多种交互率，记作向量
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">x</mi></mrow><annotation encoding="application/x-tex">\mathbf{x}</annotation></semantics></math></span>
</span>，其维度大小就是交互统计的特征数，比如关注点击率，点赞率，收藏率，转发率，那么这个
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">x</mi></mrow><annotation encoding="application/x-tex">\mathbf{x}</annotation></semantics></math></span>
</span>维度就是
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn></mrow><annotation encoding="application/x-tex">4</annotation></semantics></math></span>
</span>，每个位置为对应值。</p></li><li><p>已经得到十亿条样本
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>q</mi><mo separator="true">,</mo><mi>d</mi><mo separator="true">,</mo><mi mathvariant="bold">x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(q,d,\mathbf{x})</annotation></semantics></math></span>
</span>，其中 <strong>向量
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">x</mi></mrow><annotation encoding="application/x-tex">\mathbf{x}</annotation></semantics></math></span>
</span>是用户行为</strong>。</p></li><li><p><strong>相关性
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span>
</span>与
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">x</mi></mrow><annotation encoding="application/x-tex">\mathbf{x}</annotation></semantics></math></span>
</span>存在某种函数关系</strong>，(相关性越高，用户越有可能点击和交互)。</p></li><li><p>找出
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span>
</span>与
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">x</mi></mrow><annotation encoding="application/x-tex">\mathbf{x}</annotation></semantics></math></span>
</span>的函数关系：
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>y</mi><mo>^</mo></mover><mo>=</mo><mi>t</mi><mo stretchy="false">(</mo><mi mathvariant="bold">x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\hat{y} = t(\mathbf{x})</annotation></semantics></math></span>
</span>, <strong>函数
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo stretchy="false">(</mo><mo>⋅</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">t(\cdot)</annotation></semantics></math></span>
</span>将用户行为映射到相关性
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span>
</span></strong>，只要学到了这个函数
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo stretchy="false">(</mo><mo>⋅</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">t(\cdot)</annotation></semantics></math></span>
</span>，就能自动生成标签
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>y</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat{y}</annotation></semantics></math></span>
</span>。生成标签
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>y</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat{y}</annotation></semantics></math></span>
</span>是对真实标签
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span>
</span>的近似。</p><ul><li><p>选取几万对
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>q</mi><mo separator="true">,</mo><mi>d</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(q,d)</annotation></semantics></math></span>
</span>，人工标注相关性分数
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span>
</span>。</p></li><li><p>搜索日志记录了
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>q</mi><mo separator="true">,</mo><mi>d</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(q,d)</annotation></semantics></math></span>
</span>的用户行为
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">x</mi></mrow><annotation encoding="application/x-tex">\mathbf{x}</annotation></semantics></math></span>
</span>。</p></li><li><p>得到 <strong>几万条样本</strong>
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi mathvariant="bold">x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\mathbf{x},y)</annotation></semantics></math></span>
</span>，训练一个小模型
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo stretchy="false">(</mo><mi mathvariant="bold">x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">t(\mathbf{x})</annotation></semantics></math></span>
</span>拟合
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span>
</span>。具体来说，定义一个小模型
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo stretchy="false">(</mo><mo>⋅</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">t(\cdot)</annotation></semantics></math></span>
</span>(<code>GBDT</code>/小规模神经网络），它将用户行为
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">x</mi></mrow><annotation encoding="application/x-tex">\mathbf{x}</annotation></semantics></math></span>
</span>映射到相关性分数
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span>
</span>，这样就可以自动生成标签
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>y</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat{y}</annotation></semantics></math></span>
</span>，是对真实标签
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span>
</span>的近似。</p></li></ul></li><li><p><strong>【注意】小模型
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo stretchy="false">(</mo><mo>⋅</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">t(\cdot)</annotation></semantics></math></span>
</span>只能使用点击率、交互率等用户行为作为输入</strong>。</p><ul><li><p>尽量不使用文本特征作为输入。</p><p>小模型
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo stretchy="false">(</mo><mo>⋅</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">t(\cdot)</annotation></semantics></math></span>
</span>的作用 <strong>是把用户行为转化成相关性分数</strong>，而不是根据文本特征去判别
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>q</mi><mo separator="true">,</mo><mi>d</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(q,d)</annotation></semantics></math></span>
</span>相关性。</p></li><li><p>绝对不能用相关性 <code>BERT</code> 模型打分作为输入，<strong>否则会产生反馈回路</strong>（<code>BERT</code> 模型打分 -> 训练小模型
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo stretchy="false">(</mo><mo>⋅</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">t(\cdot)</annotation></semantics></math></span>
</span>-> 小模型
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo stretchy="false">(</mo><mo>⋅</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">t(\cdot)</annotation></semantics></math></span>
</span>生成数据 -> 训练 <code>BERT</code> 模型）</p></li></ul></li><li><p>对于所有十亿条样本
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>q</mi><mo separator="true">,</mo><mi>d</mi><mo separator="true">,</mo><mi mathvariant="bold">x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(q,d,\mathbf{x})</annotation></semantics></math></span>
</span>，用训练好的小模型打分
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>y</mi><mo>^</mo></mover><mo>=</mo><mi>t</mi><mo stretchy="false">(</mo><mi mathvariant="bold">x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\hat{y} = t(\mathbf{x})</annotation></semantics></math></span>
</span>，得到十亿条带自动生成标签的样本
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>q</mi><mo separator="true">,</mo><mi>d</mi><mo separator="true">,</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(q,d,\hat{y})</annotation></semantics></math></span>
</span>，可以用它们来训练 <code>BERT</code> 模型。</p></li></ul><h5 id=步骤-3用生成的数据训练模型 class=heading>步骤 3：用生成的数据训练模型<a href=#%e6%ad%a5%e9%aa%a4-3%e7%94%a8%e7%94%9f%e6%88%90%e7%9a%84%e6%95%b0%e6%8d%ae%e8%ae%ad%e7%bb%83%e6%a8%a1%e5%9e%8b aria-labelledby=步骤-3用生成的数据训练模型><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h5><ul><li><p>前两步得到十亿条样本
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>q</mi><mo separator="true">,</mo><mi>d</mi><mo separator="true">,</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(q,d,\hat{y})</annotation></semantics></math></span>
</span>，其中
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>y</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat{y}</annotation></semantics></math></span>
</span>是自动生成的相关性分数。</p></li><li><p>基于预训练的 <code>BERT </code>模型，用
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>q</mi><mo separator="true">,</mo><mi>d</mi><mo separator="true">,</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(q,d,\hat{y})</annotation></semantics></math></span>
</span>做监督学习。</p></li><li><p>监督学习同时用
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn></mrow><annotation encoding="application/x-tex">3</annotation></semantics></math></span>
</span>个任务，取
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn></mrow><annotation encoding="application/x-tex">3</annotation></semantics></math></span>
</span>个损失函数的加权和。</p><ul><li><p><strong>回归任务</strong>，起到“保值”的作用（模型的输出尽量接近
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>y</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat{y}</annotation></semantics></math></span>
</span>），有利于 <code>AUC</code> 指标。</p></li><li><p><strong>排序任务</strong>，起到“保序”的作用（鼓励正序对，惩罚逆序对），有利于正逆序比指标。</p></li><li><p><strong>预训练的 <code>MLM</code> 任务</strong>，避免清洗掉预训练的结果（后预训练的数据量很大，量级是十亿，会清洗掉预训练的结果）。</p></li></ul></li></ul><hr><p><strong>后预训练为什么有效？</strong></p><ul><li><p>大幅增加了 <strong>有标签样本数量</strong>（百万 -> 十亿）。</p><ul><li><p>人工标注的相关性数据只有几十万到几百万条
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>q</mi><mo separator="true">,</mo><mi>d</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(q,d, y)</annotation></semantics></math></span>
</span>。</p></li><li><p>后预训练使用十亿条
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>q</mi><mo separator="true">,</mo><mi>d</mi><mo separator="true">,</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(q,d,\hat{y})</annotation></semantics></math></span>
</span>。</p></li><li><p>巨大的数据量使模型更准确。</p></li></ul></li><li><p>用户行为
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">x</mi></mrow><annotation encoding="application/x-tex">\mathbf{x}</annotation></semantics></math></span>
</span>与相关性
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span>
</span>有很强的关联。</p><ul><li><p><span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>q</mi><mo separator="true">,</mo><mi>d</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(q,d)</annotation></semantics></math></span>
</span>的相关性越高，越有可能得到点击和交互。</p></li><li><p>小模型可以根据点击率和交互率
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">x</mi></mrow><annotation encoding="application/x-tex">\mathbf{x}</annotation></semantics></math></span>
</span>较为准确地推断
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span>
</span>。</p></li><li><p>小模型生成的标签
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>y</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat{y}</annotation></semantics></math></span>
</span>虽然有噪声，但也有很大的信息量。</p></li></ul></li></ul><h4 id=蒸馏distillation class=heading>蒸馏（distillation）<a href=#%e8%92%b8%e9%a6%8fdistillation aria-labelledby=蒸馏distillation><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h4><p><strong>为什么做蒸馏？</strong></p><p>用户每搜一个查询词，排序需要用相关性。</p><p><code>BERT</code> 模型给数百、数千对
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>q</mi><mo separator="true">,</mo><mi>d</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(q,d)</annotation></semantics></math></span>
</span>打分。BERT 模型越大，计算量越大，给相关性的打分越准。</p><p>为了平衡计算量和准确性，精排常用
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn></mrow><annotation encoding="application/x-tex">4</annotation></semantics></math></span>
</span>~
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>12</mn></mrow><annotation encoding="application/x-tex">12</annotation></semantics></math></span>
</span>层交叉 <code>BERT</code> 在线上做推理，粗排常用
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn></mrow><annotation encoding="application/x-tex">2</annotation></semantics></math></span>
</span>~
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn></mrow><annotation encoding="application/x-tex">4</annotation></semantics></math></span>
</span>层交叉 <code>BERT</code>（或双塔 <code>BERT</code>）。</p><p><strong>两种方法谁更好？</strong></p><ul><li>直接训练小模型（
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn></mrow><annotation encoding="application/x-tex">2</annotation></semantics></math></span>
</span>~
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>12</mn></mrow><annotation encoding="application/x-tex">12</annotation></semantics></math></span>
</span>层）</li><li>先训练
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>48</mn></mrow><annotation encoding="application/x-tex">48</annotation></semantics></math></span>
</span>层大模型，再蒸馏小模型（better）</li></ul><p><strong>先训练 48 层 BERT 作为 teacher，再蒸馏小模型，效果优于直接训练小模型。</strong></p><p><strong>工业界经验</strong>：</p><ul><li><span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>48</mn></mrow><annotation encoding="application/x-tex">48</annotation></semantics></math></span>
</span>层对比
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>12</mn></mrow><annotation encoding="application/x-tex">12</annotation></semantics></math></span>
</span>层，<code>AUC</code> 高
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">2\%</annotation></semantics></math></span>
</span>以上。</li><li><span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>48</mn></mrow><annotation encoding="application/x-tex">48</annotation></semantics></math></span>
</span>层蒸馏
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>12</mn></mrow><annotation encoding="application/x-tex">12</annotation></semantics></math></span>
</span>层，参数量压缩了
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>10</mn></mrow><annotation encoding="application/x-tex">10</annotation></semantics></math></span>
</span>倍以上，<code>AUC</code> 几乎无损。</li><li><span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>48</mn></mrow><annotation encoding="application/x-tex">48</annotation></semantics></math></span>
</span>层蒸馏
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn></mrow><annotation encoding="application/x-tex">4</annotation></semantics></math></span>
</span>层，<code>AUC</code> 损失
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0.5</mn><mi mathvariant="normal">%</mi></mrow><annotation encoding="application/x-tex">0.5\%</annotation></semantics></math></span>
</span>。</li></ul><hr><p><strong>怎样做蒸馏?</strong></p><p>做预训练、后预训练、微调，训练好
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>48</mn></mrow><annotation encoding="application/x-tex">48</annotation></semantics></math></span>
</span>层 BERT 大模型作为 teacher (参数量一二十亿)。</p><ul><li>Teacher 模型越大，它本身越准确，蒸馏出的 student 也越准确。</li><li><span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>48</mn></mrow><annotation encoding="application/x-tex">48</annotation></semantics></math></span>
</span>层 teacher，效果优于
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>24</mn></mrow><annotation encoding="application/x-tex">24</annotation></semantics></math></span>
</span>层和
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>12</mn></mrow><annotation encoding="application/x-tex">12</annotation></semantics></math></span>
</span>层 teacher。</li></ul><p>准备几亿对
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>q</mi><mo separator="true">,</mo><mi>d</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(q,d)</annotation></semantics></math></span>
</span>，用 teacher 给
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>q</mi><mo separator="true">,</mo><mi>d</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(q,d)</annotation></semantics></math></span>
</span>打分
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>y</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat{y}</annotation></semantics></math></span>
</span>。</p><ul><li>蒸馏的数据量越大越好。</li><li>数据量少于
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn></mrow><annotation encoding="application/x-tex">1</annotation></semantics></math></span>
</span>亿，蒸馏会损失较大 <code>AUC</code>。</li><li>数据量超过
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>10</mn></mrow><annotation encoding="application/x-tex">10</annotation></semantics></math></span>
</span>亿，边际效益很小。</li></ul><p><strong>在数据
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>q</mi><mo separator="true">,</mo><mi>d</mi><mo separator="true">,</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(q,d,\hat{y})</annotation></semantics></math></span>
</span>上做监督学习训练小模型</strong>。</p><ul><li>只训练
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn></mrow><annotation encoding="application/x-tex">1</annotation></semantics></math></span>
</span>epoch。（
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn></mrow><annotation encoding="application/x-tex">1</annotation></semantics></math></span>
</span>亿条样本上训练
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn></mrow><annotation encoding="application/x-tex">2</annotation></semantics></math></span>
</span>epoch，效果不如
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn></mrow><annotation encoding="application/x-tex">2</annotation></semantics></math></span>
</span>亿条样本上训练
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn></mrow><annotation encoding="application/x-tex">1</annotation></semantics></math></span>
</span>epoch。)</li><li><strong>与微调相同，同时用回归任务、排序任务</strong></li></ul><hr><p><strong>蒸馏：一些有效的技巧</strong>。</p><ol><li><p><strong>Student 小模型要先预热、再蒸馏</strong>。</p><ul><li><p><strong>预热</strong>：先做预训练、后预训练、微调训练 student。（与训练 teacher 的步骤相同。)</p></li><li><p>基于预热的模型，用蒸馏数据
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>q</mi><mo separator="true">,</mo><mi>d</mi><mo separator="true">,</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(q,d,\hat{y})</annotation></semantics></math></span>
</span>训练 student。</p></li></ul><p>给 student 做预热，效果优于随机初始化，也优于只做预训练</p></li><li><p><strong>不要做逐层蒸馏</strong></p><ul><li><p>逐层蒸馏：让 student 的中间层拟合 teacher 的中间层。逐层蒸馏代价大不好调，多花算力，还不如增加蒸馏数据量。</p></li><li><p>用相同的算力，直接拟合
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>y</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat{y}</annotation></semantics></math></span>
</span>优于逐层蒸馏。</p></li></ul></li><li><p><strong>多级蒸馏和单级蒸馏谁更好</strong>？</p><ul><li>多级蒸馏：
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>48</mn></mrow><annotation encoding="application/x-tex">48</annotation></semantics></math></span>
</span>层 ->
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>12</mn></mrow><annotation encoding="application/x-tex">12</annotation></semantics></math></span>
</span>层 ->
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn></mrow><annotation encoding="application/x-tex">4</annotation></semantics></math></span>
</span>层。</li><li>单级蒸馏：
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>48</mn></mrow><annotation encoding="application/x-tex">48</annotation></semantics></math></span>
</span>层 ->
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn></mrow><annotation encoding="application/x-tex">4</annotation></semantics></math></span>
</span>层。</li><li>有争议，可能是单级蒸馏更好</li></ul><p>与其做多级蒸馏，还不如用同等的算力增加数据量。</p></li></ol><hr><h4 id=总结-6 class=heading>总结<a href=#%e6%80%bb%e7%bb%93-6 aria-labelledby=总结-6><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h4><center><img style="border-radius:.3125em;box-shadow:0 .2rem .4rem rgba(34,36,38,.12);margin:1rem" src=pics/image-20250330230601113.png width=60% alt></center><ul><li><p><strong>预训练（pretrain）</strong>：用跟任务无关的文本训练模型，如用 <code>MLM</code> 等任务在海量文本数据上训练 <code>BERT</code> 模型。</p></li><li><p><strong>后预训练（post pretrain）</strong>：用一个 <code>GBDT</code> 小模型自动生成数据，将用户行为
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">x</mi></mrow><annotation encoding="application/x-tex">\mathrm{x}</annotation></semantics></math></span>
</span>映射到相关性标签
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>y</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat{y}</annotation></semantics></math></span>
</span>。用自动生成的数据训练 <code>BERT </code>大模型和小模型，结合回归任务排序任务以及预训练任务。</p></li><li><p><strong>微调（fine tuning）</strong>：使用人工标注的数据，数据量相对较小，一般几十万（最多几百万）条样本，监督学习，同时用<strong>回归</strong>和<strong>排序</strong>任务。</p></li><li><p><strong>蒸馏（distillation）</strong>：先训练大模型，用训练好的大模型给几亿条
<span class=mathjax-inline><span class=katex><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>q</mi><mo separator="true">,</mo><mi>d</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(q, d)</annotation></semantics></math></span>
</span>打分，得到蒸馏数据；基于预热好的小模型，用蒸馏数据做监督学习；最终得到的小模型部署到线上做相关性。</p></li></ul><h1 id=reference class=heading>Reference<a href=#reference aria-labelledby=reference><svg class="svg-inline--fa fas fa-link anchor" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 640 512"><use href="#fas-link"/></svg></a></h1><p><a href=https://github.com/wangshusen/SearchEngine class=markdown-link>https://github.com/wangshusen/SearchEngine</a></p></div><div class="row row-cols-2 mt-5 mb-3"><div class=col></div><div class="col text-end"><a class=previous href=/blogs/searchengine/basics/>SearchEngine-1-概要&nbsp;<svg class="svg-inline--fa fas fa-arrow-right" fill="currentcolor" aria-hidden="true" role="img" viewBox="0 0 448 512"><use href="#fas-arrow-right"/></svg></a></div></div><a href=# id=back-to-top class=back-to-top><span>▲</span>
</a><a href=# id=scroll-to-bottom class=back-to-top><span>▼</span>
</a><script>document.addEventListener("DOMContentLoaded",function(){var e=document.getElementById("back-to-top"),t=document.getElementById("scroll-to-bottom");e.addEventListener("click",function(e){e.preventDefault(),window.scrollTo({top:0,behavior:"smooth"})}),t.addEventListener("click",function(e){e.preventDefault(),window.scrollTo({top:document.body.scrollHeight,behavior:"smooth"})})})</script></div><div class="col col-md-3 col-lg-2 d-none d-md-block pt-5"><div class="toc toc-sidebar mb-5 my-md-0 mb-lg-5 p-3 text-body-secondary sticky-top"><strong class="d-block h6 my-2 pt-4">On this page:</strong><nav class=toc><a class="toc-item toc-level-1" href=/blogs/searchengine/rel/#相关性定义与分档>相关性：定义与分档 </a><a class="toc-item toc-level-2" href=/blogs/searchengine/rel/#相关-vs-不相关>相关 V.S. 不相关 </a><a class="toc-item toc-level-2" href=/blogs/searchengine/rel/#档位细分>档位细分 </a><a class="toc-item toc-level-2" href=/blogs/searchengine/rel/#标注流程>标注流程 </a><a class="toc-item toc-level-1" href=/blogs/searchengine/rel/#评价指标auc正逆序比dcg>评价指标（AUC、正逆序比、DCG） </a><a class="toc-item toc-level-2" href=/blogs/searchengine/rel/#pointwise-评价指标>Pointwise 评价指标 </a><a class="toc-item toc-level-2" href=/blogs/searchengine/rel/#pairwise-评价指标>Pairwise 评价指标 </a><a class="toc-item toc-level-2" href=/blogs/searchengine/rel/#listwise-评价指标>Listwise 评价指标 </a><a class="toc-item toc-level-2" href=/blogs/searchengine/rel/#总结-2>总结 </a><a class="toc-item toc-level-1" href=/blogs/searchengine/rel/#文本匹配tf-idfbm25词距>文本匹配（TF-IDF、BM25、词距） </a><a class="toc-item toc-level-2" href=/blogs/searchengine/rel/#链路上的相关性模型>链路上的相关性模型 </a><a class="toc-item toc-level-2" href=/blogs/searchengine/rel/#词匹配分数>词匹配分数 </a><a class="toc-item toc-level-2" href=/blogs/searchengine/rel/#词距分数-term-proximity>词距分数 (Term Proximity) </a><a class="toc-item toc-level-2" href=/blogs/searchengine/rel/#总结-3>总结 </a><a class="toc-item toc-level-1" href=/blogs/searchengine/rel/#bert-模型>BERT 模型 </a><a class="toc-item toc-level-2" href=/blogs/searchengine/rel/#模型结构线上推理>模型结构、线上推理 </a><a class="toc-item toc-level-2" href=/blogs/searchengine/rel/#bert-模型的训练>BERT 模型的训练</a></nav></div></div></div></div><footer class="container-fluid footer text-center p-3"><div class="container-xxl text-center"><small>Copyright © 2025 EV's Blog All rights reserved.
|
Powered by
<a href=https://gethinode.com class="link-bg-footer markdown-link">Hinode</a>.</small></div></footer></div><div id=toast-container class="toast-container position-fixed bottom-0 end-0 p-3"><div id=toast-copied-code-message class=toast role=alert aria-live=assertive aria-atomic=true><div class=toast-header><strong class=me-auto>EV's Blog</strong>
<button type=button class=btn-close data-bs-dismiss=toast aria-label=Close></button></div><div class=toast-body>Code copied to clipboard</div></div></div><svg xmlns:xlink="http://www.w3.org/1999/xlink" display="none"><symbol id="fas-ellipsis"><path d="M8 256a56 56 0 11112 0A56 56 0 118 256zm160 0a56 56 0 11112 0 56 56 0 11-112 0zm216-56a56 56 0 110 112 56 56 0 110-112z"/></symbol><symbol id="fas-sun"><path d="M361.5 1.2c5 2.1 8.6 6.6 9.6 11.9L391 121l107.9 19.8c5.3 1 9.8 4.6 11.9 9.6s1.5 10.7-1.6 15.2L446.9 256l62.3 90.3c3.1 4.5 3.7 10.2 1.6 15.2s-6.6 8.6-11.9 9.6L391 391 371.1 498.9c-1 5.3-4.6 9.8-9.6 11.9s-10.7 1.5-15.2-1.6L256 446.9l-90.3 62.3c-4.5 3.1-10.2 3.7-15.2 1.6s-8.6-6.6-9.6-11.9L121 391 13.1 371.1c-5.3-1-9.8-4.6-11.9-9.6s-1.5-10.7 1.6-15.2L65.1 256 2.8 165.7c-3.1-4.5-3.7-10.2-1.6-15.2s6.6-8.6 11.9-9.6L121 121 140.9 13.1c1-5.3 4.6-9.8 9.6-11.9s10.7-1.5 15.2 1.6L256 65.1 346.3 2.8c4.5-3.1 10.2-3.7 15.2-1.6zM160 256a96 96 0 11192 0 96 96 0 11-192 0zm224 0a128 128 0 10-256 0 128 128 0 10256 0z"/></symbol><symbol id="fas-moon"><path d="M223.5 32C1e2 32 0 132.3.0 256S1e2 480 223.5 480c60.6.0 115.5-24.2 155.8-63.4 5-4.9 6.3-12.5 3.1-18.7s-10.1-9.7-17-8.5c-9.8 1.7-19.8 2.6-30.1 2.6-96.9.0-175.5-78.8-175.5-176 0-65.8 36-123.1 89.3-153.3 6.1-3.5 9.2-10.5 7.7-17.3s-7.3-11.9-14.3-12.5c-6.3-.5-12.6-.8-19-.8z"/></symbol><symbol id="fas-angle-left"><path d="M41.4 233.4c-12.5 12.5-12.5 32.8.0 45.3l160 160c12.5 12.5 32.8 12.5 45.3.0s12.5-32.8.0-45.3L109.3 256 246.6 118.6c12.5-12.5 12.5-32.8.0-45.3s-32.8-12.5-45.3.0l-160 160z"/></symbol><symbol id="fas-sort"><path d="M137.4 41.4c12.5-12.5 32.8-12.5 45.3.0l128 128c9.2 9.2 11.9 22.9 6.9 34.9S301 224.1 288 224.1L32 224c-12.9.0-24.6-7.8-29.6-19.8s-2.2-25.7 6.9-34.9l128-128zm0 429.3-128-128c-9.2-9.2-11.9-22.9-6.9-34.9S19.1 288 32.1 288h256c12.9.0 24.6 7.8 29.6 19.8s2.2 25.7-6.9 34.9l-128 128c-12.5 12.5-32.8 12.5-45.3.0z"/></symbol><symbol id="fas-arrow-right"><path d="M438.6 278.6c12.5-12.5 12.5-32.8.0-45.3l-160-160c-12.5-12.5-32.8-12.5-45.3.0s-12.5 32.8.0 45.3L338.8 224H32c-17.7.0-32 14.3-32 32s14.3 32 32 32h306.7L233.4 393.4c-12.5 12.5-12.5 32.8.0 45.3s32.8 12.5 45.3.0l160-160z"/></symbol></svg>
<script src=/js/core.bundle-analytics.en.min.ceb6a67c169a28031391976dac91e1e2f460951862201b6249516a55d0fd6109.js data-category=analytics integrity="sha256-zramfBaaKAMTkZdtrJHh4vRglRhiIBtiSVFqVdD9YQk=" crossorigin=anonymous async></script><script src=/js/core.bundle.en.min.5ced88a4f77bedae238353d0ecd4735062d1de8b76e0efe9323fdf249f1e6854.js integrity="sha256-XO2IpPd77a4jg1PQ7NRzUGLR3ot24O/pMj/fJJ8eaFQ=" crossorigin=anonymous async></script></body></html>